{
  "context": "An expert-router coupling (ERC) loss aligns router decisions with expert capabilities in Mixture-of-Experts (MoE) models by enforcing constraints on internal activations, improving performance and computational efficiency. Mixture-of-Experts (MoE)models lack explicit constraints to ensure the router's decisions align well with the experts' capabilities, which ultimately limits model performance. To address this, we proposeexpert-router coupling (ERC) loss, a lightweight auxiliary loss that tightly couples the router's decisions with expert capabilities. Our approach treats each expert's router embedding as a proxy token for the tokens assigned to that expert, and feeds perturbedrouter embeddingsthrough the experts to obtaininternal activations. The ERC loss enforces two constraints on these activations: (1) Each expert must exhibit higher activation for its own proxy token than for theproxy tokensof any other expert. (2) Each proxy token must elicit stronger activation from its corresponding expert than from any other expert. These constraints jointly ensure that each router embedding faithfully represents its corresponding expert's capability, while each expert specializes in processing the tokens actually routed to it. The ERC loss is computationally efficient, operating only on n^2 activations, where n is the number of experts. This represents a fixed cost independent of batch size, unlike prior coupling methods that scale with the number of tokens (often millions per batch). Through pre-trainingMoE-LLMsranging from 3B to 15B parameters and extensive analysis on trillions of tokens, we demonstrate the effectiveness of the ERC loss. Moreover, the ERC loss offers flexible control and quantitative tracking ofexpert specialization levelsduring training, providing valuable insights into MoEs. We propose the Expert-Router Coupling (ERC) loss, a lightweight auxiliary loss that tightly couples the routerâ€™s decisions with expert capabilities. Unlike prior coupling methods that scale with the number of tokens (often millions per batch), the ERC loss introduces a fixed cost that is independent of batch size. Through pre-training MoE-LLMs ranging from 3B to 15B parameters and extensive analysis on trillions of tokens, we demonstrate the effectiveness of the ERC loss. Moreover, it offers flexible control and quantitative tracking of expert specialization levels during training, providing valuable insights into MoE models. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend arXiv explained breakdown of this paper ðŸ‘‰https://arxivexplained.com/papers/coupling-experts-and-routers-in-mixture-of-experts-via-an-auxiliary-loss arXiv lens breakdown of this paper ðŸ‘‰https://arxivlens.com/PaperView/Details/coupling-experts-and-routers-in-mixture-of-experts-via-an-auxiliary-loss-5826-7a4f3d57 Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2601005",
    "title": "Coupling Experts and Routers in Mixture-of-Experts via an Auxiliary Loss",
    "authors": [
      "Ang Lv"
    ],
    "publication_year": 2026,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2512.23447",
    "upvote": 91
  }
}