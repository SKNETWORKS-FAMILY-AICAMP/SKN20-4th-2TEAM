{
  "context": "ProGuard is a vision-language model that proactively identifies and describes out-of-distribution safety risks in multimodal content through reinforcement learning and a balanced dataset with hierarchical safety annotations. The rapid evolution of generative models has led to a continuous emergence ofmultimodal safetyrisks, exposing the limitations of existing defense methods. To address these challenges, we propose ProGuard, a vision-languageproactive guardthat identifies and describesout-of-distribution(OOD) safety risks without the need for model adjustments required by traditional reactive approaches. We first construct a modality-balanced dataset of 87K samples, each annotated with both binary safety labels and risk categories under a hierarchicalmultimodal safetytaxonomy, effectively mitigating modality bias and ensuring consistent moderation across text, image, and text-image inputs. Based on this dataset, we train our vision-language base model purely throughreinforcement learning(RL) to achieve efficient and concise reasoning. To approximate proactive safety scenarios in a controlled setting, we further introduce an OOD safety category inference task and augment the RL objective with a synonym-bank-based similarity reward that encourages the model to generate concise descriptions for unseen unsafe categories. Experimental results show that ProGuard achieves performance comparable to closed-source large models on binarysafety classification, substantially outperforms existing open-source guard models on unsafe content categorization. Most notably, ProGuard delivers a strong proactive moderation ability, improving OODrisk detectionby 52.6% and OODrisk descriptionby 64.8%. The rapid evolution of generative models has led to a continuous emergence of multimodal safety risks, exposing the limitations of existing defense methods. To address these challenges, we propose ProGuard, a vision-language proactive guard that identifies and describes out-of-distribution (OOD) safety risks without the need for model adjustments required by traditional reactive approaches. We first construct a modality-balanced dataset of 87K samples, each annotated with both binary safety labels and risk categories under a hierarchical multimodal safety taxonomy, effectively mitigating modality bias and ensuring consistent moderation across text, image, and text-image inputs. Based on this dataset, we train our vision-language base model purely through reinforcement learning (RL) to achieve efficient and concise reasoning. To approximate proactive safety scenarios in a controlled setting, we further introduce an OOD safety category inference task and augment the RL objective with a synonym-bank-based similarity reward that encourages the model to generate concise descriptions for unseen unsafe categories. Experimental results show that ProGuard achieves performance comparable to closed-source large models on binary safety classification, substantially outperforms existing open-source guard models on unsafe content categorization. Most notably, ProGuard delivers a strong proactive moderation ability, improving OOD risk detection by 52.6% and OOD risk description by 64.8%. project page:https://yushaohan.github.io/ProGuard/github:https://github.com/yushaohan/ProGuardmodels & dataset:https://huggingface.co/collections/yushaohan/proguard This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend arXiv lens breakdown of this paper ðŸ‘‰https://arxivlens.com/PaperView/Details/proguard-towards-proactive-multimodal-safeguard-3470-8b357fe7 Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2601064",
    "title": "ProGuard: Towards Proactive Multimodal Safeguard",
    "authors": [
      "Shaohan Yu",
      "Lijun Li"
    ],
    "publication_year": 2026,
    "github_url": "https://github.com/yushaohan/ProGuard",
    "huggingface_url": "https://huggingface.co/papers/2512.23573",
    "upvote": 5
  }
}