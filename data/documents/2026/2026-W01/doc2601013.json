{
  "context": "Diffusion-based vision-language models and action frameworks demonstrate superior performance in visual planning and robotic control tasks compared to autoregressive baselines. While autoregressive LargeVision-Language Models (VLMs)have achieved remarkable success, their sequential generation often limits their efficacy in complex visual planning and dynamic robotic control. In this work, we investigate the potential of constructing Vision-Language Models upondiffusion-based large language models (dLLMs)to overcome these limitations. We introduceDream-VL, an open diffusion-based VLM (dVLM) that achieves state-of-the-art performance among previous dVLMs.Dream-VLis comparable to top-tier AR-based VLMs trained on open data on various benchmarks but exhibits superior potential when applied to visual planning tasks. Building uponDream-VL, we introduceDream-VLA, a dLLM-basedVision-Language-Action model (dVLA)developed throughcontinuous pre-trainingon open robotic datasets. We demonstrate that the natively bidirectional nature of this diffusion backbone serves as a superior foundation for VLA tasks, inherently suited foraction chunkingandparallel generation, leading to significantly faster convergence in downstream fine-tuning.Dream-VLAachieves top-tier performance of 97.2% average success rate onLIBERO, 71.4% overall average onSimplerEnv-Bridge, and 60.5% overall average onSimplerEnv-Fractal, surpassing leading models such as Ï€_0 and GR00T-N1. We also validate that dVLMs surpass AR baselines on downstream tasks across different training objectives. We release bothDream-VLandDream-VLAto facilitate further research in the community. Building on the success of Dream 7B, we introduce Dream-VL and Dream-VLA, open VL and VLA models that fully unlock discrete diffusionâ€™s advantages in long-horizon planning, bidirectional reasoning, and parallel action generation for multimodal tasks. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend arXiv lens breakdown of this paper ðŸ‘‰https://arxivlens.com/PaperView/Details/dream-vl-dream-vla-open-vision-language-and-vision-language-action-models-with-diffusion-language-model-backbone-2817-91fbb75c Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2601013",
    "title": "Dream-VL & Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone",
    "authors": [
      "Jiacheng Ye"
    ],
    "publication_year": 2026,
    "github_url": "https://github.com/DreamLM/Dream-VLX",
    "huggingface_url": "https://huggingface.co/papers/2512.22615",
    "upvote": 43
  }
}