{
  "context": "Youtu-LLM is a lightweight language model optimized for computational efficiency and agentic intelligence through a compact architecture, STEM-focused training curriculum, and scalable mid-training strategies for planning and reasoning tasks. We introduce Youtu-LLM, a lightweight yet powerful language model that harmonizes high computational efficiency with native agentic intelligence. Unlike typical small models that rely on distillation, Youtu-LLM (1.96B) is pre-trained from scratch to systematically cultivate reasoning and planning capabilities. The key technical advancements are as follows: (1) Compact Architecture with Long-Context Support: Built on a denseMulti-Latent Attention (MLA) architecturewith a novelSTEM-oriented vocabulary, Youtu-LLM supports a128k context window. This design enables robustlong-context reasoningandstate trackingwithin a minimal memory footprint, making it ideal for long-horizon agent and reasoning tasks. (2) Principled \"Commonsense-STEM-Agent\" Curriculum: We curated a massive corpus of approximately 11T tokens and implemented amulti-stage training strategy. By progressively shifting the pre-training data distribution from general commonsense to complex STEM and agentic tasks, we ensure the model acquires deep cognitive abilities rather than superficial alignment. (3) ScalableAgentic Mid-training: Specifically for theagentic mid-training, we employ diversedata construction schemesto synthesize rich and varied trajectories across math, coding, and tool-use domains. This high-quality data enables the model to internalizeplanning and reflection behaviorseffectively. Extensive evaluations show that Youtu-LLM sets a new state-of-the-art for sub-2B LLMs. On general benchmarks, it achieves competitive performance against larger models, while on agent-specific tasks, it significantly surpasses existing SOTA baselines, demonstrating that lightweight models can possess strong intrinsicagentic capabilities.  This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend arXiv lens breakdown of this paper ðŸ‘‰https://arxivlens.com/PaperView/Details/youtu-llm-unlocking-the-native-agentic-potential-for-lightweight-large-language-models-8640-ff62768a Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2601002",
    "title": "Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models",
    "authors": [
      "Junru Lu",
      "Jiarui Qin",
      "Lingfeng Qiao",
      "Xing Sun",
      "Yinsong Liu",
      "Haodong Lin",
      "Zhenyi Shen"
    ],
    "publication_year": 2026,
    "github_url": "https://github.com/TencentCloudADP/youtu-tip",
    "huggingface_url": "https://huggingface.co/papers/2512.24618",
    "upvote": 112
  }
}