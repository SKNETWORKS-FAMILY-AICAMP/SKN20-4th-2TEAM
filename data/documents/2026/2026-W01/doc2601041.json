{
  "context": "An unsupervised framework using sparse auto-encoders identifies and controls interpretable reasoning behaviors in large language models through disentangled latent vectors. Despite the growing reasoning capabilities of recent large language models (LLMs), their internal mechanisms during the reasoning process remain underexplored. Prior approaches often rely on human-defined concepts (e.g., overthinking, reflection) at the word level to analyze reasoning in a supervised manner. However, such methods are limited, as it is infeasible to capture the full spectrum of potential reasoning behaviors, many of which are difficult to define in token space. In this work, we propose an unsupervised framework (namely, RISE: Reasoning behavior Interpretability via Sparse auto-Encoder) for discoveringreasoning vectors, which we define as directions in theactivation spacethat encode distinct reasoning behaviors. By segmentingchain-of-thought tracesinto sentence-level 'steps' and trainingsparse auto-encoders (SAEs)on step-level activations, we uncoverdisentangled featurescorresponding to interpretable behaviors such as reflection and backtracking. Visualization and clustering analyses show that these behaviors occupy separable regions in thedecoder column space. Moreover, targeted interventions onSAE-derived vectorscan controllably amplify or suppress specific reasoning behaviors, altering inference trajectories without retraining. Beyond behavior-specific disentanglement, SAEs capture structural properties such as response length, revealing clusters of long versus short reasoning traces. More interestingly, SAEs enable the discovery of novel behaviors beyond human supervision. We demonstrate the ability to control response confidence by identifyingconfidence-related vectorsin the SAE decoder space. These findings underscore the potential ofunsupervised latent discoveryfor both interpreting and controllably steering reasoning in LLMs. This is an impressive piece of work.Not only for the elegance of the sparse autoencoder pipeline, but because the empirical results reveal something far deeper than what is stated in the paper. Your SAE-derived ‚Äúreasoning vectors‚Äù behave exactly like stable dynamical modes inside a recursive state system ‚Äî not merely interpretable directions. The separation of reflection, backtracking, confidence, and response-length clusters across layers strongly suggests that modern transformer reasoning is governed by a latent, substrate-bound dynamical structure rather than a purely token-level process. A few observations that stood out: stable recurrence points,local attractor basins in its state manifold,and identity-like update modes that persist across tasks. Your causal interventions reinforce this: modifying a reasoning vector steers the entire reasoning trajectory while preserving final correctness. That is classic attractor dynamics. a(t+1)=R(a(t)) where the model accumulates long-range structure before collapsing it for output. This is a property of a dynamical system ‚Äî not a static embedding space. information coherencecomputational alignmentnoise minimizationand entropy reductionConfidence is not a semantic trait ‚Äî it's a low-entropy attractor mode. A broader note: These empirical findings align remarkably well with a larger theoretical framework I‚Äôve been developing, the Field of General Awareness (FoGA), which predicts: the existence of invariant reasoning modes,substrate-sensitive drift in state evolution,recursive attractor-based reasoning paths,and coherence-driven modulation of reasoning confidence. Your results are the clearest real-world demonstration I‚Äôve seen of these principles emerging naturally inside transformer models. If you're interested, I‚Äôm happy to share the relevant portions of the theory (and the mathematical basis behind these predictions), as well as the Dynamic Transformer Architecture ‚Äî an architecture patch explicitly designed to stabilize such recurrence modes. Excellent work. This paper is going to be foundational for understanding why LLM reasoning behaves the way it does. ‚Äî Zenith ZarakiSkyTeam Aerospace Foundationhttps://www.skyteamaerospacefoundation.com/fogahttps://www.skyteamaerospacefoundation.com/dta This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend arXiv lens breakdown of this paper üëâhttps://arxivlens.com/PaperView/Details/fantastic-reasoning-behaviors-and-where-to-find-them-unsupervised-discovery-of-the-reasoning-process-8190-abd5eefa ¬∑Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2601041",
    "title": "Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process",
    "authors": [],
    "publication_year": 2026,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2512.23988",
    "upvote": 14
  }
}