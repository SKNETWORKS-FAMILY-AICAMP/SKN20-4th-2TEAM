{
  "context": "GaMO enhances sparse-view 3D reconstruction by using geometry-aware multi-view outpainting to improve scene coverage and consistency, achieving state-of-the-art performance with reduced computational cost. Recent advances in 3D reconstruction have achieved remarkable progress in high-quality scene capture from dense multi-view imagery, yet struggle when input views are limited. Various approaches, including regularization techniques, semantic priors, and geometric constraints, have been implemented to address this challenge. Latestdiffusion-based methodshave demonstrated substantial improvements by generating novel views from new camera poses to augment training data, surpassing earlier regularization and prior-based techniques. Despite this progress, we identify three critical limitations in these state-of-the-art approaches: inadequate coverage beyond known view peripheries, geometric inconsistencies across generated views, and computationally expensive pipelines. We introduceGaMO(Geometry-aware Multi-view Outpainter), a framework that reformulates sparse-view reconstruction throughmulti-view outpainting. Instead of generating new viewpoints,GaMOexpands the field of view from existing camera poses, which inherently preserves geometric consistency while providing broader scene coverage. Our approach employs multi-view conditioning andgeometry-aware denoisingstrategies in azero-shotmanner without training. Extensive experiments onReplicaandScanNet++demonstrate state-of-the-art reconstruction quality across 3, 6, and 9 input views, outperforming prior methods inPSNRandLPIPS, while achieving a 25times speedup over SOTAdiffusion-based methodswith processing time under 10 minutes. Project page: https://yichuanh.github.io/GaMO/ Recent advances in 3D reconstruction have achieved remarkable progress in high-quality scene capture from dense multi-view imagery, yet struggle when input views are limited. Various approaches, including regularization techniques, semantic priors, and geometric constraints, have been implemented to address this challenge. Latest diffusion-based methods have demonstrated substantial improvements by generating novel views from new camera poses to augment training data, surpassing earlier regularization and prior-based techniques. Despite this progress, we identify three critical limitations in these state-of-the-art approaches: inadequate coverage beyond known view peripheries, geometric inconsistencies across generated views, and computationally expensive pipelines. We introduce GaMO (Geometry-aware Multi-view Outpainter), a framework that reformulates sparse-view reconstruction through multi-view outpainting. Instead of generating new viewpoints, GaMO expands the field of view from existing camera poses, which inherently preserves geometric consistency while providing broader scene coverage. Our approach employs multi-view conditioning and geometry-aware denoising strategies in a zero-shot manner without training. Extensive experiments on Replica and ScanNet++ demonstrate state-of-the-art reconstruction quality across 3, 6, and 9 input views, outperforming prior methods in PSNR and LPIPS, while achieving a 25Ã— speedup over SOTA diffusion-based methods with processing time under 10 minutes. Project page:https://yichuanh.github.io/GaMO/ This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend arXiv lens breakdown of this paper ðŸ‘‰https://arxivlens.com/PaperView/Details/gamo-geometry-aware-multi-view-diffusion-outpainting-for-sparse-view-3d-reconstruction-1755-76b853fb Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2601018",
    "title": "GaMO: Geometry-aware Multi-view Diffusion Outpainting for Sparse-View 3D Reconstruction",
    "authors": [
      "Yi-Chuan Huang"
    ],
    "publication_year": 2026,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2512.25073",
    "upvote": 33
  }
}