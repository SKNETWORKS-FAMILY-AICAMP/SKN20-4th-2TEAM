{
  "context": "Synthetic chain-of-thought traces from more capable models improve language model reasoning performance even when traces contain incorrect final answers, due to distribution alignment and partial validity of reasoning steps. We present the surprising finding that alanguage model'sreasoning capabilitiescan be improved by training onsynthetic datasetsofchain-of-thought(CoT) traces from more capable models, even when all of those traces lead to an incorrect final answer. Our experiments show this approach can yield better performance on reasoning tasks than training on human-annotated datasets. We hypothesize that two key factors explain this phenomenon: first, the distribution of synthetic data is inherently closer to thelanguage model's own distribution, making it more amenable to learning. Second, these `incorrect' traces are often only partially flawed and contain valid reasoning steps from which the model can learn. To further test the first hypothesis, we use alanguage modelto paraphrase human-annotated traces -- shifting their distribution closer to the model's own distribution -- and show that this improves performance. For the second hypothesis, we introduce increasingly flawedCoT tracesand study to what extent models are tolerant to these flaws. We demonstrate our findings across various reasoning domains likemath, algorithmic reasoning and code generation usingMATH,GSM8K,CountdownandMBPPdatasets on variouslanguage models ranging from 1.5B to 9B acrossQwen,Llama, andGemmamodels. Our study shows that curating datasets that are closer to the model's distribution is a critical aspect to consider. We also show that a correct final answer is not always a reliable indicator of a faithful reasoning process. Training on synthetic CoT traces, even with wrong final answers, improves reasoning due to aligning with the model's distribution and leveraging partial reasoning steps, outperforming human-annotated data. In our paper we explore this interesting observation and provide detailed experimental results and ablation to study the effect of models learning reasoning from unverified noisy and wrong CoTs. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend arXiv lens breakdown of this paper ðŸ‘‰https://arxivlens.com/PaperView/Details/shape-of-thought-when-distribution-matters-more-than-correctness-in-reasoning-tasks-5007-279e56e7 Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2601078",
    "title": "Shape of Thought: When Distribution Matters More than Correctness in Reasoning Tasks",
    "authors": [],
    "publication_year": 2026,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2512.22255",
    "upvote": 2
  }
}