{
  "context": "InteractiveOmni is a unified omni-modal large language model for audio-visual multi-turn interactions, offering comprehensive understanding and speech generation capabilities with efficient parameter usage. We introduce InteractiveOmni, a unified and open-source omni-modal large\nlanguage model foraudio-visual multi-turn interaction, ranging from 4B to 8B\nparameters, designed to lead the field of lightweight models by offering\ncomprehensiveomni-modal understandingandspeech generationcapabilities. To\nachieve this, we integrate thevision encoder,audio encoder, large language\nmodel, andspeech decoderinto a unified model for understanding and generation\ntasks. We design amulti-stage training strategyto ensure robust cross-modal\ncapabilities, including pre-training foromni-modal understanding, followed by\npost-training withspeech conversationandaudio-visual interaction. To enable\nhuman-like long-term conversational ability, we meticulously curate amulti-turn training datasetthat enhances the model's ability to handle complex\nand multi-turn interactions. To effectively evaluate the multi-turn memory and\nspeech interaction capabilities, we construct the multi-modal multi-turn memory\nbenchmark and themulti-turn speech interaction benchmark. Experiments\ndemonstrate that InteractiveOmni significantly outperforms leading open-source\nmodels and provides a more intelligent multi-turn audio-visual experience,\nparticularly in itslong-term memory capabilities. Notably,InteractiveOmni-4Bis comparable to the much larger model likeQwen2.5-Omni-7Bon general\nbenchmarks, and it can retain 97% of the performance of theInteractiveOmni-8Bwhile utilizing only 50% of the model size. Achieving state-of-the-art results\nagainst similarly sized models across image, audio,video understanding, andspeech generationtasks, InteractiveOmni is an accessible, open-source\nfoundation for next-generation intelligent interactive systems. We introduce InteractiveOmni, a unified and open-source omni-modal large language model for audio-visual multi-turn interaction, ranging from 4B to 8B parameters, designed to lead the field of lightweight models by offering comprehensive omni-modal understanding and speech generation capabilities. To achieve this, we integrate the vision encoder, audio encoder, large language model, and speech decoder into a unified model for understanding and generation tasks. We design a multi-stage training strategy to ensure robust cross-modal capabilities, including pre-training for omni-modal understanding, followed by post-training with speech conversation and audio-visual interaction. To enable human-like long-term conversational ability, we meticulously curate a multi-turn training dataset that enhances the model's ability to handle complex and multi-turn interactions. To effectively evaluate the multi-turn memory and speech interaction capabilities, we construct the multi-modal multi-turn memory benchmark and the multi-turn speech interaction benchmark. Experiments demonstrate that InteractiveOmni significantly outperforms leading open-source models and provides a more intelligent multi-turn audio-visual experience, particularly in its long-term memory capabilities. Notably, InteractiveOmni-4B is comparable to the much larger model like Qwen2.5-Omni-7B on general benchmarks, and it can retain 97% of the performance of the InteractiveOmni-8B while utilizing only 50% of the model size. Achieving state-of-the-art results against similarly sized models across image, audio, video understanding, and speech generation tasks, InteractiveOmni is an accessible, open-source foundation for next-generation intelligent interactive systems. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2542052",
    "title": "InteractiveOmni: A Unified Omni-modal Model for Audio-Visual Multi-turn\n  Dialogue",
    "authors": [
      "Wenwen Tong",
      "Hewei Guo",
      "Jiangnan Chen",
      "Jiahao Wang"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/SenseTime-FVG/InteractiveOmni",
    "huggingface_url": "https://huggingface.co/papers/2510.13747",
    "upvote": 29
  }
}