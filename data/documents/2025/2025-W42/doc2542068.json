{
  "context": "AdaR framework enhances LLMs' robustness and generalization in mathematical reasoning by synthesizing logically equivalent queries and using RLVR to penalize spurious logic. Mathematical reasoningis a primary indicator of large language models (LLMs)\nintelligence. However, existing LLMs exhibit failures of robustness and\ngeneralization. This paper attributes these deficiencies tospurious reasoning,\ni.e., producing answers from superficial features. To address this challenge,\nwe propose theAdaRframework to enableadaptive reasoning, wherein models rely\non problem-solving logic to produce answers.AdaRsynthesizes logically\nequivalent queries by varying variable values, and trains models withRLVRon\nthese data to penalize spurious logic while encouraging adaptive logic. To\nimprove data quality, we extract the problem-solving logic from the original\nquery and generate the corresponding answer by code execution, then apply a\nsanity check. Experimental results demonstrate thatAdaRimproves robustness\nand generalization, achieving substantial improvement inmathematical reasoningwhile maintaining highdata efficiency. Analysis indicates thatdata synthesisandRLVRfunction in a coordinated manner to enableadaptive reasoningin LLMs.\nSubsequent analyses derive key design insights into the effect of critical\nfactors and the applicability to instruct LLMs. Our project is available at\nhttps://github.com/LaiZhejian/AdaR Large Language Models (LLMs) have shown impressive reasoning capabilities, yet they often rely onspurious reasoning— producing answers from superficial features, leading to failure at robustness and generalization. We proposeAdaRframework to enable adaptive reasoning, wherein models rely on problem-solving logic to produce answers.AdaRsynthesizes logically equivalent queries by varying variable values, and trains models with RLVR on these data to penalize spurious logic while encouraging adaptive logic. The framework integratesdata synthesisandRLVR trainingto enhance bothrobustness (in-domain)andgeneralization (out-of-domain).  Figure 1.Subfigure I:Three reasoning modes — direct inference (black), spurious reasoning (red), adaptive reasoning (green).Subfigure II:Logic-preserving variable perturbation and gold-answer generation via executable logic.Subfigure III:RLVR optimization encouraging adaptive reasoning through comparative feedback. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend ·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2542068",
    "title": "Making Mathematical Reasoning Adaptive",
    "authors": [
      "Zhejian Lai"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/NJUNLP/AdaR",
    "huggingface_url": "https://huggingface.co/papers/2510.04617",
    "upvote": 22
  }
}