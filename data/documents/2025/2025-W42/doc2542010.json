{
  "context": "AEPO, an agentic RL algorithm, addresses entropy-related challenges in web agent training, enhancing performance and stability across various datasets. Recently,Agentic Reinforcement Learning(Agentic RL) has made significant\nprogress in incentivizing the multi-turn, long-horizon tool-use capabilities of\nweb agents. While mainstream agentic RL algorithms autonomously explore\nhigh-uncertainty tool-call steps under the guidance ofentropy, excessive\nreliance onentropysignals can impose further constraints, leading to the\ntraining collapse. In this paper, we delve into the challenges caused byentropyand propose the AgenticEntropy-Balanced Policy Optimization (AEPO), an\nagentic RL algorithm designed to balanceentropyin both therolloutand policy\nupdate phases.AEPOcomprises two core components: (1) a dynamicentropy-balancedrolloutmechanism that adaptively allocate global and branch\nsampling budget throughentropypre-monitoring, while imposing abranch penaltyon consecutive high-entropytool-call steps to prevent over-branching issues;\nand (2)Entropy-Balanced Policy Optimization that inserts a stop-gradient\noperation into thehigh-entropy clippingterm to preserve and properly rescale\ngradients on high-entropytokens, while incorporatingentropy-aware advantage\nestimation to prioritize learning on high-uncertainty tokens. Results across 14\nchallenging datasets show thatAEPOconsistently outperforms 7 mainstream RL\nalgorithms. With just 1K RL samples, Qwen3-14B withAEPOachieves impressive\nresults: 47.6% on GAIA, 11.2% on Humanity's Last Exam, and 43.0% on WebWalker\nfor Pass@1; 65.0% on GAIA, 26.0% on Humanity's Last Exam, and 70.0% on\nWebWalker for Pass@5. Further analysis reveals thatAEPOimprovesrolloutsampling diversity while maintaining stablepolicy entropy, facilitating\nscalable web agent training. We proposeAgentic Entropy-Balanced Policy Optimization (AEPO), an entropy-balanced agentic RL algorithm designed for training multi-turn web agents.AEPO focuses on balancing and rationalizing rollout branching and policy updates under the guidance of high-entropy tool calls, thereby achieving more stable RL training.  With just 1ùêæ RL samples, Qwen3-14B with AEPO achieves impressive results:47.6% on GAIA, 11.2% on Humanity‚Äôs Last Exam, and 43.0% on WebWalkerQA for Pass@1; 65.0% on GAIA, 26.0% on Humanity‚Äôs Last Exam, and 70.0% onWebWalkerQA for Pass@5. üîß All the code, datasets and model checkpoints of AEPO are fully open-sourced: Github:https://github.com/dongguanting/ARPO Models:https://huggingface.co/collections/dongguanting/aepo-68ef6832c99697ee03d5e1c7 üî• Key Insights: We systematically reveal two entropy-driven issues inherent to agentic RL:\"High-Entropy Rollout Collapse\"and\"High-Entropy Token Gradient Clipping\"(as shown in the above figure). Through preliminary experiments, we quantify their impact on multi-turn web-agent training, offering empirical evidence for further research into entropy balancing. We propose aDynamic Entropy-Balanced Rollout mechanism, which adaptively allocates rollout sampling budgets via entropy pre-monitoring, while imposing a branch penalty on consecutive high-entropy steps to prevent over-branching issues. We introduceEntropy-Balanced Policy Optimization, which intuitively integrates a stop-gradient operation into the high-entropy clipping term to preserve and rescale gradients on high-entropy tokens, while incorporatingentropy-aware advantage estimationto prioritize learning on high-uncertainty tokens. Experiments on 14 challenging benchmarks demonstrate that AEPO consistently outperforms 7 mainstream RL algorithms in web agent training.With just 1ùêæ RL samples, Qwen3-14B with AEPO achieves impressive results: 47.6% on GAIA, 11.2% on Humanity‚Äôs Last Exam, and 43.0% on WebWalkerQA for Pass@1; 65.0% on GAIA, 26.0% on Humanity‚Äôs Last Exam, and 70.0% on WebWalkerQA for Pass@5. ‚ú® Two entropy-driven challenges:  üî• Overview of AEPO:  arXiv explained breakdown of this paper üëâhttps://arxivexplained.com/papers/agentic-entropy-balanced-policy-optimization This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend ¬∑Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2542010",
    "title": "Agentic Entropy-Balanced Policy Optimization",
    "authors": [
      "Guanting Dong",
      "Licheng Bao",
      "Zhicheng Dou"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/RUC-NLPIR/ARPO/blob/main/",
    "huggingface_url": "https://huggingface.co/papers/2510.14545",
    "upvote": 104
  }
}