{
  "context": "VPPO, a novel policy gradient algorithm, enhances multimodal RLVR by leveraging token perception to refine learning signals and improve reasoning capabilities in Large Vision-Language Models. WhileReinforcement Learning with Verifiable Rewards(RLVR) has advanced the\nreasoning capabilities ofLarge Vision-Language Models(LVLMs), most existing\nmethods inmultimodal reasoningneglect the critical role of visual perception\nwithin the RLVR optimization process. In this paper, we undertake a pioneering\nexploration of multimodal RLVR through the novel perspective of token\nperception, which measures thevisual dependencyof each generated token. With\na granular analysis ofChain-of-Thought(CoT) processes, we uncover two key\ninsights: first,token perceptionin arollout trajectoryis sparsely\ndistributed, where only a small fraction of tokens have highvisual dependencyfor visually-grounded reasoning; second, different trajectories exhibit\nsignificant divergence in their overallvisual dependency. Based on these\nobservations, we proposeVisually-Perceptive Policy Optimization(VPPO), a\nnovelpolicy gradient algorithmthat explicitly leveragestoken perceptionto\nrefine the learning signal. Specifically, VPPO achieves this through a dual\nmechanism: it reweights a trajectory's advantage by its overall visual\ndependency, and focuses policy updates exclusively on perceptually pivotal\ntokens. On a comprehensive suite of eight perception and reasoning benchmarks,\nVPPO demonstrates substantial gains over leading open-source RL-tuned models,\nwith its effectiveness consistently validated across 7B and 32B model scales.\nOur findings not only establish a new token-level perceptual perspective for\nanalyzing multimodal RLVR but also present a novel and effective optimization\nstrategy to significantly enhance themultimodal reasoningcapabilities of\nLVLMs. While Reinforcement Learning with Verifiable Rewards (RLVR) has advanced the reasoning capabilities of Large Vision-Language Models (LVLMs), most existing methods in multimodal reasoning neglect the critical role of visual perception within the RLVR optimization process. In this paper, we undertake a pioneering exploration of multimodal RLVR through the novel perspective of token perception, which measures the visual dependency of each generated token. With a granular analysis of Chain-of-Thought (CoT) processes, we uncover two key insights: first, token perception in a rollout trajectory is sparsely distributed, where only a small fraction of tokens have high visual dependency for visually-grounded reasoning; second, different trajectories exhibit significant divergence in their overall visual dependency. Github：https://github.com/huaixuheqing/VPPO-RL This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend ·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2542038",
    "title": "Spotlight on Token Perception for Multimodal Reinforcement Learning",
    "authors": [
      "Siyuan Huang"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/huaixuheqing/VPPO-RL",
    "huggingface_url": "https://huggingface.co/papers/2510.09285",
    "upvote": 36
  }
}