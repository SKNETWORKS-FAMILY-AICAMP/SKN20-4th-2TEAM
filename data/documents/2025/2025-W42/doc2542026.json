{
  "context": "StreamingVLM is a real-time vision-language model that efficiently processes infinite video streams using a compact KV cache and supervised fine-tuning, achieving high performance on long videos and diverse benchmarks. Vision-language models(VLMs) could powerreal-time assistantsand autonomous\nagents, but they face a critical challenge: understanding near-infinite video\nstreams without escalating latency and memory usage. Processing entire videos\nwith full attention leads toquadratic computational costsand poor performance\non long videos. Meanwhile, simplesliding window methodsare also flawed, as\nthey either break coherence or suffer from high latency due to redundant\nrecomputation. In this paper, we introduce StreamingVLM, a model designed for\nreal-time, stable understanding of infinite visual input. Our approach is a\nunified framework that aligns training with streaming inference. During\ninference, we maintain a compact KV cache by reusing states ofattention sinks,\na short window of recentvision tokens, and a long window of recent text\ntokens. This streaming ability is instilled via a simplesupervised fine-tuning(SFT) strategy that applies full attention on short, overlapped video chunks,\nwhich effectively mimics theinference-time attention patternwithout training\non prohibitively long contexts. For evaluation, we buildInf-Streams-Eval, a\nnew benchmark with videos averaging over two hours that requires dense,\nper-second alignment between frames and text. OnInf-Streams-Eval, StreamingVLM\nachieves a 66.18%win rateagainst GPT-4O mini and maintains stable, real-time\nperformance at up to 8 FPS on a singleNVIDIA H100. Notably, ourSFTstrategy\nalso enhances generalVQA abilitieswithout any VQA-specific fine-tuning,\nimproving performance onLongVideoBenchby +4.30 andOVOBench Realtimeby\n+5.96. Code is available at https://github.com/mit-han-lab/streaming-vlm. StreamingVLM enables real-time, stable understanding of effectively infinite video by keeping a compact KV cache and aligning training with streaming inference. It avoids quadratic cost and sliding-window pitfalls, runs up to 8 FPS on a single H100, and wins 66.18% vs GPT-4o mini on a new long-video benchmark. It also boosts general VQA without task-specific finetuning. You can grasp the gist by skimming this section first. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend @librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2542026",
    "title": "StreamingVLM: Real-Time Understanding for Infinite Video Streams",
    "authors": [
      "Yukang Chen"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/mit-han-lab/streaming-vlm",
    "huggingface_url": "https://huggingface.co/papers/2510.09608",
    "upvote": 50
  }
}