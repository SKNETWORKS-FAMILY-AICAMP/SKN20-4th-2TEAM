{
  "context": "Replacing VAEs with pretrained representation encoders in Diffusion Transformers enhances generative quality and convergence speed without auxiliary losses. Latent generative modeling, where a pretrained autoencoder maps pixels into a\nlatent space for the diffusion process, has become the standard strategy forDiffusion Transformers (DiT); however, the autoencoder component has barely\nevolved. Most DiTs continue to rely on the originalVAE encoder, which\nintroduces several limitations: outdated backbones that compromise\narchitectural simplicity, low-dimensional latent spaces that restrict\ninformation capacity, and weak representations that result from purely\nreconstruction-based training and ultimately limit generative quality. In this\nwork, we explore replacing the VAE withpretrained representation encoders(e.g.,DINO,SigLIP,MAE) paired with trained decoders, forming what we termRepresentation Autoencoders (RAEs). These models provide both high-quality\nreconstructions and semantically rich latent spaces, while allowing for a\nscalabletransformer-based architecture. Since these latent spaces are\ntypically high-dimensional, a key challenge is enablingdiffusion transformersto operate effectively within them. We analyze the sources of this difficulty,\npropose theoretically motivated solutions, and validate them empirically. Our\napproach achieves faster convergence without auxiliary representation alignment\nlosses. Using a DiT variant equipped with a lightweight, wide DDT head, we\nachieve strongimage generationresults onImageNet: 1.51FIDat 256x256 (no\nguidance) and 1.13 at both 256x256 and 512x512 (with guidance). RAE offers\nclear advantages and should be the new default for diffusion transformer\ntraining. We can use pretrained representation models to train the diffusion model, and it works better than VAE! This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend good work arXiv explained breakdown of this paper ðŸ‘‰https://arxivexplained.com/papers/diffusion-transformers-with-representation-autoencoders Here is an AI podcast on the paper if anyone is looking for a quick listen:https://spotifycreators-web.app.link/e/dq8YjT0cDXb arXiv lens breakdown of this paper ðŸ‘‰https://arxivlens.com/PaperView/Details/diffusion-transformers-with-representation-autoencoders-5427-ebadae0f Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2542002",
    "title": "Diffusion Transformers with Representation Autoencoders",
    "authors": [
      "Boyang Zheng",
      "Saining Xie"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/bytetriper/RAE",
    "huggingface_url": "https://huggingface.co/papers/2510.11690",
    "upvote": 165
  }
}