{
  "context": "Parallel decoding in diffusion LLMs degrades generation quality due to ignored token dependencies, highlighting the need for new decoding methods and benchmarks. While mostautoregressive LLMsare constrained to one-by-one decoding,diffusion LLMs(dLLMs) have attracted growing interest for their potential to\ndramatically accelerate inference throughparallel decoding. Despite this\npromise, theconditional independence assumptionindLLMscauses parallel\ndecoding to ignore token dependencies, inevitably degrading generation quality\nwhen these dependencies are strong. However, existing works largely overlook\nthese inherent challenges, and evaluations on standard benchmarks (e.g., math\nand coding) are not sufficient to capture the quality degradation caused byparallel decoding. To address this gap, we first provide aninformation-theoretic analysisofparallel decoding. We then conduct case\nstudies on analytically tractablesynthetic list operationsfrom both data\ndistribution and decoding strategy perspectives, offering quantitative insights\nthat highlight the fundamental limitations ofparallel decoding. Building on\nthese insights, we proposeParallelBench, the first benchmark specifically\ndesigned fordLLMs, featuring realistic tasks that are trivial for humans andautoregressive LLMsyet exceptionally challenging fordLLMsunder parallel\ndecoding. UsingParallelBench, we systematically analyze bothdLLMsandautoregressive LLMs, revealing that: (i)dLLMsunderparallel decodingcan\nsuffer dramatic quality degradation inreal-world scenarios, and (ii) currentparallel decodingstrategies struggle to adapt their degree of parallelism\nbased on task difficulty, thus failing to achieve meaningful speedup without\ncompromising quality. Our findings underscore the pressing need for innovative\ndecoding methods that can overcome the currentspeed-quality trade-off. We\nrelease our benchmark to help accelerate the development of truly efficientdLLMs. Diffusion LLMs (dLLMs) promise faster generation via parallel decoding. However, this speed often comes at the cost of quality, as they ignore token dependencies, an issue that existing benchmarks do not sufficiently capture. To address this issue, we introduceParallelBench, the first benchmark designed to rigorously test this trade-off through realistic tasks that humans and autoregressive (AR) LLMs can easily solve, but which cause dLLMs to collapse as parallelism grows. We releaseParallelBenchto drive research towards truly efficient dLLMs that can overcome this challenge.Project Page This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2542059",
    "title": "ParallelBench: Understanding the Trade-offs of Parallel Decoding in\n  Diffusion LLMs",
    "authors": [
      "Minjae Lee",
      "Yuezhou Hu"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/furiosa-ai/ParallelBench",
    "huggingface_url": "https://huggingface.co/papers/2510.04767",
    "upvote": 27
  }
}