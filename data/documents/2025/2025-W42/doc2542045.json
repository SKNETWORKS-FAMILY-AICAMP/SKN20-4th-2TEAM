{
  "context": "Large Language Models (LLMs) process every token through all layers of a\ntransformer stack, causing wasted computation on simple queries and\ninsufficient flexibility for harder ones that need deeper reasoning.\nAdaptive-depth methods can improve efficiency, but prior approaches rely on\ncostly inference-time search, architectural changes, or large-scale retraining,\nand in practice often degrade accuracy despite efficiency gains. We introduce\nDr.LLM, Dynamic routing of Layers for LLMs, a retrofittable framework that\nequips pretrained models with lightweight per-layer routers deciding to skip,\nexecute, or repeat a block. Routers are trained with explicit supervision:\nusing Monte Carlo Tree Search (MCTS), we derive high-quality layer\nconfigurations that preserve or improve accuracy under a compute budget. Our\ndesign, windowed pooling for stable routing, focal loss with class balancing,\nand bottleneck MLP routers, ensures robustness under class imbalance and long\nsequences. On ARC (logic) and DART (math), Dr.LLM improves accuracy by up to\n+3.4%p while saving 5 layers per example on average. Routers generalize to\nout-of-domain tasks (MMLU, GSM8k, AIME, TruthfulQA, SQuADv2, GPQA, PIQA,\nAGIEval) with only 0.85% accuracy drop while retaining efficiency, and\noutperform prior routing methods by up to +7.7%p. Overall, Dr.LLM shows that\nexplicitly supervised routers retrofit frozen LLMs for budget-aware,\naccuracy-driven inference without altering base weights. Large Language Models (LLMs) process every token through all layers of a transformer stack, wasting compute on simple queries and lacking flexibility for harder ones that need deeper reasoning. Dr.LLM (Dynamic Routing of Layers for LLMs)is a retrofittable framework that adds lightweight per-layer routers to pretrained models.Each router decides whether to skip, execute, or repeat a layer, enabling adaptive depth without retraining or architectural changes. Routers are trained with explicit supervision from Monte Carlo Tree Search (MCTS), generating high-quality layer configurations that preserve or improve accuracy under a compute budget.Stabilized with windowed pooling, focal loss, and bottleneck MLPs, Dr.LLM maintains robustness under class imbalance and long sequences. ðŸ“ˆResults ðŸ’¡ Dr.LLM equips frozen LLMs forbudget-aware,accuracy-driven inferenceâ€” no base weight modification required. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2542045",
    "title": "Dr.LLM: Dynamic Layer Routing in LLMs",
    "authors": [
      "Ahmed Heakl",
      "Martin Gubri"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/parameterlab/dr-llm",
    "huggingface_url": "https://huggingface.co/papers/2510.12773",
    "upvote": 31
  }
}