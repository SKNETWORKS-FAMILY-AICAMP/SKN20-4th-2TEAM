{
  "context": "Environment Tuning enables LLM agents to learn complex behaviors from problem instances using a structured curriculum, environment augmentation, and progress rewards, achieving competitive in-distribution performance and superior out-of-distribution generalization. Large Language Model (LLM) agents show great promise for complex, multi-turn\ntool-use tasks, but their development is often hampered by the extreme scarcity\nof high-quality training data.Supervised fine-tuning(SFT) on synthetic data\nleads to overfitting, whereas standardreinforcement learning(RL) struggles\nwith a critical cold-start problem and training instability. To address these\nchallenges, we introduceEnvironment Tuning, a novel training\nparadigm that enables agents to learn complex behaviors directly from problem\ninstances without relying on pre-collected expert trajectories.Environment Tuningorchestrates this learning process through astructured curriculum, actionableenvironment augmentationthat provides\ncorrective feedback, and fine-grainedprogress rewardsto ensure stable and\nefficient exploration. Using only 400 problem instances from Berkeley\nFunction-Calling Leaderboard (BFCL) benchmark, our method not only achieves\ncompetitivein-distribution performanceagainst strong baselines but also\ndemonstrates superiorout-of-distribution generalization, overcoming the\nperformance collapse common to SFT-based approaches. Our work presents a\nparadigm shift fromsupervised fine-tuningon static trajectories to dynamic,\nenvironment-based exploration, paving the way for training more robust and\ndata-efficient agents. There are three critical challenges in training LLM agents for multi-turn tool use: To tackle these issues, we propose Environment Tuning, a novel paradigm that shifts focus from fine-tuning the agent to tuning the learning environment itself. Our method combines three key components: a structured 4-stage curriculum (from syntax mastery to full complexity), actionable environment augmentation that provides corrective hints instead of cryptic error messages, and fine-grained progress rewards that replace sparse binary feedback with dense turn-by-turn signals. The experimental results are quite striking: using only 400 training samples, we boost Qwen2.5-7B-Instruct from 7% to 37% on BFCL V3, and more importantly, demonstrate superior out-of-distribution generalization where traditional SFT methods often collapse. For instance, on ACEBench Agent, we nearly double ToolACE-2's performance from 8.5% to 15.0%. This suggests that learning through dynamic environmental interaction fosters more robust generalization than training on static trajectories - a compelling insight for the future of agent training in data-scarce scenarios.  This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2542055",
    "title": "Don't Just Fine-tune the Agent, Tune the Environment",
    "authors": [
      "Siyuan Lu",
      "Qintong Wu",
      "Chenyi Zhuang"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2510.10197",
    "upvote": 28
  }
}