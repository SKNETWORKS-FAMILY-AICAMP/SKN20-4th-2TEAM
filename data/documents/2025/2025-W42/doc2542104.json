{
  "context": "AdaMoE, a Mixture-of-Experts architecture, enhances VLA models by leveraging pretrained weights and improving computational efficiency, achieving superior performance in robotic manipulation tasks. Vision-Language-Action (VLA) models are experiencing rapid development and\ndemonstrating promising capabilities in robotic manipulation tasks. However,\nscaling up VLA models presents several critical challenges: (1) Training new\nVLA models from scratch demands substantial computational resources and\nextensive datasets. Given the current scarcity of robot data, it becomes\nparticularly valuable to fully leverage well-pretrained VLA model weights\nduring the scaling process. (2) Real-time control requires carefully balancing\nmodel capacity with computational efficiency. To address these challenges, We\npropose AdaMoE, aMixture-of-Experts(MoE) architecture that inherits\npretrained weights from dense VLA models, and scales up the action expert by\nsubstituting thefeedforward layersintosparsely activated MoE layers. AdaMoEemploys adecoupling techniquethat decouplesexpert selectionfrom expert\nweighting through an independentscale adapterworking alongside the\ntraditionalrouter. This enables experts to be selected based on task relevance\nwhile contributing with independently controlled weights, allowingcollaborative expert utilizationrather than winner-takes-all dynamics. Our\napproach demonstrates that expertise need not monopolize. Instead, throughcollaborative expert utilization, we can achieve superior performance while\nmaintaining computational efficiency. AdaMoEconsistently outperforms the\nbaseline model across key benchmarks, delivering performance gains of 1.8% onLIBEROand 9.3% onRoboTwin. Most importantly, a substantial 21.5% improvement\nin real-world experiments validates its practical effectiveness for robotic\nmanipulation tasks. (1) We present an efficient approach to scale up VLA models. By inheriting weights from well-pretrained VLA foundation models, we extend them into MoE architectures at low cost with well-balanced experts.(2) We introduce a novel MoE architecture specifically designed for VLA models. Through decoupling token selection from expert weighting, this architecture enables both effective load balancing and performance improvement.(3) We demonstrate substantial performance improvements on established benchmarks, achieving 1.8% improvement over the $\\pi_0$ baseline on LIBERO tasks and 9.3% success rate gain on 19 RoboTwin hard setting tasks. Most importantly, a substantial 21.5% improvement in real-world experiments validates its practical effectiveness for robotic manipulation tasks. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2542104",
    "title": "Expertise need not monopolize: Action-Specialized Mixture of Experts for\n  Vision-Language-Action Learning",
    "authors": [
      "Weijie Shen",
      "Zhixuan Liang",
      "Yao Mu"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/swjTheDad/AdaMoE-VLA",
    "huggingface_url": "https://huggingface.co/papers/2510.14300",
    "upvote": 11
  }
}