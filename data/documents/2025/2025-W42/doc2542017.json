{
  "context": "NEO, a novel family of native Vision-Language Models, addresses fundamental constraints and integrates vision and language within a unified framework, achieving competitive performance with limited data. The edifice of nativeVision-Language Models(VLMs) has emerged as a rising\ncontender to typicalmodular VLMs, shaped by evolving model architectures and\ntraining paradigms. Yet, two lingering clouds cast shadows over its widespread\nexploration and promotion: (-) What fundamental constraints setnative VLMsapart from modular ones, and to what extent can these barriers be overcome? (-)\nHow to make research innative VLMsmore accessible and democratized, thereby\naccelerating progress in the field. In this paper, we clarify these challenges\nand outline guiding principles for constructingnative VLMs. Specifically, one\nnative VLM primitive should: (i) effectively align pixel and word\nrepresentations within a sharedsemantic space; (ii) seamlessly integrate the\nstrengths of formerly separate vision and language modules; (iii) inherently\nembody variouscross-modal propertiesthat support unified vision-language\nencoding, aligning, and reasoning. Hence, we launch NEO, a novel family ofnative VLMsbuilt from first principles, capable of rivaling top-tier modular\ncounterparts across diverse real-world scenarios. With only 390M image-text\nexamples, NEO efficiently developsvisual perceptionfrom scratch while\nmitigatingvision-language conflictsinside a dense andmonolithic modelcrafted from our elaborate primitives. We position NEO as a cornerstone for\nscalable and powerfulnative VLMs, paired with a rich set of reusable\ncomponents that foster a cost-effective and extensible ecosystem. Our code and\nmodels are publicly available at: https://github.com/EvolvingLMMs-Lab/NEO. ðŸŒŸNEO: Native Vision-Language PrimitivesðŸŒŸ constructs native VLMs from first principles and shows an alternative multimodal pathway:  End-to-end training, unified native primitives, and intrinsically multimodal design. ðŸ”¥ Unified Native Architecture ðŸ”¥: Innovates native VLM primitives that perform pixelâ€“word encoding, alignment, and reasoning within a single dense model across different scales.ðŸ”¥ Extreme Training Efficiency ðŸ”¥: With only 390M image-text examples, NEO develops strong visual perception from scratch, achieving performance on par with top modular VLMs such as Qwen2.5-VL across multiple benchmarks.ðŸ”¥ Building a Native Ecosystem ðŸ”¥: Provides a rich set of reusable components that lower development costs and facilitate high-performance native large model research, accelerating the native VLM ecosystem. ðŸ”— Paper link:https://arxiv.org/abs/2510.14979ðŸ”— Code link:https://github.com/EvolvingLMMs-Lab/NEOðŸ”— Model link:https://huggingface.co/collections/Paranioar/neo1-0-68f0db9cbac952be3eca7089 This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2542017",
    "title": "From Pixels to Words -- Towards Native Vision-Language Primitives at\n  Scale",
    "authors": [
      "Haiwen Diao",
      "Mingxuan Li"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/EvolvingLMMs-Lab/NEO",
    "huggingface_url": "https://huggingface.co/papers/2510.14979",
    "upvote": 66
  }
}