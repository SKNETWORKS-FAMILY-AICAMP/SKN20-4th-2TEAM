{
  "context": "Continual exposure to low-quality web text leads to cognitive decline in large language models, affecting reasoning, context understanding, safety, and personality traits, with partial recovery possible through instruction tuning and clean data pre-training. We propose and test theLLM Brain Rot Hypothesis: continual exposure to junk\nweb text induces lastingcognitive declineinlarge language models(LLMs). To\ncausally isolate data quality, we run controlled experiments on real Twitter/X\ncorpora, constructing junk and reversely controlled datasets via two orthogonal\noperationalizations: M1 (engagement degree) and M2 (semantic quality), with\nmatched token scale and training operations across conditions. Contrary to the\ncontrol group, continual pre-training of 4 LLMs on the junk dataset causes\nnon-trivial declines (Hedges' g>0.3) onreasoning, long-context\nunderstanding,safety, and inflating \"dark traits\" (e.g., psychopathy,\nnarcissism). The gradual mixtures of junk and control datasets also yield\ndose-response cognition decay: for example, under M1,ARC-Challengewith Chain\nOf Thoughts drops 74.9 rightarrow 57.2 andRULER-CWE84.4 rightarrow 52.3\nas junk ratio rises from 0% to 100%.\n  Error forensics reveal several key insights. First, we identifythought-skippingas the primary lesion: models increasingly truncate or skipreasoningchains, explaining most of the error growth. Second, partial but\nincomplete healing is observed: scaling instruction tuning and clean data\npre-training improve the declined cognition yet cannot restore baseline\ncapability, suggesting persistentrepresentational driftrather than format\nmismatch. Finally, we discover that the popularity, a non-semantic metric, of a\ntweet is a better indicator of the Brain Rot effect than the length in M1.\nTogether, the results provide significant, multi-perspective evidence that data\nquality is a causal driver of LLM capability decay, reframing curation for\ncontinual pretraining as atraining-time safetyproblem and motivating\nroutine \"cognitive health checks\" for deployed LLMs. New Finding: LLMs can get \"Brain Rot\" just like humans -- getting dumb after browsing (learning via next-token predictions) enormous junk data on Twitter/X. The junk data are not traditional garbage training data, but content that is engaging or brainless. A quick-read post on our work:https://x.com/hjy836/status/1980061302497161253 This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend arXiv explained breakdown of this paper ðŸ‘‰https://arxivexplained.com/papers/llms-can-get-brain-rot Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2542066",
    "title": "LLMs Can Get \"Brain Rot\"!",
    "authors": [
      "Shuo Xing",
      "Junyuan Hong",
      "Yifan Wang"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/llm-brain-rot/llm-brain-rot",
    "huggingface_url": "https://huggingface.co/papers/2510.13928",
    "upvote": 22
  }
}