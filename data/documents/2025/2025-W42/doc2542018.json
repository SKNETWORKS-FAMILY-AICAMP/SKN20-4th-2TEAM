{
  "context": "UniMoE-Audio, a unified speech and music generation model using a Dynamic-Capacity Mixture-of-Experts framework, addresses data imbalance and task conflicts, achieving state-of-the-art performance and enhanced cross-domain synergy. Recent advances in unified multimodal models indicate a clear trend towards\ncomprehensive content generation. However, the auditory domain remains a\nsignificant challenge, with music and speech often developed in isolation,\nhindering progress towards universal audio synthesis. This separation stems\nfrom inherent task conflicts and severe data imbalances, which impede the\ndevelopment of a truly unified audio generation model. To address this\nchallenge, we propose UniMoE-Audio, a unified speech and music generation model\nwithin a novelDynamic-Capacity Mixture-of-Experts(MoE) framework.\nArchitecturally, UniMoE-Audio introduces aTop-P routing strategyfor dynamic\nexpert number allocation, and ahybrid expert designcomprisingrouted expertsfor domain-specific knowledge,shared expertsfor domain-agnostic features, andnull expertsfor adaptive computation skipping. To tackle data imbalance, we\nintroduce a three-stage training curriculum: 1)Independent Specialist Trainingleverages original datasets to instill domain-specific knowledge into each\n\"proto-expert\" without interference; 2)MoE Integration and Warmupincorporates\nthese specialists into the UniMoE-Audio architecture, warming up the gate\nmodule and shared expert using a subset of balanced dataset; and 3) Synergistic\nJoint Training trains the entire model end-to-end on the fully balanced\ndataset, fostering enhanced cross-domain synergy. Extensive experiments show\nthat UniMoE-Audio not only achieves state-of-the-art performance on major\nspeech and music generation benchmarks, but also demonstrates superior\nsynergistic learning, mitigating the performance degradation typically seen in\nnaive joint training. Our findings highlight the substantial potential of\nspecializedMoEarchitecture and curated training strategies in advancing the\nfield ofuniversal audio generation. Homepage:\nhttps://mukioxun.github.io/Uni-MoE-site/home.html üéÆ Homepage:https://mukioxun.github.io/Uni-MoE-site/home.html‚≠ê Code:https://github.com/HITsz-TMG/Uni-MoE/blob/master/UniMoE-Audioü§ñ Model:https://huggingface.co/HIT-TMG/UniMoE-Audio-Preview This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend arXiv explained breakdown of this paper üëâhttps://arxivexplained.com/papers/unimoe-audio-unified-speech-and-music-generation-with-dynamic-capacity-moe ¬∑Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2542018",
    "title": "UniMoE-Audio: Unified Speech and Music Generation with Dynamic-Capacity\n  MoE",
    "authors": [
      "Yunxin Li",
      "Xinyu Chen",
      "Haoyuan Shi"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/HITsz-TMG/Uni-MoE/tree/master/UniMoE-Audio",
    "huggingface_url": "https://huggingface.co/papers/2510.13344",
    "upvote": 62
  }
}