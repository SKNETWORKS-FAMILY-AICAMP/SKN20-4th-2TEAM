{
  "context": "Large Language Models face challenges in long-horizon agentic tasks as their\nconstrained memory is easily overwhelmed by distracting or irrelevant context.\nExisting working memory methods typically rely on external, heuristic\nmechanisms that are decoupled from the agent's core policy. In this work, we\nreframe working memory management as a learnable, intrinsic capability. We\npropose a novel framework, Memory-as-Action, where an agent actively manages\nits working memory by executing explicit editing operations as part of a\nunified policy. This formulation allows an agent, trained via reinforcement\nlearning, to balance memory curation against long-term task objectives under\ngiven resource constraints. However, such memory editing actions break the\nstandard assumption of a continuously growing prefix in LLM interactions,\nleading to what we call trajectory fractures. These non-prefix changes disrupt\nthe causal continuity required by standard policy gradient methods, making\nthose methods inapplicable. To address this, we propose a new algorithm,\nDynamic Context Policy Optimization, which enables stable end-to-end\nreinforcement learning by segmenting trajectories at memory action points and\napplying trajectory-level advantages to the resulting action segments. Our\nresults demonstrate that jointly optimizing for task reasoning and memory\nmanagement in an end-to-end fashion not only reduces overall computational\nconsumption but also improves task performance, driven by adaptive context\ncuration strategies tailored to the model's intrinsic capabilities. Large Language Models face challenges in long-horizon agentic tasks as theirconstrained memory is easily overwhelmed by distracting or irrelevant context.Existing working memory methods typically rely on external, heuristic mecha-nisms that are decoupled from the agent’s core policy. In this work, we reframeworking memory management as a learnable, intrinsic capability. We propose anovel framework, Memory-as-Action, where an agent actively manages its work-ing memory by executing explicit editing operations as part of a unified policy.This formulation allows an agent, trained via reinforcement learning, to balancememory curation against long-term task objectives under given resource con-straints. However, such memory editing actions break the standard assumptionof a continuously growing prefix in LLM interactions, leading to what we calltrajectory fractures. These non-prefix changes disrupt the causal continuity re-quired by standard policy gradient methods, making those methods inapplicable.To address this, we propose a new algorithm, Dynamic Context Policy Optimiza-tion, which enables stable end-to-end reinforcement learning by segmenting tra-jectories at memory action points and applying trajectory-level advantages to theresulting action segments. Our results demonstrate that jointly optimizing for taskreasoning and memory management in an end-to-end fashion not only reducesoverall computational consumption but also improves task performance, drivenby adaptive context curation strategies tailored to the model’s intrinsic capabili-ties. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend ·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2542084",
    "title": "Memory as Action: Autonomous Context Curation for Long-Horizon Agentic\n  Tasks",
    "authors": [
      "Yuxiang Zhang",
      "Xueyuan Lin",
      "Shangxi Wu"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/ADaM-BJTU/MemAct",
    "huggingface_url": "https://huggingface.co/papers/2510.12635",
    "upvote": 16
  }
}