{
  "context": "Elastic-Cache optimizes key-value cache management in diffusion large language models to reduce decoding latency without sacrificing prediction accuracy. This work studies how to adaptively recompute key-value (KV) caches fordiffusion large language models(DLMs) to maximize prediction accuracy while\nminimizing decoding latency. Prior methods' decoders recomputeQKVfor all\ntokens at every denoising step and layer, despite KV states changing little\nacross most steps, especially in shallow layers, leading to substantial\nredundancy. We make three observations: (1) distant {bf MASK} tokens\nprimarily act as a length-bias and can be cached block-wise beyond the active\nprediction window; (2) KV dynamics increase with depth, suggesting that\nselective refresh starting from deeper layers is sufficient; and (3) the\nmost-attended token exhibits the smallest KV drift, providing a conservative\nlower bound on cache change for other tokens. Building on these, we propose\n{bfElastic-Cache}, a training-free, architecture-agnostic strategy that\njointly decides {when} to refresh (via anattention-aware drift teston the\nmost-attended token) and {where} to refresh (via adepth-aware schedulethat\nrecomputes from a chosen layer onward while reusing shallow-layer caches and\noff-window MASK caches). Unlike fixed-period schemes,Elastic-Cacheperforms\nadaptive, layer-aware cache updates for diffusion LLMs, reducing redundant\ncomputation and accelerating decoding with negligible loss in generation\nquality. Experiments on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V across\nmathematical reasoning and code generation tasks demonstrate consistent\nspeedups: 8.7times on GSM8K (256 tokens), 45.1times on longer sequences,\nand 4.8times on HumanEval, while consistently maintaining higher accuracy\nthan the baseline. Our method achieves significantly higherthroughput(6.8times on GSM8K) than existing confidence-based approaches while\npreservinggeneration quality, enabling practical deployment of diffusion LLMs. ðŸš€ Attention Is All You Need for KV Cache in Diffusion LLMs ðŸš€ Making Diffusion LLMs Practical! We introduce Elastic-Cache, the first adaptive, layer-aware KV caching strategy for diffusion language models, achieving massive speedups without sacrificing generation quality. ðŸš€Intelligent Cache Updates: Adaptively decides when to refresh (attention-aware drift detection) and where to refresh (depth-selective updates), eliminating redundant computation across denoising steps. ðŸš€ðŸš€Exceptional Speedups : Achieves 8.7Ã— faster inference on GSM8K, 45.1Ã— on longer sequences, and 4.8Ã— on HumanEval, while maintaining or even improving accuracy compared to baselines. ðŸš€ðŸš€ðŸ”¥ Training-Free & Universal : Works out-of-the-box with any diffusion LLM architecture. No retraining needed, just plug and play! ðŸ”— Paper:https://arxiv.org/abs/2510.14973ðŸ”— Project page:https://vila-lab.github.io/elastic-cache-webpage/ This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2542034",
    "title": "Attention Is All You Need for KV Cache in Diffusion LLMs",
    "authors": [
      "Quan Nguyen-Tri",
      "Mukul Ranjan"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2510.14973",
    "upvote": 40
  }
}