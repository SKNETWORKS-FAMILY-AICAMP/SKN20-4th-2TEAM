{
  "context": "A spatial reasoning model using scale-aware experts and progressive rewards demonstrates competitive performance across diverse tasks and scales using a large, curated dataset. With the current surge in spatial reasoning explorations, researchers have\nmade significant progress in understanding indoor scenes, but still struggle\nwith diverse applications such as robotics and autonomous driving. This paper\naims to advance all-scale spatial reasoning across diverse scenarios by\ntackling two key challenges: 1) the heavy reliance on indoor 3D scans and\nlabor-intensive manual annotations for dataset curation; 2) the absence of\neffective all-scale scene modeling, which often leads to overfitting to\nindividual scenes. In this paper, we introduce a holistic solution that\nintegrates astructured spatial reasoningknowledge system, scale-aware\nmodeling, and aprogressive trainingparadigm, as the first attempt to broaden\nthe all-scale spatial intelligence of MLLMs to the best of our knowledge. Using\na task-specific, specialist-driven automated pipeline, we curate over 38K video\nscenes across 5 spatial scales to createSpaceVista-1M, a dataset comprising\napproximately 1Mspatial QA pairsspanning 19 diverse task types. While\nspecialist models can inject useful domain knowledge, they are not reliable for\nevaluation. We then build an all-scale benchmark with precise annotations by\nmanually recording, retrieving, and assembling video-based data. However, naive\ntraining withSpaceVista-1Moften yields suboptimal results due to the\npotential knowledge conflict. Accordingly, we introduceSpaceVista-7B, aspatial reasoning modelthat accepts dense inputs beyond semantics and uses\nscale as an anchor forscale-aware expertsandprogressive rewards. Finally,\nextensive evaluations across 5 benchmarks, including ourSpaceVista-Bench,\ndemonstrate competitive performance, showcasing strong generalization across\nall scales and scenarios. Our dataset, model, and benchmark will be released on\nhttps://peiwensun2000.github.io/mm2km . With the current surge in spatial reasoning explorations, researchers have made significant progress in understanding indoor scenes, but still struggle with diverse applications such as robotics and autonomous driving. This paper aims to advance all-scale spatial reasoning across diverse scenarios by tackling two key challenges: 1) the heavy reliance on indoor 3D scans and labor-intensive manual annotations for dataset curation; 2) the absence of effective all-scale scene modeling, which often leads to overfitting to individual scenes. In this paper, we introduce a holistic solution that integrates a structured spatial reasoning knowledge system, scale-aware modeling, and a progressive training paradigm, as the first attempt to broaden the all-scale spatial intelligence of MLLMs to the best of our knowledge. Using a task-specific, specialist-driven automated pipeline, we curate over 38K video scenes across 5 spatial scales to create SpaceVista-1M, a dataset comprising approximately 1M spatial QA pairs spanning 19 diverse task types. While specialist models can inject useful domain knowledge, they are not reliable for evaluation. We then build an all-scale benchmark with precise annotations by manually recording, retrieving, and assembling video-based data. However, naive training with SpaceVista-1M often yields suboptimal results due to the potential knowledge conflict. Accordingly, we introduce SpaceVista-7B, a spatial reasoning model that accepts dense inputs beyond semantics and uses scale as an anchor for scale-aware experts and progressive rewards. Finally, extensive evaluations across 5 benchmarks, including our SpaceVista-Bench, demonstrate competitive performance, showcasing strong generalization across all scales and scenarios. Project page:https://peiwensun2000.github.io/mm2kmGithub page:https://github.com/PeiwenSun2000/SpaceVista This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2542080",
    "title": "SpaceVista: All-Scale Visual Spatial Reasoning from mm to km",
    "authors": [
      "Peiwen Sun",
      "Shiqiang Lang",
      "Kaituo Feng"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/PeiwenSun2000/SpaceVista",
    "huggingface_url": "https://huggingface.co/papers/2510.09606",
    "upvote": 17
  }
}