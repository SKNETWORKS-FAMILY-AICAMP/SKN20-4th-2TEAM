{
  "context": "Recently, remarkable progress has been made in Unified Multimodal Models\n(UMMs), which integrate vision-language generation and understanding\ncapabilities within a single framework. However, a significant gap exists where\na model's strong visual understanding often fails to transfer to its visual\ngeneration. A model might correctly understand an image based on user\ninstructions, yet be unable to generate a faithful image from text prompts.\nThis phenomenon directly raises a compelling question: Can a model achieve\nself-improvement by using its understanding module to reward its generation\nmodule? To bridge this gap and achieve self-improvement, we introduce SRUM, a\nself-rewarding post-training framework that can be directly applied to existing\nUMMs of various designs. SRUM creates a feedback loop where the model's own\nunderstanding module acts as an internal ``evaluator'', providing corrective\nsignals to improve its generation module, without requiring additional\nhuman-labeled data. To ensure this feedback is comprehensive, we designed a\nglobal-local dual reward system. To tackle the inherent structural complexity\nof images, this system offers multi-scale guidance: a global reward\nensures the correctness of the overall visual semantics and layout, while a\nlocal reward refines fine-grained, object-level fidelity. SRUM leads\nto powerful capabilities and shows strong generalization, boosting performance\non T2I-CompBench from 82.18 to 88.37 and on T2I-ReasonBench from 43.82\nto 46.75. Overall, our work establishes a powerful new paradigm for\nenabling a UMMs' understanding module to guide and enhance its own generation\nvia self-rewarding. A post-training framework that creates a cost-effective, self-iterative optimization loop. SRUM compels a model's understanding component to enhance its generation component for better compositional, reasoning-informed and knowledge-informed generation. Try our new demo!SRUM_SPACE This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2542072",
    "title": "SRUM: Fine-Grained Self-Rewarding for Unified Multimodal Models",
    "authors": [
      "Weiyang Jin"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/WayneJin0918/SRUM",
    "huggingface_url": "https://huggingface.co/papers/2510.12784",
    "upvote": 19
  }
}