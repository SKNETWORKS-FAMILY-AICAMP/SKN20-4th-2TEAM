{
  "context": "A method to reduce object hallucinations in large vision-language models by identifying and masking uncertain visual tokens in the vision encoder. Largevision-language models(LVLMs), which integrate avision encoder(VE)\nwith a large language model, have achieved remarkable success across various\ntasks. However, there are still crucial challenges in LVLMs such as object\nhallucination, generating descriptions of objects that are not in the input\nimage. Here, we argue that uncertainvisual tokenswithin the VE is a key\nfactor that contributes toobject hallucination. Our statistical analysis found\nthat there are positive correlations betweenvisual tokenswith high epistemic\nuncertainty and the occurrence of hallucinations. Furthermore, we show\ntheoretically and empirically thatvisual tokensin early VE layers that\nexhibit large representation deviations under smalladversarial perturbationsindicate highepistemic uncertainty. Based on these findings, we propose a\nsimple yet effective strategy to mitigateobject hallucinationby modifying the\nVE only. Our method comprises a proxy method withadversarial perturbationsfor\nidentifying uncertainvisual tokensefficiently and a method to mask these\nuncertainvisual tokensduring theself-attentionprocess in the middle layers\nof the VE, suppressing their influence onvisual encodingand thus alleviating\nhallucinations. Extensive experiments show that our method significantly\nreducesobject hallucinations in LVLMs and can synergistically work with other\nprior arts. This paper investigates the problem of object hallucination‚Äîwhen large vision-language models (LVLMs) describe objects that don‚Äôt actually appear in an image. The authors reveal that epistemic uncertainty in visual tokens from the vision encoder (VE) is a key factor behind these hallucinations. To address this, they propose a simple yet effective method that:‚Ä¢\tDetects uncertain visual tokens using adversarial perturbations üß†‚ö°‚Ä¢\tMasks these uncertain tokens during the self-attention process in the vision encoder üñºÔ∏èüîç‚Ä¢\tWorks efficiently without retraining and can be combined with existing mitigation techniques Extensive experiments on benchmarks like CHAIR, POPE, and AMBER show significant reductions in hallucination rates while maintaining high caption quality. This approach provides new insights into how visual uncertainty affects model reliability and offers a lightweight solution for more trustworthy LVLMs ü§ñ‚ú®. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend ¬∑Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2542089",
    "title": "On Epistemic Uncertainty of Visual Tokens for Object Hallucinations in\n  Large Vision-Language Models",
    "authors": [
      "Hoigi Seo",
      "Hyunjin Cho"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/joohoonlee/Epistemic",
    "huggingface_url": "https://huggingface.co/papers/2510.09008",
    "upvote": 15
  }
}