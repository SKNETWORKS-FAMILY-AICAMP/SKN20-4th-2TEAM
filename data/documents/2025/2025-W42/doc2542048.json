{
  "context": "Trace Anything, a neural network, predicts video trajectories in a single pass, achieving state-of-the-art performance and demonstrating efficiency and emergent abilities like motion forecasting. Effective spatio-temporal representation is fundamental to modeling,\nunderstanding, and predicting dynamics in videos. The atomic unit of a video,\nthe pixel, traces a continuous 3D trajectory over time, serving as the\nprimitive element of dynamics. Based on this principle, we propose representing\nany video as aTrajectory Field: a dense mapping that assigns a continuous 3D\ntrajectory function of time to each pixel in every frame. With this\nrepresentation, we introduceTrace Anything, a neural network that predicts the\nentiretrajectory fieldin a single feed-forward pass. Specifically, for each\npixel in each frame, our model predicts a set of control points that\nparameterizes a trajectory (i.e., aB-spline), yielding its 3D position at\narbitrary query time instants. We trained theTrace Anythingmodel on\nlarge-scale 4D data, including data from our new platform, and our experiments\ndemonstrate that: (i)Trace Anythingachieves state-of-the-art performance on\nour new benchmark fortrajectory field estimationand performs competitively on\nestablishedpoint-tracking benchmarks; (ii) it offers significant efficiency\ngains thanks to its one-pass paradigm, without requiring iterative optimization\nor auxiliary estimators; and (iii) it exhibits emergent abilities, includinggoal-conditioned manipulation,motion forecasting, andspatio-temporal fusion.\nProject page: https://trace-anything.github.io/. Effective spatio-temporal representation is fundamental to modeling, understanding, and predicting dynamics in videos. The atomic unit of a video, the pixel, traces a continuous 3D trajectory over time, serving as the primitive element of dynamics. Based on this principle, we propose representing any video as a Trajectory Field: a dense mapping that assigns a continuous 3D trajectory function of time to each pixel in every frame. With this representation, we introduce Trace Anything, a neural network that predicts the entire trajectory field in a single feed-forward pass. Specifically, for each pixel in each frame, our model predicts a set of control points that parameterizes a trajectory (i.e., a B-spline), yielding its 3D position at arbitrary query time instants. We trained the Trace Anything model on large-scale 4D data, including data from our new platform, and our experiments demonstrate that: (i) Trace Anything achieves state-of-the-art performance on our new benchmark for trajectory field estimation and performs competitively on established point-tracking benchmarks; (ii) it offers significant efficiency gains thanks to its one-pass paradigm, without requiring iterative optimization or auxiliary estimators; and (iii) it exhibits emergent abilities, including goal-conditioned manipulation, motion forecasting, and spatio-temporal fusion. Project page: this https URL. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2542048",
    "title": "Trace Anything: Representing Any Video in 4D via Trajectory Fields",
    "authors": [
      "Xinhang Liu",
      "Yuxi Xiao",
      "Donny Y. Chen",
      "Bingyi Kang"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/ByteDance-Seed/TraceAnything",
    "huggingface_url": "https://huggingface.co/papers/2510.13802",
    "upvote": 30
  }
}