{
  "context": "A diffusion-based model addresses copy-paste artifacts in text-to-image generation by using a large-scale paired dataset and a contrastive identity loss to balance identity fidelity and variation. Identity-consistent generationhas become an important focus intext-to-imageresearch, with recent models achieving notable success in producing images\naligned with a reference identity. Yet, the scarcity of large-scale paired\ndatasets containing multiple images of the same individual forces most\napproaches to adoptreconstruction-based training. This reliance often leads to\na failure mode we termcopy-paste, where the model directly replicates the\nreference face rather than preserving identity across naturalvariations in\npose, expression, or lighting. Such over-similarity undermines controllability\nand limits the expressive power of generation. To address these limitations, we\n(1) construct a large-scale paired datasetMultiID-2M, tailored for\nmulti-person scenarios, providing diverse references for each identity; (2)\nintroduce a benchmark that quantifies bothcopy-pasteartifacts and the\ntrade-off betweenidentity fidelityandvariation; and (3) propose a novel\ntraining paradigm with acontrastive identity lossthat leverages paired data\nto balance fidelity with diversity. These contributions culminate in\nWithAnyone, adiffusion-based modelthat effectively mitigatescopy-pastewhile\npreserving high identity similarity. Extensive qualitative and quantitative\nexperiments demonstrate that WithAnyone significantly reducescopy-pasteartifacts, improves controllability over pose and expression, and maintains\nstrongperceptual quality. User studies further validate that our method\nachieves highidentity fidelitywhile enabling expressive controllable\ngeneration. Identity-consistent generation has become an important focus in text-to-image research, with recent models achieving notable success in producing images aligned with a reference identity. Yet, the scarcity of large-scale paired datasets containing multiple images of the same individual forces most approaches to adopt reconstruction-based training. This reliance often leads to a failure mode we term copy-paste, where the model directly replicates the reference face rather than preserving identity across natural variations in pose, expression, or lighting. Such over-similarity undermines controllability and limits the expressive power of generation. To address these limitations, we (1) construct a large-scale paired dataset MultiID-2M, tailored for multi-person scenarios, providing diverse references for each identity; (2) introduce a benchmark that quantifies both copy-paste artifacts and the trade-off between identity fidelity and variation; and (3) propose a novel training paradigm with a contrastive identity loss that leverages paired data to balance fidelity with diversity. These contributions culminate in WithAnyone, a diffusion-based model that effectively mitigates copy-paste while preserving high identity similarity. Extensive qualitative and quantitative experiments demonstrate that WithAnyone significantly reduces copy-paste artifacts, improves controllability over pose and expression, and maintains strong perceptual quality. User studies further validate that our method achieves high identity fidelity while enabling expressive controllable generation.  This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2542013",
    "title": "WithAnyone: Towards Controllable and ID Consistent Image Generation",
    "authors": [
      "Hengyuan Xu",
      "Wei Cheng",
      "Peng Xing",
      "Yixiao Fang",
      "Gang Yu"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/Doby-Xu/WithAnyone",
    "huggingface_url": "https://huggingface.co/papers/2510.14975",
    "upvote": 84
  }
}