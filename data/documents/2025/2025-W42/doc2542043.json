{
  "context": "Information Gain-based Policy Optimization (IGPO) enhances multi-turn reasoning in large language models by providing dense intrinsic rewards derived from the model's belief updates, improving accuracy and sample efficiency. Large language model(LLM)-based agents are increasingly trained withreinforcement learning(RL) to enhance their ability to interact with external\nenvironments throughtool use, particularly in search-based settings that\nrequiremulti-turn reasoningandknowledge acquisition. However, existing\napproaches typically rely on outcome-based rewards that are only provided at\nthe final answer. Thisreward sparsitybecomes particularly problematic in\nmulti-turn settings, where long trajectories exacerbate two critical issues:\n(i)advantage collapse, where all rollouts receive identical rewards and\nprovide no useful learning signals, and (ii) lack of fine-grained credit\nassignment, where dependencies between turns are obscured, especially in\nlong-horizon tasks. In this paper, we propose Information Gain-based Policy\nOptimization (IGPO), a simple yet effective RL framework that provides dense\nand intrinsic supervision for multi-turn agent training. IGPO models each\ninteraction turn as an incremental process of acquiring information about the\nground truth, and defines turn-level rewards as the marginal increase in the\npolicy's probability of producing the correct answer. Unlike prior\nprocess-level reward approaches that depend on external reward models or costly\nMonte Carlo estimation, IGPO derivesintrinsic rewardsdirectly from the\nmodel's ownbelief updates. These intrinsic turn-level rewards are combined\nwith outcome-level supervision to formdense reward trajectories. Extensive\nexperiments on both in-domain and out-of-domain benchmarks demonstrate that\nIGPO consistently outperforms strong baselines in multi-turn scenarios,\nachieving higher accuracy and improvedsample efficiency. Our main contributions can be summarized as follows: (1) We analyze the phenomenon of advantage collapse in outcome-reward–based optimization, and reveal the inefficiency of existing processlevel rewards due to reliance on external knowledge or high-variance estimation. (2) We propose IGPO, a simple yet effective policy optimization framework that leverages turn-level information gain to provide dense, ground-truth-aware supervision while preserving outcome-level alignment. (3) Comprehensive experiments demonstrate that IGPO outperforms strong baselines across multiple benchmarks and significantly improves sample efficiency, especially for smaller models. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend ·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2542043",
    "title": "Information Gain-based Policy Optimization: A Simple and Effective\n  Approach for Multi-Turn LLM Agents",
    "authors": [],
    "publication_year": 2025,
    "github_url": "https://github.com/GuoqingWang1/IGPO",
    "huggingface_url": "https://huggingface.co/papers/2510.14967",
    "upvote": 33
  }
}