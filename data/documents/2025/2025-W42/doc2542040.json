{
  "context": "RLFR uses flow rewards derived from latent space to improve reinforcement learning with verifiable rewards, demonstrating reliable reward shaping and efficient context comprehension. Reinforcement Learning with Verifiable Rewards(RLVR) has recently emerged as\na promising framework for improving reasoning abilities in Large Language\nModels (LLMs). However, policy optimized with binary verification prone to\noverlook potential valuable exploration in reasoning trajectory. In view of\nheavy annotation cost of goldenProcess Reward Models(PRMs), recent works\nattempt using auxiliary signals forreward shapingofprocess tokens, involving\nentropy and likelihood collected fromlogit space. In this work, we offer a\nnovel perspective on shapingRLVRwithflow rewardsderived fromlatent space,\nand propose RLFR, where theflow fieldsof model latents are constructed from\neither off-policy high-quality data andon-policy rejection samplingdata, and\nthevelocity deviationsof policy latents within it are quantified to serve as\na reward signal. RLFR first demonstrates that a well-established flow field can\nbe a sound environment for reward signal collection, highlighting the\nexpressivelatent spaceis much underexplored. Moreover, RLFR is able to\ncompress any off-policy expert data as reference for constituting reward\nsignals, and we show that the efficient context dependence compressed within\nthehidden statesare utilized, rather than individual token-level denotation\nfor context comprehending. Experiments on both language and multimodal\nreasoning benchmarks demonstrate the reliability offlow rewards, and\nsuggesting a promising paradigm forreward shapingwith auxiliary signals. Project Page:https://jinghaoleven.github.io/RLFR/Github:https://github.com/Jinghaoleven/RLFRHugging face:https://huggingface.co/collections/JingHaoZ/rlfr-68e9046eaeb8207e868a4f02 This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2542040",
    "title": "RLFR: Extending Reinforcement Learning for LLMs with Flow Environment",
    "authors": [
      "Jinghao Zhang"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/Jinghaoleven/RLFR",
    "huggingface_url": "https://huggingface.co/papers/2510.10201",
    "upvote": 35
  }
}