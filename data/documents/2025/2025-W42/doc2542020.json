{
  "context": "Attention mechanisms in LLMs are analyzed to reveal reasoning patterns, leading to novel RL strategies that improve performance by focusing on critical tokens. The reasoning pattern ofLarge language models(LLMs) remains opaque, andReinforcement learning(RL) typically applies uniform credit across an entire\ngeneration, blurring the distinction between pivotal and routine steps. This\nwork positions attention as a privileged substrate that renders the internal\nlogic of LLMs legible, not merely as a byproduct of computation, but as a\nmechanistic blueprint of reasoning itself. We first distinguishattention headsbetween locally andglobally focusedinformation processing and reveal thatlocally focusedheads produce a sawtooth pattern near the diagonal indicating\nphrasal chunks, whileglobally focusedheads expose tokens that exert broad\ndownstream influence over future tokens. We formalize these with two metrics:\n1)Windowed Average Attention Distance, which measures the extent of backward\nattention within a clipped window; 2)Future Attention Influence, which\nquantifies a token's global importance as the average attention it receives\nfrom subsequent tokens. Taken together, these signals reveal a recurring\npreplan-and-anchor mechanism, where the model first performs a long-range\ncontextual reference to generate an introductory token, which is immediately\nfollowed by or coincides with a semantic anchor token that organizes subsequent\nreasoning. Leveraging these insights, we introduce three novel RL strategies\nthat dynamically performtargeted credit assignmentto critical nodes (preplan\ntokens,anchor tokens, and their temporal coupling) and show consistent\nperformance gains across various reasoning tasks. By aligning optimization with\nthe model's intrinsic reasoning rhythm, we aim to transform opaque optimization\ninto an actionable structure-aware process, hoping to offer a potential step\ntoward more transparent and effective optimization of LLM reasoning. üî• Core Summary:üîπ Redefining the Role of Attention: Attention is not just a byproduct of language model computations but a structured blueprint that reveals the underlying logic of reasoning. By analyzing attention patterns, we can more clearly capture the model's \"thought process\" in information integration and sequence generation, providing an interpretable framework for a reasoning process that is largely still a black box, helping to make the model's decision-making more transparent.üîπ Revolutionizing RL Algorithms: By aligning optimization objectives with the model's intrinsic reasoning rhythm, we transform traditional sequence-level rewards, which are uniformly distributed at the token level, into dynamic reward assignments that are structurally aware of the reasoning process. This mechanism dynamically identifies and strengthens key reasoning steps, driving model optimization into a more transparent, finer, and more efficient paradigm. üß† Key Reasoning Patterns Revealed by Attentionüîπ Local Chunking: Local attention exhibits a typical near-diagonal sawtooth pattern, reflecting the model's dense internal construction at the \"chunk\" level. At chunk boundaries, the model performs long-range contextual retrieval (often accompanied by higher token entropy), and subsequent generation is often guided by this reference.üîπ Global Anchor Planning: Global attention identifies sparse but crucial core anchor tokens, which have broad global influence over subsequent tokens, frequently referenced back by later tokens. Experiments show that perturbing these anchors significantly alters the subsequent reasoning path.üîπ Preplan-Anchor Coupling Mechanism: A stable temporal coupling exists between local foresight signals and global anchor signals, forming a recurring reasoning rhythm: the model first generates a guiding token as a \"preplan\", followed by anchoring a core semantic node, thus systematically organizing the subsequent reasoning process. ‚öôÔ∏è RL Algorithm Innovation: From Uniform Rewards to Structure-Aware Credit AssignmentTraditional sequence-level rewards are evenly distributed at the token level, ignoring key nodes in the reasoning structure. We propose a dynamic credit redistribution mechanism based on attention rhythm, aligning the optimization process with the model's intrinsic reasoning structure. Specifically, we implement three strategies:üîπ Preplan Guidance Strategy: Strengthens tokens that guide local chunk construction, improving long-range context referencing ability.üîπ Anchor Enhancement Strategy: Focuses on optimizing semantic anchors with global influence to enhance reasoning planning.üîπ Coupling Alignment Strategy: Strengthens the temporal coordination between preplanning and anchors, promoting a structured reasoning process. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend ¬∑Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2542020",
    "title": "Attention Illuminates LLM Reasoning: The Preplan-and-Anchor Rhythm\n  Enables Fine-Grained Policy Optimization",
    "authors": [
      "Yang Li",
      "Yijia Luo",
      "Han Lu"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2510.13554",
    "upvote": 57
  }
}