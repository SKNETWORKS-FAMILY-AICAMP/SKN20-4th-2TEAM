{
  "context": "GIR-Bench evaluates unified multimodal models across understanding-generation consistency, reasoning-centric text-to-image generation, and multi-step reasoning in editing, highlighting gaps in their capabilities. Unified multimodal modelsintegrate the reasoning capacity of large language\nmodels with bothimage understandingand generation, showing great promise for\nadvancedmultimodal intelligence. However, the community still lacks a rigorousreasoning-centric benchmarkto systematically evaluate the alignment between\nunderstanding and generation, and their generalization potential in complex\nvisual tasks. To this end, we introduce GIR-Bench, a comprehensive\nbenchmark that evaluates unified models across three complementary\nperspectives. Firstly, we investigateunderstanding-generation consistency(GIR-Bench-UGC), asking whether models can consistently leverage the same\nknowledge in both understanding and generation tasks. Secondly, we investigate\nwhether models can perform reasoning-centrictext-to-image generationthat\nrequires applying logical constraints and implicit knowledge to generate\nfaithful visual content (GIR-Bench-T2I). Thirdly, we evaluate whether models\ncan handlemulti-step reasoningin editing (GIR-Bench-Edit). For each subset,\nwe carefully design different task-specificevaluation pipelinestailored for\neach task. This enables fine-grained and interpretable evaluation while\nmitigating biases from the prevalentMLLM-as-a-Judge paradigm. Extensive\nablations over various unified models and generation-only systems have shown\nthat: Although unified models are more capable of reasoning-driven visual\ntasks, they still exhibit a persistent gap between understanding and\ngeneration. The data and code for GIR-Bench are available at\nhttps://hkust-longgroup.github.io/GIR-Bench{https://hkust-longgroup.github.io/GIR-Bench}. Unified multimodal models integrate the reasoning capacity of large language models with both image understanding and generation, showing great promise for advanced multimodal intelligence. However, the community still lacks a rigorous reasoning-centric benchmark to systematically evaluate the alignment between understanding and generation, and their generalization potential in complex visual tasks. To this end, we introduce \\textbf{GIR-Bench}, a comprehensive benchmark that evaluates unified models across three complementary perspectives. Firstly, we investigate understanding-generation consistency (GIR-Bench-UGC), asking whether models can consistently leverage the same knowledge in both understanding and generation tasks. Secondly, we investigate whether models can perform reasoning-centric text-to-image generation that requires applying logical constraints and implicit knowledge to generate faithful visual content (GIR-Bench-T2I). Thirdly, we evaluate whether models can handle multi-step reasoning in editing (GIR-Bench-Edit). For each subset, we carefully design different task-specific evaluation pipelines tailored for each task. This enables fine-grained and interpretable evaluation while mitigating biases from the prevalent MLLM-as-a-Judge paradigm. Extensive ablations over various unified models and generation-only systems have shown that: Although unified models are more capable of reasoning-driven visual tasks, they still exhibit a persistent gap between understanding and generation. Reasoning-centric evaluation of multimodal unified models across Understanding â€“ Generation Consistency (UGC), Text-to-Image, and Editing, revealing the persistent gap between reasoning and faithful generation.ðŸ”— Project Page:https://hkust-longgroup.github.io/GIR-Bench/ðŸ’» Github Code:https://github.com/HKUST-LongGroup/GIR-Bench/tree/main?tab=readme-ov-fileðŸ“Š HF Datasets:https://huggingface.co/datasets/lihxxx/GIR-BenchðŸŒŸ We warmly welcome the community to follow, use, and contribute to our benchmark! This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2542079",
    "title": "GIR-Bench: Versatile Benchmark for Generating Images with Reasoning",
    "authors": [
      "Hongxiang Li",
      "Bin Lin"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/HKUST-LongGroup/GIR-Bench",
    "huggingface_url": "https://huggingface.co/papers/2510.11026",
    "upvote": 17
  }
}