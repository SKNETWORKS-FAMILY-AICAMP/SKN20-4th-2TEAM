{
  "context": "LaSeR, a reinforcement learning algorithm, enhances Large Language Models by aligning last-token self-rewarding scores with verifier-based reasoning rewards, improving reasoning performance and inference-time scaling. Reinforcement Learning with Verifiable Rewards(RLVR) has recently emerged as\na core paradigm for enhancing the reasoning capabilities of Large Language\nModels (LLMs). To address the lack of verification signals at test time, prior\nstudies incorporate the training of model'sself-verificationcapability into\nthe standardRLVRprocess, thereby unifying reasoning and verification\ncapabilities within a single LLM. However, previous practice requires the LLM\nto sequentially generate solutions andself-verifications using two separate\nprompt templates, which significantly reduces efficiency. In this work, we\ntheoretically reveal that the closed-form solution to the RL objective ofself-verificationcan be reduced to a remarkably simple form: the true\nreasoning reward of a solution is equal to itslast-token self-rewardingscore,\nwhich is computed as the difference between thepolicy model's next-token\nlog-probability assigned to any pre-specified token at the solution's last\ntoken and a pre-calculated constant, scaled by theKL coefficient. Based on\nthis insight, we propose LaSeR (Reinforcement Learning with Last-Token\nSelf-Rewarding), an algorithm that simply augments the originalRLVRloss with\naMSE lossthat aligns thelast-token self-rewardingscores with verifier-based\nreasoning rewards, jointly optimizing the reasoning and self-rewarding\ncapabilities of LLMs. The optimized self-rewarding scores can be utilized in\nboth training and testing to enhance model performance. Notably, our algorithm\nderives these scores from the predicted next-token probability distribution of\nthe last token immediately after generation, incurring only the minimal extra\ncost of one additional token inference. Experiments show that our method not\nonly improves the model'sreasoning performancebut also equips it with\nremarkable self-rewarding capability, thereby boosting its inference-time\nscaling performance. ðŸ”¥ðŸ”¥We propose LaSeR, a lightweight and effective algorithm that simultaneously optimizes both the reasoning and self-rewarding capabilities of LLMs with minimal additional cost, by introducing a simple MSE loss into the standard RLVR objective. The optimized self-rewarding scores can serve as auxiliary reward signals in both training and testing stages to enhance model performance. Github repo:https://github.com/RUCBM/LaSeR Models:https://huggingface.co/collections/Keven16/laser-68eddd427d58817e2b09373a This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2542035",
    "title": "LaSeR: Reinforcement Learning with Last-Token Self-Rewarding",
    "authors": [
      "Wenkai Yang"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/RUCBM/LaSeR",
    "huggingface_url": "https://huggingface.co/papers/2510.14943",
    "upvote": 39
  }
}