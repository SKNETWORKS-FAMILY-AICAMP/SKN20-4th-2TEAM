{
  "context": "PsiloQA, a multilingual dataset with span-level hallucinations, enhances hallucination detection in large language models across 14 languages using an automated pipeline and encoder-based models. Hallucination detectionremains a fundamental challenge for the safe and\nreliable deployment oflarge language models(LLMs), especially in applications\nrequiring factual accuracy. Existing hallucination benchmarks often operate at\nthe sequence level and are limited to English, lacking the fine-grained,multilingualsupervision needed for a comprehensive evaluation. In this work,\nwe introducePsiloQA, a large-scale,multilingualdataset annotated withspan-level hallucinationsacross 14 languages.PsiloQAis constructed through\nan automated three-stage pipeline: generating question-answer pairs from\nWikipedia usingGPT-4o, eliciting potentially hallucinated answers from diverse\nLLMs in a no-context setting, and automatically annotating hallucinated spans\nusingGPT-4oby comparing against golden answers and retrieved context. We\nevaluate a wide range ofhallucination detectionmethods -- includinguncertainty quantification,LLM-based tagging, and fine-tunedencoder models--\nand show that encoder-based models achieve the strongest performance across\nlanguages. Furthermore,PsiloQAdemonstrates effective cross-lingual\ngeneralization and supports robustknowledge transferto other benchmarks, all\nwhile being significantly more cost-efficient than human-annotated datasets.\nOur dataset and results advance the development of scalable, fine-grainedhallucination detectioninmultilingualsettings. Hallucination detection remains a fundamental challenge for the safe and reliable deployment of large language models (LLMs), especially in applications requiring factual accuracy. Existing hallucination benchmarks often operate at the sequence level and are limited to English, lacking the fine-grained, multilingual supervision needed for a comprehensive evaluation. In this work, we introduce PsiloQA, a large-scale, multilingual dataset annotated with span-level hallucinations across 14 languages. PsiloQA is constructed through an automated three-stage pipeline: generating question‚Äìanswer pairs from Wikipedia using GPT-4o, eliciting potentially hallucinated answers from diverse LLMs in a no-context setting, and automatically annotating hallucinated spans using GPT-4o by comparing against golden answers and retrieved context. We evaluate a wide range of hallucination detection methods ‚Äì including uncertainty quantification, LLM-based tagging, and fine-tuned encoder models ‚Äì and show that encoder-based models achieve the strongest performance across languages. Furthermore, PsiloQA demonstrates effective cross-lingual generalization and supports robust knowledge transfer to other benchmarks, all while being significantly more cost-efficient than human-annotated datasets. Our dataset and results advance the development of scalable, fine-grained hallucination detection in multilingual settings. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend arXiv explained breakdown of this paper üëâhttps://arxivexplained.com/papers/when-models-lie-we-learn-multilingual-span-level-hallucination-detection-with-psiloqa It's strange why this paper received over 100 upvotes, as it seems to be addressing a relatively niche area compared to other papersÔºü Hallucination detection is pretty much at the core of any application of LLM, imo so this topic is quite important. If we cannot be sure about reliability of the generation we can't really use it. But I agree that other papers of the day were really strong and possibly technically more elaborated. how are test sets proposed in this dataset different than the  30 language (silver and gold) test datasets proposed in the paperhttps://arxiv.org/abs/2502.12769? It would be interesting to compare the dataset and the approaches used for hallucination detection. Thank you very much for sharing this paper, very interesting work. In our PsiloQA dataset, the key difference is that we focus on an automatic multilingual data-generation methodology for span-level hallucination detector training. Instead of translating an existing dataset, we generate entirely new hallucinated examples via a variety of language models and then annotate them.  By contrast, in mFAVA, the approach is to start from an English dataset, translate it (hence noisy training data), and for evaluation synthesize or annotate hallucinations (silver vs gold) across many languages. Thus, our work targets more ‚Äúin the wild‚Äù hallucinations created by diverse models, while this work emphasises broad multilingual coverage via translation and controlled synthetic construction. ¬∑Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2542007",
    "title": "When Models Lie, We Learn: Multilingual Span-Level Hallucination\n  Detection with PsiloQA",
    "authors": [
      "Elisei Rykov",
      "Vasily Konovalov",
      "Julia Belikova"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/s-nlp/PsiloQA",
    "huggingface_url": "https://huggingface.co/papers/2510.04849",
    "upvote": 114
  }
}