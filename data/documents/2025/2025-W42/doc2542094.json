{
  "context": "A method called DISCO selects samples with the greatest model disagreements to predict performance, achieving state-of-the-art results across various benchmarks with reduced computational cost. Evaluating modern machine learning models has become prohibitively expensive.\nBenchmarks such as LMMs-Eval and HELM demand thousands of GPU hours per model.\nCostly evaluation reduces inclusivity, slows the cycle of innovation, and\nworsens environmental impact. The typical approach follows two steps. First,\nselect an anchor subset of data. Second, train a mapping from the accuracy on\nthis subset to the final test result. The drawback is that anchor selection\ndepends on clustering, which can be complex and sensitive to design choices. We\nargue that promoting diversity among samples is not essential; what matters is\nto select samples that maximise diversity in model responses. Our\nmethod,Diversifying Sample Condensation (DISCO), selects the top-k\nsamples with the greatestmodel disagreements. This uses greedy, sample-wise\nstatistics rather than global clustering. The approach is conceptually simpler.\nFrom a theoretical view, inter-model disagreement provides aninformation-theoretically optimal rulefor suchgreedy selection.\nDISCO shows empirical gains over prior methods, achieving\nstate-of-the-art results in performance prediction acrossMMLU,Hellaswag,Winogrande, andARC. Code is available here:\nhttps://github.com/arubique/disco-public. How to evaluate your LLMs on benchmarks like MMLU at 1% cost? The answer is in our new paper, where we show that outputs on a small subset of test samples that maximise diversity in model responses are very predictive of the full dataset performance. Project page:https://arubique.github.io/disco-site/Paper:https://arxiv.org/abs/2510.07959Code:https://github.com/arubique/disco-public Big thanks to my co-authors Benjamin Raible, Martin Gubri, and Seong Joon Oh This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2542094",
    "title": "DISCO: Diversifying Sample Condensation for Efficient Model Evaluation",
    "authors": [
      "Martin Gubri"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/arubique/disco-public",
    "huggingface_url": "https://huggingface.co/papers/2510.07959",
    "upvote": 14
  }
}