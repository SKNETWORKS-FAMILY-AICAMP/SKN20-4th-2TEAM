{
  "context": "VideoReward Thinker enhances multimodal reward models with visual reasoning operations and a configurable memory window, improving accuracy on video preference benchmarks. Recent advancements inmultimodal reward models(RMs) have substantially\nimproved post-training forvisual generative models. However, current RMs face\ninherent limitations: (1) visual inputs consume large context budgets, forcing\nfewer frames and causing loss of fine-grained details; and (2) all visual\ninformation is packed into the initial prompt, exacerbating hallucination and\nforgetting during chain-of-thought reasoning. To overcome these issues, we\nintroduce VideoReward Thinker (VR-Thinker), a thinking-with-image framework\nthat equips the RM withvisual reasoning operations(e.g., select frame) and a\nconfigurablevisual memory window. This allows the RM to actively acquire and\nupdate visual evidence within context limits, improving reasoning fidelity and\nreliability. We activate visual reasoning via areinforcement fine-tuningpipeline: (i) Cold Start withcurated visual chain-of-thought datato distill\nbasic reasoning skills and operation formatting; (ii) select samples whose\nper-dimension and overall judgments are all correct, then conduct Rejection\nsampling Fine-Tuning on these high-quality traces to further enhance reasoning;\nand (iii) applyGroup Relative Policy Optimization(GRPO) to strengthen\nreasoning. Our approach delivers state-of-the-art accuracy among open-source\nmodels on video preference benchmarks, especially for longer videos: a 7B\nVR-Thinker achieves 80.5% onVideoGen Reward, 82.3% onGenAI-Bench, and 75.6%\nonMJ-Bench-Video. These results validate the effectiveness and promise of\nthinking-with-image multimodal reward modeling. Recent advancements in multimodal reward models (RMs) have substantially improved posttraining for visual generative models. However, current RMs face inherent limitations: (1) visualinputs consume large context budgets, forcing fewer frames and causing loss of fine-grained details;and (2) all visual information is packed into the initial prompt, exacerbating hallucination andforgetting during chain-of-thought reasoning. To overcome these issues, we introduce VideoRewardThinker (VR-Thinker), a thinking-with-image framework that equips the RM with visual reasoningoperations (e.g., select frame) and a configurable visual memory window. This allows the RM toactively acquire and update visual evidence within context limits, improving reasoning fidelity andreliability. We activate visual reasoning via a reinforcement fine-tuning pipeline: (i) Cold Start withcurated visual chain-of-thought data to distill basic reasoning skills and operation formatting; (ii)select samples whose per-dimension and overall judgments are all correct, then conduct Rejectionsampling Fine-Tuning on these high-quality traces to further enhance reasoning; and (iii) applyGroup Relative Policy Optimization (GRPO) to strengthen reasoning. Our approach delivers state-of-the-art accuracy among open-source models on video preference benchmarks, especially for longervideos: a 7B VR-Thinker achieves 80.5% on VideoGen Reward, 82.3% on GenAI-Bench, and 75.6%on MJ-Bench-Video. These results validate the effectiveness and promise of thinking-with-imagemultimodal reward modeling. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2542077",
    "title": "VR-Thinker: Boosting Video Reward Models through Thinking-with-Image\n  Reasoning",
    "authors": [
      "Qunzhong Wang"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/qunzhongwang/vr-thinker",
    "huggingface_url": "https://huggingface.co/papers/2510.10518",
    "upvote": 18
  }
}