{
  "context": "Generative Universal Verifier enhances multimodal reasoning by providing reliable visual verification through ViVerBench, OmniVerifier-7B, and OmniVerifier-TTS, improving generation and refinement capabilities. We introduceGenerative Universal Verifier, a novel concept and plugin\ndesigned for next-generationmultimodal reasoninginvision-language modelsand\nunified multimodal models, providing the fundamental capability of reflection\nand refinement on visual outcomes during the reasoning and generation process.\nThis work makes three main contributions: (1) We buildViVerBench, a\ncomprehensive benchmark spanning 16 categories of critical tasks for evaluating\nvisual outcomes inmultimodal reasoning. Results show that existing VLMs\nconsistently underperform across these tasks, underscoring a substantial gap\nfrom human-level capability in reliablevisual verification. (2) We design two\nautomated pipelines to construct large-scalevisual verificationdata and trainOmniVerifier-7B, the first omni-capable generative verifier trained for\nuniversalvisual verificationand achieves notable gains onViVerBench(+8.3).\nThrough training, we identify threeatomic capabilitiesinvisual verificationand demonstrate how they generalize and interact synergistically. (3) We\nproposeOmniVerifier-TTS, asequential test-time scalingparadigm that\nleverages the universal verifier to bridge image generation and editing within\nunified models, enhancing the upper bound of generative ability through\niterative fine-grained optimization. Beyond generation, we extend universal\nverifier to broader world-modeling interleaved reasoning scenarios.\nEmpirically,OmniVerifier-TTSachieves improvements onT2I-ReasonBench(+3.7),\nandGenEval++(+4.3), outperforming existing parallel test-time scaling methods,\nsuch as Best-of-N. By endowingmultimodal reasoningwith reliable visual\nverification, OmniVerifier advances both reliable reflection during generation\nand scalable test-time refinement, marking a step toward more trustworthy and\ncontrollable next-generation reasoning systems. We introduce Generative Universal Verifier, a novel concept and plugin designed for next-generation multimodal reasoning in vision-language models and unified multimodal models, providing the fundamental capability of reflection and refinement on visual outcomes during the reasoning and generation process. This work makes three main contributions: (1) We build ViVerBench, a comprehensive benchmark spanning 16 categories of critical tasks for evaluating visual outcomes in multimodal reasoning. Results show that existing VLMs consistently underperform across these tasks, underscoring a substantial gap from human-level capability in reliable visual verification. (2) We design two automated pipelines to construct large-scale visual verification data and train OmniVerifier-7B, the first omni-capable generative verifier trained for universal visual verification and achieves notable gains on ViVerBench(+8.3). Through training, we identify three atomic capabilities in visual verification and demonstrate how they generalize and interact synergistically. (3) We propose OmniVerifier-TTS, a sequential test-time scaling paradigm that leverages the universal verifier to bridge image generation and editing within unified models, enhancing the upper bound of generative ability through iterative fine-grained optimization. Beyond generation, we extend universal verifier to broader world-modeling interleaved reasoning scenarios. Empirically, OmniVerifier-TTS achieves improvements on T2I-ReasonBench(+3.7), and GenEval++(+4.3), outperforming existing parallel test-time scaling methods, such as Best-of-N. By endowing multimodal reasoning with reliable visual verification, OmniVerifier advances both reliable reflection during generation and scalable test-time refinement, marking a step toward more trustworthy and controllable next-generation reasoning systems. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2542062",
    "title": "Generative Universal Verifier as Multimodal Meta-Reasoner",
    "authors": [
      "Xinchen Zhang",
      "Ling Yang"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/Cominclip/OmniVerifier",
    "huggingface_url": "https://huggingface.co/papers/2510.13804",
    "upvote": 25
  }
}