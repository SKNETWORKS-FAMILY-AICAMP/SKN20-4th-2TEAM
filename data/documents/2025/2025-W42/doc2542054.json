{
  "context": "AT-GRPO, a tailored RL algorithm for multi-agent systems, significantly enhances performance across various tasks by addressing unique challenges in on-policy RL. Multi-agent systems(MAS) andreinforcement learning(RL) are widely used to\nenhance the agentic capabilities oflarge language models(LLMs). MAS improves\ntask performance throughrole-based orchestration, while RL uses environmental\nrewards to learn stronger policies, such asGRPO-style optimization. However,\napplyingon-policy RLto MAS remains underexplored and presents unique\nchallenges. Algorithmically, standard GRPO grouping assumptions break down\nbecause prompts vary by role and by turn. System-wise, the training stack must\nsupport MAS-workflow rollouts and on-policy updates for bothsingle-policyandmulti-policymodels.\n  We propose AT-GRPO, which includes (i) an agent- andturn-wise grouped RLalgorithm tailored to MAS and (ii) a training system that supports both single-\nandmulti-policyregimes. Across game, planning, coding, andmath tasks,\nAT-GRPO delivers substantial gains. Onlong-horizon planning, it increases\naccuracy from a 14.0 to 47.0 percent single-agent RL baseline to 96.0 to 99.5\npercent. It also improvesreasoning performance, with average gains of 3.87 to\n7.62 percent oncoding tasksand 9.0 to 17.93 percent on math. Code and\nenvironments are available at: https://github.com/pettingllms-ai/PettingLLMs. Multi-agent systems (MAS) and reinforcement learning (RL) are widely used to enhance the agentic capabilities of large language models (LLMs). MAS improves task performance through role-based orchestration, while RL uses environmental rewards to learn stronger policies, such as GRPO-style optimization. However, applying on-policy RL to MAS remains underexplored and presents unique challenges. Algorithmically, standard GRPO grouping assumptions break down because prompts vary by role and by turn. System-wise, the training stack must support MAS-workflow rollouts and on-policy updates for both single-policy and multi-policy models. We propose AT-GRPO, which includes (i) an agent- and turn-wise grouped RL algorithm tailored to MAS and (ii) a training system that supports both single- and multi-policy regimes. Across game, planning, coding, and math tasks, AT-GRPO delivers substantial gains. On long-horizon planning, it increases accuracy from a 14.0 to 47.0 percent single-agent RL baseline to 96.0 to 99.5 percent. It also improves reasoning performance, with average gains of 3.87 to 7.62 percent on coding tasks and 9.0 to 17.93 percent on math. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2542054",
    "title": "Stronger Together: On-Policy Reinforcement Learning for Collaborative\n  LLMs",
    "authors": [
      "Yujie Zhao",
      "Lanxiang Hu"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/pettingllms-ai/PettingLLMs",
    "huggingface_url": "https://huggingface.co/papers/2510.11062",
    "upvote": 28
  }
}