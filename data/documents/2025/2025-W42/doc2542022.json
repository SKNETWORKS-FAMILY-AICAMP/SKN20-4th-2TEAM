{
  "context": "BitNet Distillation fine-tunes large language models to 1.58-bit precision using SubLN, multi-head attention distillation, and continual pre-training, achieving comparable performance with significant memory and inference speed improvements. In this paper, we presentBitNet Distillation(BitDistill), a lightweight\npipeline that fine-tunes off-the-shelf full-precisionLLMs(e.g.,Qwen) into\n1.58-bit precision (i.e.,ternary weights{-1, 0, 1}) for specific downstream\ntasks, achieving strong task-specific performance with minimal computational\ncost. Specifically,BitDistillincorporates three key techniques: theSubLNmodule, as introduced in BitNet;multi-head attention distillation, based on\nMiniLM; andcontinual pre-training, which serves as a crucial warm-up step to\nmitigate the scalability issue of the performance gap between finetuned\nfull-precision and 1.58-bitLLMson specific tasks. Experimental results show\nthatBitDistillachieves performance comparable to the full-precision\ncounterpart models across model size, while enabling up to 10xmemory savingsand 2.65x faster inference on CPUs. Code is available at\nhttps://github.com/microsoft/BitNet. In this paper, we present BitNet Distillation (BitDistill), a lightweight pipeline that fine-tunes off-the-shelf full-precision LLMs (e.g., Qwen) into 1.58-bit precision (i.e., ternary weights {-1, 0, 1}) for specific downstream tasks, achieving strong task-specific performance with minimal computational cost. Specifically, BitDistill incorporates three key techniques: the SubLN module, as introduced in BitNet; multi-head attention distillation, based on MiniLM; and continual pre-training, which serves as a crucial warm-up step to mitigate the scalability issue of the performance gap between finetuned full-precision and 1.58-bit LLMs on specific tasks. Experimental results show that BitDistill achieves performance comparable to the full-precision counterpart models across model size, while enabling up to 10x memory savings and 2.65x faster inference on CPUs. Is there any way to convert those distilled 1.58-bit weights to GGUF afterwards? Would love to experiment with this, but I need to deploy on llama.cpp :o Yes, we're actively working on this! We're developing compatibility for Qwen-like models on bitnet.cpp. You can follow our progress athttps://github.com/microsoft/BitNet This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2542022",
    "title": "BitNet Distillation",
    "authors": [
      "Li Dong"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/microsoft/BitNet",
    "huggingface_url": "https://huggingface.co/papers/2510.13998",
    "upvote": 55
  }
}