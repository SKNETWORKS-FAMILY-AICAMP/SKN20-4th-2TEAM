{
  "context": "D2E framework uses desktop interactions to pretrain embodied AI, achieving high success rates in physical manipulation and navigation tasks. Large language models leverage internet-scale text data, yetembodied AIremains constrained by the prohibitive costs of physical trajectory collection.Desktop environments-- particularly gaming -- offer a compelling alternative:\nthey provide richsensorimotor interactionsat scale while maintaining the\nstructured observation-action coupling essential for embodied learning. We\npresent D2E (Desktop toEmbodied AI), a framework that demonstrates desktop\ninteractions can serve as an effective pretraining substrate for roboticsembodied AItasks. Unlike prior work that remained domain-specific (e.g., VPT\nfor Minecraft) or kept data proprietary (e.g., SIMA), D2E establishes a\ncomplete pipeline from scalable desktop data collection to verified transfer in\nembodied domains. Our framework comprises three components: (1) theOWA Toolkitthat unifies diverse desktop interactions into a standardized format with 152x\ncompression, (2) theGeneralist-IDMthat achieves strong zero-shot\ngeneralization across unseen games throughtimestamp-based event prediction,\nenablinginternet-scale pseudo-labeling, and (3)VAPTthat transfers\ndesktop-pretrained representations to physical manipulation and navigation.\nUsing 1.3K+ hours of data (259 hours of human demonstrations, and 1K+ hours of\npseudo-labeled gameplay), we achieve a total of 96.6% success rate on LIBERO\nmanipulation and 83.3% onCANVAS navigationbenchmarks. This validates that\nsensorimotor primitives in digital interactions exhibit sufficient invariance\nto transfer meaningfully to physical embodied tasks, establishing desktop\npretraining as a practical paradigm for robotics. We will make all our work\npublic, including theOWA toolkit, datasets of human-collected and\npseudo-labeled, andVAPT-trained models available at\nhttps://worv-ai.github.io/d2e/ We presentD2E ğŸ®â†’ğŸ¤–, a framework that scalesVision-Action Pretrainingondesktop interaction datato accelerateEmbodied AIğŸš€.By turning ordinary game and desktop interactions into training fuel,D2Ebuilds richvisuomotor priorsthat transfer from screens to robots âœ¨OWA Toolkit ğŸ–¥ï¸â€” a unified recorder + storage format formulti-modal desktop data(screen, keyboard, mouse).OWAcompresses raw gameplay into a compactOWAMcapformat â€” achieving152Ã— storage efficiencywhile preservingtemporal precisionâš¡. ğŸ§ Generalist-IDMâ€” a universal inverse dynamics model predictingnext-event tokenspurely fromtimestampsâ±ï¸.It generalizes to unseen games and enablespseudo-labeling of 1,055 h of YouTube gameplay, trained with the dataset far beyond259 h of human-recorded data across 20 gamesğŸ®ğŸ“Š. ğŸ”¬VAPT (Vision-Action PreTraining)â€” pretraining a1B-param InternVL3 backboneon our1.3K hours dataset, then transferring toreal-world robot domainsğŸ¦¾. ğŸ¤– When transferred to embodied domains,D2Eachieves96.6 %ğŸ”¥ success on LIBERO-manipulationand83.3 %ğŸ”¥ on CANVAS-navigation, demonstratingstrong generalizationfrom desktop to real-world tasks. ğŸŒD2Edemonstrates thatdesktop-scale learningcan unlocklow-cost, high-transfer embodied intelligenceand enableinternet-scale embodied ai pretraining, bringing the gap between thedigital and physical worlds. ğŸ“„Paper:https://arxiv.org/abs/2510.05684ğŸ’»Project:https://worv-ai.github.io/d2e/ This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend arXiv explained breakdown of this paper ğŸ‘‰https://arxivexplained.com/papers/d2e-scaling-vision-action-pretraining-on-desktop-data-for-transfer-to-embodied-ai Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2542004",
    "title": "D2E: Scaling Vision-Action Pretraining on Desktop Data for Transfer to\n  Embodied AI",
    "authors": [
      "Suwhan Choi",
      "Jaeyoon Jung",
      "Haebin Seong",
      "Minchan Kim",
      "Yongjun Cho",
      "Yunsung Lee"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/worv-ai/D2E",
    "huggingface_url": "https://huggingface.co/papers/2510.05684",
    "upvote": 141
  }
}