{
  "context": "MR-ALIGN, a Meta-Reasoning informed alignment framework, enhances the factuality of large reasoning models by aligning their reasoning process, improving accuracy and reducing misleading reasoning. Large reasoning models(LRMs) show strong capabilities in complex reasoning,\nyet their marginal gains on evidence-dependent factual questions are limited.\nWe find this limitation is partially attributable to a reasoning-answer hit\ngap, where the model identifies the correct facts during reasoning but fails to\nincorporate them into the final response, thereby reducing factual fidelity. To\naddress this issue, we proposeMR-ALIGN, aMeta-Reasoninginformed alignment\nframework that enhances factuality without relying on external verifiers.MR-ALIGNquantifiesstate transition probabilitiesalong the model's thinking\nprocess and constructs atransition-aware implicit rewardthat reinforces\nbeneficial reasoning patterns while suppressing defective ones at the atomic\nthinking segments. This re-weighting reshapestoken-level signalsintoprobability-aware segment scores, encouragingcoherent reasoning trajectoriesthat are more conducive to factual correctness. Empirical evaluations across\nfourfactual QA datasetsand onelong-form factuality benchmarkshow thatMR-ALIGNconsistently improves accuracy and truthfulness while reducing\nmisleading reasoning. These results highlight that aligning the reasoning\nprocess itself, rather than merely the outputs, is pivotal for advancing\nfactuality in LRMs. Github Repo:https://github.com/gudehhh666/MR-ALIGN Large reasoning models (LRMs) show strong capabilities in complex reasoning, yet their marginal gains on evidence-dependent factual questions are limited. We find this limitation is partially attributable to a reasoning-answer hit gap, where the model identifies the correct facts during reasoning but fails to incorporate them into the final response, thereby reducing factual fidelity. To address this issue, we propose MR-ALIGN, a Meta-Reasoning informed alignment framework that enhances factuality without relying on external verifiers. MR-ALIGN quantifies state transition probabilities along the model's thinking process and constructs a transition-aware implicit reward that reinforces beneficial reasoning patterns while suppressing defective ones at the atomic thinking segments. This re-weighting reshapes token-level signals into probability-aware segment scores, encouraging coherent reasoning trajectories that are more conducive to factual correctness. Empirical evaluations across four factual QA datasets and one long-form factuality benchmark show that MR-ALIGN consistently improves accuracy and truthfulness while reducing misleading reasoning. These results highlight that aligning the reasoning process itself, rather than merely the outputs, is pivotal for advancing factuality in LRMs. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2545024",
    "title": "MR-Align: Meta-Reasoning Informed Factuality Alignment for Large\n  Reasoning Models",
    "authors": [
      "Xinming Wang",
      "Hongzhu Yi",
      "Yi Chen"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/gudehhh666/MR-ALIGN",
    "huggingface_url": "https://huggingface.co/papers/2510.24794",
    "upvote": 31
  }
}