{
  "context": "Sequential scaling in language model reasoning outperforms parallel scaling across multiple models and benchmarks, with inverse-entropy weighted voting further enhancing accuracy. We revisittest-time scalingforlanguage model reasoningand ask a\nfundamental question: at equaltoken budgetandcompute, is it better to run\nmultipleindependent chainsin parallel, or to run fewer chains that\niteratively refine throughsequential steps? Through comprehensive evaluation\nacross 5 state-of-the-art open source models and 3 challenging reasoning\nbenchmarks, we find thatsequential scalingwhere chains explicitly build upon\nprevious attempts consistently outperforms the dominant parallel\nself-consistency paradigm in 95.6% of configurations with gains in accuracy\nupto 46.7%. Further, we introduceinverse-entropy weighted voting, a novel\ntraining-free method to further boost the accuracy ofsequential scaling. By\nweighing answers in proportion to the inverse entropy of their reasoning\nchains, we increase our success rate over parallel majority and establish it as\nthe optimaltest-time scalingstrategy. Our findings fundamentally challenge\ntheparallel reasoning orthodoxythat has dominatedtest-time scalingsince\nWang et al.'s self-consistency decoding (Wang et al., 2022), positioning\nsequential refinement as the robust default for modern LLM reasoning and\nnecessitating a paradigm shift in how we approachinference-time optimization. Our paper challenges - majority voting - the conventional method of increasing accuracy on reasoning benchmarks. We show that sequential voting, where reasoning chains are generated sequentially not parallely, can double the performance on reasoning benchmarks such as AIME-2025 and GPQA-Diamond without using additional tokens or chains. Using our method, we achieve upto 80% performance on GPQA-Daimond. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2545084",
    "title": "The Sequential Edge: Inverse-Entropy Voting Beats Parallel\n  Self-Consistency at Matched Compute",
    "authors": [],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2511.02309",
    "upvote": 4
  }
}