{
  "context": "Theoretical analysis proves the existence of strong lottery tickets within multi-head attention mechanisms and extends the strong lottery ticket hypothesis to transformers without normalization layers. Thestrong lottery ticket hypothesis(SLTH) conjectures that high-performing\nsubnetworks, calledstrong lottery tickets(SLTs), are hidden in randomly\ninitialized neural networks. Although recent theoretical studies have\nestablished the SLTH across various neural architectures, the SLTH for\ntransformer architectures still lacks theoretical understanding. In particular,\nthe current theory of the SLTH does not yet account for the multi-head\nattention (MHA) mechanism, a core component oftransformers. To address this\ngap, we introduce a theoretical analysis of the existence of SLTs within MHAs.\nWe prove that, if a randomly initialized MHA of H heads and input dimension\nd has the hidden dimension O(dlog(Hd^{3/2})) for the key and value, it\ncontains an SLT that approximates an arbitrary MHA with the same input\ndimension with high probability. Furthermore, by leveraging this theory for\nMHAs, we extend the SLTH totransformerswithoutnormalization layers. We\nempirically validate our theoretical findings, demonstrating that the\napproximation error between the SLT within a source model (MHA and transformer)\nand an approximate target counterpart decreases exponentially by increasing the\nhidden dimension of the source model. We extend the strong lottery ticket hypothesis to attention mechanisms and transformers. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Hey, Your paper onSLTH for attention mechanismsis brilliant. Seriously. I've been following lottery ticket research for a while, but applying it specifically tomulti-head attentionwith that level of theoretical depth? That's fresh. What I love most is that this isn't just theoretical elegance — it'sactionable. But I'm trying to wrap my head around the full training procedure, and I'd love your insight. From my understanding, the training flow would look something like: Question:Do you use any special attention dropout or regularization here?I'm wondering if the overparameterization needs stabilization techniques. Question:Do you pruneQ, K, V, and O projectionsat the same rate, or does each need different sparsity?I imagineK/Vwith 4x dimensions can tolerate more aggressive pruning. Question:Do you find the sparse network needs the same number of training steps, or can it converge faster? The part I'm most curious about is how this interacts withmulti-head attentionspecifically.Does each head discover its own lottery ticket independently?Or do you need to maintain some kind of head-wise balance? Also wondering about thecomputational trade-offsduring training — obviously the overparameterized phase is more expensive, but if the lottery ticket is strong enough, does the sparse retraining compensate?What's thetotal training costcompared to just training a standard dense model? And one more practical question:For the4x overparameterization, is that dimension increase appliedbefore or after the head split? Like, if you have12 heads with 64-dim each (768 total), do you go to768×4 = 3072before splitting into heads, or64×4 = 256 per head? The reason I'm digging into this is that I think your approach could be agame-changerfor how we think about training large models: Train big, compress smart, deploy efficiently — but the devil's in the details of making it actually work. Looking forward to your thoughts, and seriously, keep pushing this work forward! Cheers,Ujjwal TyagiAI Researcher & Scientist, Shirova AI Interesting ·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2545042",
    "title": "The Strong Lottery Ticket Hypothesis for Multi-Head Attention Mechanisms",
    "authors": [
      "Hikari Otsuka"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/Hikari43/SLTH-for-MHAs",
    "huggingface_url": "https://huggingface.co/papers/2511.04217",
    "upvote": 16
  }
}