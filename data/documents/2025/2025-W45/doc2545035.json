{
  "context": "ToolScope, an agentic framework for multimodal large language models, enhances visual question answering by integrating external tools and achieving significant performance improvements across various benchmarks. Recently,large language models(LLMs) have demonstrated remarkable\nproblem-solving capabilities by autonomously integrating with external tools\nfor collaborative reasoning. However, due to the inherently complex and diverse\nnature of multimodal information, enablingmultimodal large language models(MLLMs) to flexibly and efficiently utilize external tools during reasoning\nremains an underexplored challenge. In this work, we introduceToolScope, an\nagentic framework designed to unify global planning with local multimodal\nperception, adopting a specializedPerceivetool to mitigates visual context\ndegradation inlong-horizon VQAtask.ToolScopecomprises three primary\ncomponents: theGlobal Navigator, theAgentic Executor, and the Response\nSynthesizer. TheGlobal Navigatorfunctions as a \"telescope\", offering\nhigh-level strategic guidance. TheAgentic Executoroperates iteratively to\naugment MLLM with local perception through the integration of external\ntools-Search,Code, andPerceive. Finally, theResponse Synthesizerconsolidates and organizes the reasoning process into a coherent, user-friendly\noutput. We evaluateToolScopeon fourVQAbenchmarks across diverse domains,\nincludingVQA 2.0,ScienceQA,MAT-SearchandMathVista. It demonstrates strong\ngeneralization capabilities, achieving an average performance improvement of up\nto +6.69% across all datasets. ðŸ‘‹ We're excited to share our work onToolScope, a training-free framework that enhances multimodal LLMs with adaptive tool use for complex visual reasoning tasks. 1. Novel Three-Phase Architecture 2. Visual Context PreservationOur dedicatedPerceive tooladdresses a critical challenge in long-horizon reasoningâ€”it enables models to dynamically re-attend to visual details, mitigating context degradation that plagues traditional approaches. 3. Strong Empirical Results 4. Plug-and-Play Design Compatible with any vLLM-supported model including InternVL3-8B, Qwen2.5-VL series, and more! ðŸ“„ Paper:https://arxiv.org/abs/2510.27363ðŸ’» Code:https://github.com/dengmengjie/ToolScope Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2545035",
    "title": "ToolScope: An Agentic Framework for Vision-Guided and Long-Horizon Tool\n  Use",
    "authors": [
      "Mengjie Deng",
      "Zhicheng Dou"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/dengmengjie/ToolScope",
    "huggingface_url": "https://huggingface.co/papers/2510.27363",
    "upvote": 22
  }
}