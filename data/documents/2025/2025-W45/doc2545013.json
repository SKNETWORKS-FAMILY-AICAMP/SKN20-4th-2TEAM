{
  "context": "MIRA is a benchmark that evaluates models using intermediate visual images to enhance reasoning, showing significant performance improvements over text-only methods. We propose MIRA, a new benchmark designed to evaluate models in scenarios\nwhere generatingintermediate visual imagesis essential for successful\nreasoning. Unlike traditionalCoT methodsthat rely solely on text, tasks in\nMIRA require models to generate and utilize intermediate images - such assketches,structural diagrams, orpath drawings- to guide their reasoning\nprocess. This setup closely mirrors how humans solve complex problems through\n\"drawing to think\". To solve this, MIRA focuses on tasks that are intrinsically\nchallenging and involve complex structures, spatial relationships, or reasoning\nsteps that are difficult to express through language alone. To ensure that our\nevaluation data is of high-quality, we include 546multimodal problems,\nannotated withintermediate visual imagesand final answers. We also propose a\nunifiedevaluation protocolfor MIRA that spans three levels of evaluation\ninput: direct input with image and question only, text-only CoT input with\nimage and thinking prompts, andVisual-CoT inputwith both annotated image\nclues and textual thinking prompts. To probe the upper bound of model capacity\non our benchmark, we also reportpass@kandmajority voting accuraciesunder\ndifferent k settings. Experimental results show that existing multimodal large\nlanguage models, including strongest private models as well as strong\nopen-weight models, perform poorly when relying solely on textual prompts.\nHowever, when intermediate visual cues are provided, model performance improves\nconsistently, yielding an average relative gain of 33.7% across all models and\ntasks. We also probe the upper bound by expanding the search space and\ndesigning textual prompts aligned with Visual-CoT, but both yield only limited\nimprovements compared to our Visual-CoT setting. These results underscore the\ncritical role of imagined visual information in enabling successful reasoning\non MIRA. We propose MIRA, a new benchmark designed to evaluate models in scenarios where generating intermediate visual images is essential for successful reasoning. Unlike traditional CoT methods that rely solely on text, tasks in MIRA require models to generate and utilize intermediate images - such as sketches, structural diagrams, or path drawings - to guide their reasoning process. This setup closely mirrors how humans solve complex problems through \"drawing to think\". To solve this, MIRA focuses on tasks that are intrinsically challenging and involve complex structures, spatial relationships, or reasoning steps that are difficult to express through language alone. To ensure that our evaluation data is of high-quality, we include 546 multimodal problems, annotated with intermediate visual images and final answers. We also propose a unified evaluation protocol for MIRA that spans three levels of evaluation input: direct input with image and question only, text-only CoT input with image and thinking prompts, and Visual-CoT input with both annotated image clues and textual thinking prompts. To probe the upper bound of model capacity on our benchmark, we also report pass@k and majority voting accuracies under different k settings. Experimental results show that existing multimodal large language models, including strongest private models as well as strong open-weight models, perform poorly when relying solely on textual prompts. However, when intermediate visual cues are provided, model performance improves consistently, yielding an average relative gain of 33.7% across all models and tasks. We also probe the upper bound by expanding the search space and designing textual prompts aligned with Visual-CoT, but both yield only limited improvements compared to our Visual-CoT setting. These results underscore the critical role of imagined visual information in enabling successful reasoning on MIRA. To evaluate visual CoT, we released RBench-V as early as May, and the related paper has been accepted by NeurIPS 2025. However, the paper does not include any comparisons or discussions regarding RBench-V, which is somewhat disappointing. For  details,:https://arxiv.org/abs/2505.16770 Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2545013",
    "title": "When Visualizing is the First Step to Reasoning: MIRA, a Benchmark for\n  Visual Chain-of-Thought",
    "authors": [],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2511.02779",
    "upvote": 58
  }
}