{
  "context": "Viewpoint Learning enhances the spatial reasoning capabilities of Multimodal Large Language Models using a two-stage fine-tuning strategy and a hybrid cold-start initialization method, improving performance on both in-domain and out-of-domain 3D reasoning tasks. Recent advances inMultimodal Large Language Models(MLLMs) have\nsignificantly improved 2D visual understanding, prompting interest in their\napplication to complex 3D reasoning tasks. However, it remains unclear whether\nthese models can effectively capture the detailed spatial information required\nfor robust real-world performance, especially cross-view consistency, a key\nrequirement for accurate 3D reasoning. Considering this issue, we introduceViewpoint Learning, a task designed to evaluate and improve the spatial\nreasoning capabilities of MLLMs. We present theViewpoint-100K dataset,\nconsisting of 100K object-centric image pairs with diverse viewpoints and\ncorresponding question-answer pairs. Our approach employs a two-stage\nfine-tuning strategy: first, foundational knowledge is injected to the baseline\nMLLM viaSupervised Fine-Tuning(SFT) on Viewpoint-100K, resulting in\nsignificant improvements across multiple tasks; second, generalization is\nenhanced throughReinforcement Learningusing the Group Relative Policy\nOptimization (GRPO) algorithm on a broader set of questions. Additionally, we\nintroduce ahybrid cold-start initializationmethod designed to simultaneously\nlearn viewpoint representations and maintain coherent reasoning thinking.\nExperimental results show that our approach significantly activates the spatial\nreasoning ability of MLLM, improving performance on both in-domain and\nout-of-domain reasoning tasks. Our findings highlight the value of developing\nfoundational spatial skills in MLLMs, supporting future progress in robotics,\nautonomous systems, and 3D scene understanding. This paper attempts to solve that current MLLMs cannot effectively capture the detailed spatial information required for robust real-world performance, especially cross-view consistency, a key requirement for accurate 3D reasoning. Furthermore, it has been accepted by NeurIPS 2025. Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2545059",
    "title": "Actial: Activate Spatial Reasoning Ability of Multimodal Large Language\n  Models",
    "authors": [
      "Wenxuan Huang"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2511.01618",
    "upvote": 10
  }
}