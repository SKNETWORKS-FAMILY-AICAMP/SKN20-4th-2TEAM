{
  "context": "A framework decomposes modality following in multimodal large language models into relative reasoning uncertainty and inherent modality preference, providing insights into how models resolve conflicting information. Multimodal large language models(MLLMs) must resolve conflicts when\ndifferent modalities provide contradictory information, a process we termmodality following. Prior work measured this behavior only with coarse\ndataset-level statistics, overlooking the influence of model's confidence in\nunimodal reasoning. In this paper, we introduce a new framework that decomposesmodality followinginto two fundamental factors:relative reasoning uncertainty(the case-specific confidence gap between unimodal predictions) and inherent\nmodality preference( a model's stable bias when uncertainties are balanced). To\nvalidate this framework, we construct a controllable dataset that\nsystematically varies the reasoning difficulty of visual and textual inputs.\nUsingentropyas a fine-grained uncertainty metric, we uncover a universal law:\nthe probability of following a modality decreases monotonically as its relative\nuncertainty increases. At the relative difficulty level where the model tends\nto follow both modalities with comparable probability what we call the balance\npoint, a practical indicator of the model's inherent preference. Unlike\ntraditional macro-level ratios, this measure offers a more principled and less\nconfounded way to characterize modality bias, disentangling it from unimodal\ncapabilities and dataset artifacts. Further, by probinglayer-wise predictions,\nwe reveal the internal mechanism of oscillation: in ambiguous regions near thebalance point, models vacillate between modalities across layers, explaining\nexternally observed indecision. Together, these findings establish relative\nuncertainty and inherent preference as the two governing principles of modality\nfollowing, offering both a quantitative framework and mechanistic insight into\nhow MLLMs resolve conflicting information. Multimodal large language models (MLLMs) must resolve conflicts when different modalities provide contradictory information, a process we term modality following. Prior work measured this behavior only with coarse dataset-level statistics, overlooking the influence of model's confidence in unimodal reasoning. In this paper, we introduce a new framework that decomposes modality following into two fundamental factors: relative reasoning uncertainty (the case-specific confidence gap between unimodal predictions) and inherent modality preference( a model's stable bias when uncertainties are balanced). To validate this framework, we construct a controllable dataset that systematically varies the reasoning difficulty of visual and textual inputs. Using entropy as a fine-grained uncertainty metric, we uncover a universal law: the probability of following a modality decreases monotonically as its relative uncertainty increases. At the relative difficulty level where the model tends to follow both modalities with comparable probability what we call the balance point, a practical indicator of the model's inherent preference. Unlike traditional macro-level ratios, this measure offers a more principled and less confounded way to characterize modality bias, disentangling it from unimodal capabilities and dataset artifacts. Further, by probing layer-wise predictions, we reveal the internal mechanism of oscillation: in ambiguous regions near the balance point, models vacillate between modalities across layers, explaining externally observed indecision. Together, these findings establish relative uncertainty and inherent preference as the two governing principles of modality following, offering both a quantitative framework and mechanistic insight into how MLLMs resolve conflicting information. Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2545031",
    "title": "When Modalities Conflict: How Unimodal Reasoning Uncertainty Governs\n  Preference Dynamics in MLLMs",
    "authors": [
      "Yang Shi"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2511.02243",
    "upvote": 24
  }
}