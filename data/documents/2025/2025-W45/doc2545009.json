{
  "context": "A comprehensive comparison of low-precision floating-point and integer quantization in large language models reveals that fine-grained integer formats, especially MXINT8, offer superior accuracy and efficiency over floating-point formats in many cases. Modern AI hardware, such as Nvidia's Blackwell architecture, is increasingly\nembracinglow-precision floating-point(FP) formats to handle the pervasive\nactivation outliers inLarge Language Models(LLMs). Despite this industry\ntrend, a unified comparison of FP and integer (INT) quantization across varying\ngranularities has been missing, leaving algorithm and hardware co-design\nwithout clear guidance. This paper fills that gap by systematically\ninvestigating the trade-offs between FP and INT formats. We reveal a critical\nperformance crossover: while FP excels incoarse-grained quantization, the\ncomparison at fine-grained (block-wise) levels is more nuanced. Our\ncomprehensive comparison demonstrates that for popular 8-bit fine-grained\nformats (e.g., MX with block size 32),MXINT8is superior to its FP counterpart\nin both algorithmic accuracy and hardware efficiency. However, for 4-bit\nformats, FP (e.g.,MXFP4,NVFP4) often holds an accuracy advantage , though we\nshow thatNVINT4can surpassNVFP4when outlier-mitigation techniques likeHadamard rotationare applied. We also introduce asymmetric clippingmethod\nthat resolvesgradient biasin fine-grained low-bit INT training, enabling\nnearly lossless performance forMXINT8training. These findings challenge the\ncurrent hardware trajectory, demonstrating that a one-size-fits-all FP approach\nis suboptimal and advocating that fine-grained INT formats, particularlyMXINT8, offer a better balance of accuracy, power, and efficiency for future AI\naccelerators. Is nvidia wrong? This paper proves that MXINT8 is better than MXFP8 From the paper:We train 1B and 3B Llama3-style [13] models on the OLMo2-Mix-1124 [33] pretraining dataset, with 100B and 200B training tokens, respectively. Once tested on large model + dataset then it will be a game changer. Thanks! This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Has this approach been verified to  on larger-scale data and models? Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2545009",
    "title": "INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization\n  Formats",
    "authors": [
      "Meng Wu"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/ChenMnZ/INT_vs_FP",
    "huggingface_url": "https://huggingface.co/papers/2510.25602",
    "upvote": 77
  }
}