{
  "context": "CodeClash evaluates language models' ability to iteratively develop code for open-ended objectives through competitive multi-round tournaments. Current benchmarks for coding evaluatelanguage models(LMs) on concrete,\nwell-specified tasks such as fixing specific bugs or writing targeted tests.\nHowever, human programmers do not spend all day incessantly addressing isolated\ntasks. Instead, real-world software development is grounded in the pursuit of\nhigh-level goals, like improving user retention or reducing costs. Evaluating\nwhether LMs can also iteratively develop code to better accomplish open-ended\nobjectives without any explicit guidance remains an open challenge. To address\nthis, we introduceCodeClash, a benchmark where LMs compete in multi-round\ntournaments to build the bestcodebasefor achieving a competitive objective.\nEach round proceeds in two phases: agents edit their code, then theircodebases\ncompete head-to-head in a code arena that determines winners based on\nobjectives like score maximization, resource acquisition, or survival. Whether\nit's writing notes, scrutinizing documentation, analyzing competition logs, or\ncreating test suites, models must decide for themselves how to improve theircodebases both absolutely and against their opponents. We run 1680 tournaments\n(25,200 rounds total) to evaluate 8 LMs across 6 arenas. Our results reveal\nthat while models exhibit diverse development styles, they share fundamental\nlimitations instrategic reasoning. Models also struggle with long-termcodebasemaintenance, as repositories become progressively messy and redundant.\nThese limitations are stark: top models lose every round against expert human\nprogrammers. We open-sourceCodeClashto advance the study of autonomous,\ngoal-oriented code development. Code evaluations for LMs (e.g. HumanEval, SWE-bench) are heavily task-oriented - \"implement a function\", \"fix a bug\", \"write a test\". We tell LMs exactly what we want them to do, and grade them on unit test correctness. But this type of framing overlooks a significant aspect of software engineer. When we write code and develop impressive software systems, we are driven bygoals. High level objectives (e.g., improve user retention, reduce costs, increase revenue) fundamentally motivate why we build in the first place. What if we had a coding evaluation that captured this dynamism of real world software dev? We're excited to share our attempt atbenchmarking goal-driven software engineering - CodeClash! In CodeClash, 2+ models compete to build the best codebase for achieving a high level objective over the course of a multi-round tournament.  Each round has two phases: The LM that wins the majority of rounds is declared winner. Thanks for reading! If this is exciting, please check out the paper and our website for all the details! https://codeclash.ai/ Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2545063",
    "title": "CodeClash: Benchmarking Goal-Oriented Software Engineering",
    "authors": [
      "John Yang"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/CodeClash-ai/CodeClash",
    "huggingface_url": "https://huggingface.co/papers/2511.00839",
    "upvote": 9
  }
}