{
  "context": "AyurParam-2.9B, a domain-specialized bilingual language model fine-tuned for Ayurveda, outperforms other models in its size class and demonstrates competitive performance on specialized medical knowledge tasks. Current large language models excel at broad, general-purpose tasks, but\nconsistently underperform when exposed to highly specialized domains that\nrequire deep cultural, linguistic, and subject-matter expertise. In particular,\ntraditional medical systems such as Ayurveda embody centuries of nuanced\ntextual and clinical knowledge that mainstream LLMs fail to accurately\ninterpret or apply. We introduce AyurParam-2.9B, adomain-specialized,bilingual language modelfine-tunedfrom Param-1-2.9B using an extensive,\nexpertly curatedAyurveda datasetspanning classical texts and clinical\nguidance. AyurParam's dataset incorporatescontext-aware,reasoning, andobjective-style Q&Ain both English and Hindi, with rigorous annotation\nprotocols for factual precision and instructional clarity. Benchmarked onBhashaBench-Ayur, AyurParam not only surpasses all open-source\ninstruction-tuned models in its size class (1.5--3B parameters), but also\ndemonstrates competitive or superior performance compared to much larger\nmodels. The results from AyurParam highlight the necessity for authentic domain\nadaptation andhigh-quality supervisionin delivering reliable, culturally\ncongruent AI for specialized medical knowledge. Current large language models excel at broad, general-purpose tasks, but consistently underperformwhen exposed to highly specialized domains that require deep cultural, linguistic, and subjectmatter expertise. In particular, traditional medical systems such as Ayurveda embody centuriesof nuanced textual and clinical knowledge that mainstream LLMs fail to accurately interpret orapply. We introduce AyurParam-2.9B, a domain-specialized, bilingual language model fine-tunedfrom Param-1-2.9B using an extensive, expertly curated Ayurveda dataset spanning classical textsand clinical guidance. AyurParam’s dataset incorporates context-aware, reasoning, and objectivestyle Q&A in both English and Hindi, with rigorous annotation protocols for factual precisionand instructional clarity. Benchmarked on BhashaBench-Ayur, AyurParam not only surpasses allopen-source instruction-tuned models in its size class (1.5–3B parameters), but also demonstratescompetitive or superior performance compared to much larger models. The results from AyurParamhighlight the necessity for authentic domain adaptation and high-quality supervision in deliveringreliable, culturally congruent AI for specialized medical knowledge. ·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2545090",
    "title": "AyurParam: A State-of-the-Art Bilingual Language Model for Ayurveda",
    "authors": [
      "Mohd Nauman",
      "Vijay Devane"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2511.02374",
    "upvote": 3
  }
}