{
  "context": "Influence functions and off-policy estimation improve data selection in RLVR, accelerating training and reducing data usage for large language models. Data selection is a critical aspect of Reinforcement Learning with Verifiable\nRewards (RLVR) for enhancing the reasoning capabilities of large language\nmodels (LLMs). Current data selection methods are largely heuristic-based,\nlacking theoretical guarantees and generalizability. This work proposes a\ntheoretically-grounded approach usinginfluence functionsto estimate the\ncontribution of each data point to the learning objective. To overcome the\nprohibitive computational cost ofpolicy rolloutsrequired for online influence\nestimation, we introduce anoff-policy influence estimationmethod that\nefficiently approximates data influence using pre-collected offline\ntrajectories. Furthermore, to manage thehigh-dimensional gradientsof LLMs, we\nemploysparse random projectionto reduce dimensionality and improve storage\nand computation efficiency. Leveraging these techniques, we develop\nCurriculum RL with Off-Policy\nInfluence guidance (CROPI), amulti-stage RL frameworkthat\niteratively selects the most influential data for the current policy.\nExperiments on models up to 7B parameters demonstrate that CROPI significantly\naccelerates training. On a 1.5B model, it achieves a 2.66x step-level\nacceleration while using only 10\\% of the data per stage compared to\nfull-dataset training. Our results highlight the substantial potential of\ninfluence-based data selection for efficient RLVR. ðŸš€We introduce CROPI, a curriculum reinforcement learning framework for LLMs that brings theoretically grounded, rollout-free data selection to RLVR. CROPI is powered by Off-Policy Influence Estimation: an influence-function-based approach that approximates how each data point affects the current online policy using pre-collected trajectories, avoiding costly new rollouts. To scale to high-dimensional LLM gradients, we use Sparse Random Projection with a simple pre-projection dropout step that reduces numerical noise while preserving inner products and improving storage and compute efficiency.CROPI splits RL training into stages and, at each stage, selects the subset of data with the highest estimated influence on the current checkpoint. Across models from 1.5B to 7B parameters and varying context lengths, CROPI outperforms full-dataset training and heuristic baselines. On a 1.5B model, it delivers a 2.66Ã— step-level acceleration while training on only 10% of the data per stageâ€”demonstrating the practical gains of influence-based data selection for online RLVR.Key contributions: This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2545060",
    "title": "Data-Efficient RLVR via Off-Policy Influence Guidance",
    "authors": [
      "Erle Zhu",
      "Yuan Wang"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2510.26491",
    "upvote": 10
  }
}