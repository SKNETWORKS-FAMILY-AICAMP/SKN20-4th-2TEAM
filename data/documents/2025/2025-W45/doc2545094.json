{
  "context": "GUI-AIMA, an attention-based and coordinate-free framework, enhances GUI grounding by aligning MLLM attention with patch-wise signals, achieving state-of-the-art performance with minimal training data. Graphical user interface (GUI) grounding is a key function of computer-use\nagents, which maps natural-language instructions to actionable screen regions.\nExisting approaches based onMultimodal Large Language Models(MLLMs) typically\nformulate it as a text-based coordinate generation task, yet directly\ngenerating precise coordinates from visual inputs remains challenging and\ncomputationally intensive. An intuitive way to implementGUI groundingis to\nfirst select visual patches relevant to the instructions and then determine the\nprecise click location within those patches. Based on the observations that\ngeneralMLLMshave somenative grounding capability, nested within their\nattentions, we propose GUI-AIMA, anattention-basedandcoordinate-freesupervised fine-tuningframework for efficientGUI grounding. GUI-AIMA aligns\nthe intrinsic multimodal attention ofMLLMswithpatch-wise groundingsignals.\nThese signals are calculated adaptively for diverse user instructions bymulti-head aggregationon simplifiedquery-visual attention matrices. Besides,\nitscoordinate-freemanner can easily integrate a plug-and-play zoom-in stage.\nGUI-AIMA-3B was trained with only 85k screenshots, demonstrating exceptionaldata efficiencyand verifying that light training can trigger the native\ngrounding capability ofMLLMs. It achieves state-of-the-art performance among\n3B models, attaining an average accuracy of 58.6% onScreenSpot-Proand 62.2%\nonOSWorld-G. Project page: https://github.com/sjz5202/GUI-AIMA TLDR: SoTA results on GUI grounding with efficient training. A new way to perform grounding with new insights. GUI-AIMA aligns the intrinsic multi-modal attention of MLLMs with patch-wise grounding signals. These signals are calculated adaptively for diverse user instructions by multi-head aggregation onsimplified query-visual attention matrices. Besides, its coordinate-free manner can easily integrate a plug-and-play zoom-in stage. GUI-AIMA-3B was trained with only 85k screenshots, demonstrating exceptional data efficiency and verifying that light training can trigger the native grounding capability of MLLMs. It achieves state-of-the-art performance among 3B models, attaining an average accuracy of 58.6% on ScreenSpot-Pro and 62.2% on OSWorld-G. Project page:https://github.com/sjz5202/GUI-AIMA Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2545094",
    "title": "GUI-AIMA: Aligning Intrinsic Multimodal Attention with a Context Anchor\n  for GUI Grounding",
    "authors": [],
    "publication_year": 2025,
    "github_url": "https://github.com/sjz5202/GUI-AIMA",
    "huggingface_url": "https://huggingface.co/papers/2511.00810",
    "upvote": 3
  }
}