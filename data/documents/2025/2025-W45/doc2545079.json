{
  "context": "This survey reviews Efficient Vision-Language-Action models, addressing computational and data challenges through model design, training, and data collection techniques. Vision-Language-Action models (VLAs) represent a significant frontier inembodied intelligence, aiming to bridgedigital knowledgewith physical-world\ninteraction. While these models have demonstrated remarkable generalist\ncapabilities, their deployment is severely hampered by the substantial\ncomputational and data requirements inherent to their underlying large-scale\nfoundation models. Motivated by the urgent need to address these challenges,\nthis survey presents the first comprehensive review of Efficient\nVision-Language-Action models (Efficient VLAs) across the entire\ndata-model-training process. Specifically, we introduce a unified taxonomy to\nsystematically organize the disparate efforts in this domain, categorizing\ncurrent techniques into three core pillars: (1) Efficient Model Design,\nfocusing onefficient architecturesandmodel compression; (2) Efficient\nTraining, which reduces computational burdens during model learning; and (3)Efficient Data Collection, which addresses the bottlenecks in acquiring and\nutilizing robotic data. Through a critical review of state-of-the-art methods\nwithin this framework, this survey not only establishes a foundational\nreference for the community but also summarizes representative applications,\ndelineates key challenges, and charts a roadmap for future research. We\nmaintain a continuously updated project page to track our latest developments:\nhttps://evla-survey.github.io/ This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2545079",
    "title": "A Survey on Efficient Vision-Language-Action Models",
    "authors": [],
    "publication_year": 2025,
    "github_url": "https://github.com/YuZhaoshu/Efficient-VLAs-Survey",
    "huggingface_url": "https://huggingface.co/papers/2510.24795",
    "upvote": 5
  }
}