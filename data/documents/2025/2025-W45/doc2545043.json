{
  "context": "Orion-MSP, a tabular in-context learning architecture, addresses limitations in current models by incorporating multi-scale processing, block-sparse attention, and a Perceiver-style memory, achieving state-of-the-art performance on diverse benchmarks. Tabular data remain the predominant format for real-world applications. Yet,\ndeveloping effective neural models for tabular data remains challenging due to\nheterogeneous feature types and complex interactions occurring at multiple\nscales. Recent advances intabular in-context learning(ICL), such asTabPFNandTabICL, have achieved state-of-the-art performance comparable togradient-boosted trees(GBTs) without task-specific fine-tuning. However,\ncurrent architectures exhibit key limitations: (1) single-scale feature\nprocessing that overlooks hierarchical dependencies, (2) dense attention with\nquadratic scaling in table width, and (3) strictly sequential component\nprocessing that prevents iterative representation refinement and\ncross-component communication. To address these challenges, we introduce\nOrion-MSP, a tabular ICL architecture featuring three key innovations: (1)multi-scale processingto capture hierarchical feature interactions; (2)block-sparse attentioncombining windowed, global, and random patterns for\nscalable efficiency and long-range connectivity; and (3) a Perceiver-style\nmemory enabling safe bidirectional information flow across components. Across\ndiverse benchmarks, Orion-MSP matches or surpasses state-of-the-art performance\nwhile scaling effectively to high-dimensional tables, establishing a new\nstandard for efficienttabular in-context learning. The model is publicly\navailable at https://github.com/Lexsi-Labs/Orion-MSP . Orion-MSP is a tabular foundation model that combines multi-scale sparse attention with Perceiver-style memory for efficient in-context learning on tabular data. The model processes features at multiple resolutions simultaneously, capturing both local feature interactions and global dataset-level patterns through hierarchical attention mechanisms. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2545043",
    "title": "Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning",
    "authors": [
      "Pratinav Seth"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/Lexsi-Labs/Orion-MSP",
    "huggingface_url": "https://huggingface.co/papers/2511.02818",
    "upvote": 15
  }
}