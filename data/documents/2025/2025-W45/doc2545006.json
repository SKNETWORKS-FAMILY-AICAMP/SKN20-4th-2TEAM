{
  "context": "Ling 2.0, a reasoning-oriented language model series, achieves high efficiency and accuracy through a Mixture-of-Experts paradigm, sparse activation, and innovative training techniques. We introduce Ling 2.0, a series reasoning-oriented language foundation built\nupon the principle that every activation boosts reasoning capability. Designed\nto scale from tens of billions to one trillion parameters under a unifiedMixture-of-Experts (MoE)paradigm, Ling 2.0 emphasizeshigh sparsity,cross-scale consistency, and efficiency guided by empirical scaling laws. The\nseries includes three non-thinking (instruct) models - Ling-mini-2.0,\nLing-flash-2.0, and Ling-1T - ranging from 16B to 1T total parameters and\nachieving up to 7-fold active-compute efficiency compared with dense\ncounterparts. Ling 2.0 integrates coordinated innovations across model\narchitecture, pre-training, post-training, and infrastructure: a high-sparsity\nMoE withMTPfor efficient reasoning,reasoning-oriented dataand mid-training\nCoT activation,reinforcement-based fine-tuning (DFT,Evo-CoT), and full-scale\nFP8 training withfine-grained heterogeneous pipelines. At the trillion scale,\nLing-1T establishes a new Pareto frontier ofreasoning accuracyversuscomputational efficiency, demonstrating thatsparse activation, when properly\naligned with reasoning objectives, enables scalable and efficient intelligence.\nCollectively, Ling 2.0 provides a coherent, open, and efficient foundation for\nadvancing future reasoning and thinking models, including the Ring series built\nupon the same base. Technical Report for Ling 2.0 series, including model architecture, pre-training, training infrastructure, post-training of the reflex-grade non-thinking version and comprehensive evaluations. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2545006",
    "title": "Every Activation Boosted: Scaling General Reasoner to 1 Trillion Open\n  Language Foundation",
    "authors": [
      "Ang Li",
      "Borui Ye",
      "Changxin Tian",
      "Cong Zhang",
      "Cunyin Peng"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2510.22115",
    "upvote": 83
  }
}