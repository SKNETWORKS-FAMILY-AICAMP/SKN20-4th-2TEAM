{
  "context": "UniPruneBench is a unified benchmark for evaluating visual token pruning in multimodal LLMs, providing standardized protocols and system-level metrics to assess performance across various tasks and models. Largemultimodal models(LMMs) often suffer from severe inference\ninefficiency due to the large number ofvisual tokensintroduced by image\nencoders. While recenttoken compressionmethods, such aspruningandmerging,\nhave shown promise in reducing redundancy, their evaluation remains fragmented\nand inconsistent. In this work, we presentUniPruneBench, a unified and\nextensible benchmark for visual tokenpruningin multimodal LLMs.UniPruneBenchprovides standardized protocols across sixability dimensionsand tendatasets,\ncovering ten representativecompression algorithmsand three families of LMMs\n(LLaVA-v1.5,Intern-VL3, andQwen2.5-VL). Beyond task accuracy, it incorporates\nsystem-level metrics such asruntimeandprefilling latencyto provide a\nholistic view. Our experiments uncover several key findings: (1) randompruningis a surprisingly strong baseline, (2) no single method consistently\noutperforms others across scenarios, (3)pruning sensitivityvaries\nsignificantly across tasks, withOCRbeing most vulnerable, and (4)pruningratio is the dominant factor governing performance degradation. We believeUniPruneBenchwill serve as a reliable foundation for future research on\nefficient multimodal modeling. Large multimodal models (LMMs) often suffer from severe inference inefficiencydue to the large number of visual tokens introduced by image encoders. Whilerecent token compression methods, such as pruning and merging, have shownpromise in reducing redundancy, their evaluation remains fragmented and inconsistent. In this work, we present UniPruneBench, a unified and extensible benchmark for visual token pruning in multimodal LLMs. UniPruneBench providesstandardized protocols across six ability dimensions and ten datasets, covering tenrepresentative compression algorithms and three families of LMMs (LLaVA-v1.5,Intern-VL3, and Qwen2.5-VL). Beyond task accuracy, it incorporates systemlevel metrics such as runtime and prefilling latency to provide a holistic view.Our experiments uncover several key findings: (1) random pruning is a surprisingly strong baseline, (2) no single method consistently outperforms others acrossscenarios, (3) pruning sensitivity varies significantly across tasks, with OCR being most vulnerable, and (4) pruning ratio is the dominant factor governing performance degradation. We believe UniPruneBench will serve as a reliable foundationfor future research on efficient multimodal modeling. Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2545062",
    "title": "Can Visual Input Be Compressed? A Visual Token Compression Benchmark for\n  Large Multimodal Models",
    "authors": [
      "Kailin Jiang"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/TianfanPeng/VLMUniPruneBench",
    "huggingface_url": "https://huggingface.co/papers/2511.02650",
    "upvote": 9
  }
}