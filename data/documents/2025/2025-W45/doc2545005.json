{
  "context": "Systematic study reveals that naive action fine-tuning degrades visual representations in Vision-Language-Action models, but targeted strategies can mitigate this and improve generalization. The growing success of Vision-Language-Action (VLA) models stems from the\npromise that pretrainedVision-Language Models(VLMs) can endow agents with\ntransferable world knowledge and vision-language (VL) grounding, laying a\nfoundation for action models with broader generalization. Yet when these VLMs\nare adapted to the action modality, it remains unclear to what extent their\noriginal VL representations and knowledge are preserved. In this work, we\nconduct a systematic study of representation retention duringVLA fine-tuning,\nshowing that naive action fine-tuning leads to degradation of visual\nrepresentations. To characterize and measure these effects, we probe VLA's\nhidden representations and analyzeattention maps, further, we design a set of\ntargeted tasks and methods that contrast VLA models with their counterpart\nVLMs, isolating changes in VL capabilities induced by action fine-tuning. We\nfurther evaluate a range of strategies for aligningvisual representationsand\nintroduce a simple yet effective method that mitigates degradation and yields\nimproved generalization to out-of-distribution (OOD) scenarios. Taken together,\nour analysis clarifies the trade-off between action fine-tuning and the\ndegradation of VL representations and highlights practical approaches to\nrecover inherited VL capabilities. Code is publicly available:\nhttps://blind-vla-paper.github.io ArXiv:https://arxiv.org/abs/2510.25616Project Page:https://blind-vla-paper.github.io/Codehttps://github.com/CognitiveAISystems/BlindVLA Action fine-tuning often blinds VLA models: they lose the visualâ€“language (VL) priors that made them smart. We show how to keep those priors intact with a tiny alignment loss. ðŸ‘€ðŸ¤– ðŸ” We probed VL representations of OpenVLA and observe 3 huge problems: (1) attention sink - attention maps become diffuse, noisy, and weakly correlated with the target object, (2) representation collapse - patch embeddings lose separability, (3) domain forgetting - VL understanding in symbolic/abstract categories degrade after naive SFT. âœ¨Following the Platonic Representation Hypothesis, we introduce Visual Representation Alignment. During SFT, we pull a VLAâ€™s visual tokens toward a frozen teacherâ€™s features (cosine sim) via a lightweight frozen projector. Keep perception anchored while learning to act. Think of it like learning to drive with a compass. The policy learns control, but the compass (teacher-aligned vision) prevents drift into shortcutty, brittle perception. ðŸ“ŠTo measure the transfer of VL understanding and knowledge from VLMs to VLAs independently of low-level control, we built VL-Think - a minimal pick-and-place suite that isolates VL understanding from control. It probes symbols, colors, arrows, traffic/weather icons, etc., so drops reflect VL forgetting, not grasp skill. Visual Representation Alignment mitigates domain forgetting and boosts performance in Color and Shape tasks, even surpassing the PrismaticVLM upper bound. Yet limited data diversity and LoRA capacity hinder recovery of rarer VL concepts â€” a key direction for future work. ðŸ“ˆ We evaluate our approach in Simpler-based environments using the VL-Think and the benchmark introduced by RL4VLA measuring VLA generalization across Vision (textures, noise), Semantics (unseen objects, paraphrases, distractors), and Execution (randomized poses, object changes).  Results: Align (ours) > naive SFT > Vision Encoder Freeze across OOD settings. Our alignment method yields consistent improvements across all evaluation axes. Linear probing on ImageNet-100 shows stronger features. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend arXiv explained breakdown of this paper ðŸ‘‰https://arxivexplained.com/papers/dont-blind-your-vla-aligning-visual-representations-for-ood-generalization Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2545005",
    "title": "Don't Blind Your VLA: Aligning Visual Representations for OOD\n  Generalization",
    "authors": [
      "Nikita Kachaev",
      "Mikhail Kolosov",
      "Daniil Zelezetsky"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/CognitiveAISystems/BlindVLA",
    "huggingface_url": "https://huggingface.co/papers/2510.25616",
    "upvote": 96
  }
}