{
  "context": "A multimodal diffusion transformer framework, DUal-STream diffusion (DUST), enhances Vision-Language-Action models by maintaining separate modality streams and enabling cross-modal knowledge sharing, achieving improved performance in both simulated and real-world robotic tasks. Recently, augmenting Vision-Language-Action models (VLAs) with world modeling\nhas shown promise in improving robotic policy learning. However, it remains\nchallenging to jointly predict next-state observations and action sequences\nbecause of the inherent difference between the two modalities. To address this,\nwe propose DUal-STream diffusion (DUST), a world-model augmented VLA framework\nthat handles the modality conflict and enhances the performance of VLAs across\ndiverse tasks. Specifically, we propose amultimodal diffusion transformerarchitecture that explicitly maintains separatemodality streamswhile still\nenablingcross-modal knowledge sharing. In addition, we introduce independent\nnoise perturbations for each modality and adecoupled flow-matching loss. This\ndesign enables the model to learn the joint distribution in a bidirectional\nmanner while avoiding the need for a unified latent space. Based on the\ndecoupling of modalities during training, we also introduce a joint sampling\nmethod that supportstest-time scaling, where action andvision tokensevolve\nasynchronously at different rates. Through experiments on simulated benchmarks\nsuch asRoboCasaandGR-1, DUST achieves up to 6% gains over baseline methods,\nwhile ourtest-time scalingapproach provides an additional 2-5% boost. On\nreal-world tasks with theFranka Research 3, DUST improves success rates by\n13%, confirming its effectiveness beyond simulation. Furthermore,pre-trainingon action-free videos fromBridgeV2yields significanttransfer gainsonRoboCasa, underscoring DUST's potential for large-scale VLA pretraining. Recently, augmenting Vision-Language-Action models (VLAs) with world modeling has shown promise in improving robotic policy learning. However, it remains challenging to jointly predict next-state observations and action sequences because of the inherent difference between the two modalities. To address this, we propose DUal-STream diffusion (DUST), a world-model augmented VLA framework that handles the modality conflict and enhances the performance of VLAs across diverse tasks. Specifically, we propose a multimodal diffusion transformer architecture that explicitly maintains separate modality streams while still enabling cross-modal knowledge sharing. In addition, we introduce independent noise perturbations for each modality and a decoupled flow-matching loss. This design enables the model to learn the joint distribution in a bidirectional manner while avoiding the need for a unified latent space. Based on the decoupling of modalities during training, we also introduce a joint sampling method that supports test-time scaling, where action and vision tokens evolve asynchronously at different rates. Through experiments on simulated benchmarks such as RoboCasa and GR-1, DUST achieves up to 6% gains over baseline methods, while our test-time scaling approach provides an additional 2-5% boost. On real-world tasks with the Franka Research 3, DUST improves success rates by 13%, confirming its effectiveness beyond simulation. Furthermore, pre-training on action-free videos from BridgeV2 yields significant transfer gains on RoboCasa, underscoring DUST's potential for large-scale VLA pretraining. Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2545065",
    "title": "Dual-Stream Diffusion for World-Model Augmented Vision-Language-Action\n  Model",
    "authors": [
      "John Won"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2510.27607",
    "upvote": 8
  }
}