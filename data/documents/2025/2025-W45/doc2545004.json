{
  "context": "V-Thinker, a multimodal reasoning assistant using reinforcement learning, enhances image-interactive thinking by synthesizing datasets and aligning perception for improved performance in vision-centric tasks. Empowering LargeMultimodal Models(LMMs) to deeply integrate image\ninteraction withlong-horizon reasoningcapabilities remains a long-standing\nchallenge in this field. Recent advances in vision-centric reasoning explore a\npromising \"Thinking with Images\" paradigm for LMMs, marking a shift from\nimage-assisted reasoning toimage-interactive thinking. While this milestone\nenables models to focus on fine-grained image regions, progress remains\nconstrained by limited visual tool spaces and task-specific workflow designs.\nTo bridge this gap, we present V-Thinker, a general-purpose multimodal\nreasoning assistant that enables interactive, vision-centric thinking throughend-to-end reinforcement learning. V-Thinker comprises two key components: (1)\naData Evolution Flywheelthat automatically synthesizes, evolves, and verifies\ninteractive reasoning datasets across three dimensions-diversity, quality, and\ndifficulty; and (2) aVisual Progressive Training Curriculumthat first aligns\nperception viapoint-level supervision, then integrates interactive reasoning\nthrough atwo-stage reinforcement learningframework. Furthermore, we introduceVTBench, an expert-verified benchmark targeting vision-centric interactive\nreasoning tasks. Extensive experiments demonstrate that V-Thinker consistently\noutperforms strong LMM-based baselines in both general and interactive\nreasoning scenarios, providing valuable insights for advancing\nimage-interactive reasoning applications. V-Thinker is a general-purpose multimodal reasoning assistant that enables Interactive Thinking with Images through end-to-end reinforcement learning. Unlike traditional vision-language models, V-Thinker actively interacts with visual content‚Äîediting, annotating, and transforming images to simplify complex problems. üíª Github:https://github.com/We-Math/V-Thinkerü§ó Dataset:https://huggingface.co/datasets/We-Math/V-Interaction-400K V-Thinker is a general-purpose multimodal reasoning assistant that enables interactive, vision-centric thinking through end-to-end reinforcement learning.Unlike traditional vision-language models that passively process visual inputs, V-Thinker actively interacts with images ‚Äî editing, annotating, and transforming them to simplify complex problems and achieve reasoning grounded in perception and logic. To address the limited diversity and scalability of existing visual reasoning datasets, we rethink the traditional data synthesis paradigm by transforming models from ‚Äúsolvers‚Äù to ‚Äúcreators.‚ÄùThis establishes a new vision-centric data synthesis framework that empowers models to autonomously generate high-quality, diverse, and knowledge-grounded multimodal reasoning data.Built upon this foundation, V-Thinker integrates a unified post-training paradigm that combines data evolution, perception alignment, and interactive reasoning into a coherent pipeline for advancing vision-centric reasoning. Representative examples of V-Thinker's knowledge-driven synthesis spanning diverse reasoning domains. Complete interactive reasoning samples of V-Thinker on open-source benchmarks.  Visualization of the evolved knowledge system through the Data Evolution Flywheel. Qualitative analysis of V-Thinker-7B on vision-centric interactive reasoning tasks.  This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend arXiv explained breakdown of this paper üëâhttps://arxivexplained.com/papers/v-thinker-interactive-thinking-with-images ¬∑Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2545004",
    "title": "V-Thinker: Interactive Thinking with Images",
    "authors": [
      "Runqi Qiao",
      "Shiqiang Lang"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/We-Math/V-Thinker",
    "huggingface_url": "https://huggingface.co/papers/2511.04460",
    "upvote": 97
  }
}