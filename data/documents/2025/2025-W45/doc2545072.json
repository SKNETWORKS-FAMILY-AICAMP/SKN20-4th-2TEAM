{
  "context": "A Unified Diffusion VLA and Joint Discrete Denoising Diffusion Process (JD3P) model integrates multiple modalities through a synchronous denoising process, achieving state-of-the-art performance in vision-language-action tasks with faster inference. Vision-language-action (VLA) models aim to understand natural language\ninstructions and visual observations and to execute corresponding actions as an\nembodied agent. Recent work integrates future images into the\nunderstanding-acting loop, yielding unified VLAs that jointly understand,\ngenerate, and act -- reading text and images and producing future images and\nactions. However, these models either rely on external experts for modality\nunification or treat image generation and action prediction as separate\nprocesses, limiting the benefits of direct synergy between these tasks. Our\ncore philosophy is to optimize generation and action jointly through asynchronous denoising process, where the iterative refinement enables actions\nto evolve from initialization, under constant and sufficient visual guidance.\nWe ground this philosophy in our proposedUnified Diffusion VLAand Joint\nDiscrete Denoising Diffusion Process (JD3P), which is a joint diffusion process\nthat integrates multiple modalities into a single denoising trajectory to serve\nas the key mechanism enabling understanding, generation, and acting to be\nintrinsically synergistic. Our model and theory are built on a unified\ntokenized space of all modalities and ahybrid attention mechanism. We further\npropose atwo-stage training pipelineand severalinference-time techniquesthat optimize performance and efficiency. Our approach achieves\nstate-of-the-art performance on benchmarks such asCALVIN,LIBERO, andSimplerEnvwith 4times faster inference than autoregressive methods, and we\ndemonstrate its effectiveness through in-depth analysis and real-world\nevaluations. Our project page is available at\nhttps://irpn-eai.github.io/UD-VLA.github.io/. We releasethe first open-sourced diffusion Vision-language-action model---Unified Diffusion VLA.Arxiv:https://arxiv.org/abs/2511.01718Project page:https://irpn-eai.github.io/UD-VLA.github.io/Github repo:https://github.com/OpenHelix-Team/UD-VLA Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2545072",
    "title": "Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete\n  Denoising Diffusion Process",
    "authors": [
      "Jiayi Chen"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/OpenHelix-Team/UD-VLA",
    "huggingface_url": "https://huggingface.co/papers/2511.01718",
    "upvote": 6
  }
}