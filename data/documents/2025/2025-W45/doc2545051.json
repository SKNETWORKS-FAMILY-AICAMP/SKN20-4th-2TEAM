{
  "context": "NaviTrace is a Visual Question Answering benchmark for evaluating robotic navigation capabilities using a semantic-aware trace score across various scenarios and embodiment types. Vision-language modelsdemonstrate unprecedented performance and\ngeneralization across a wide range of tasks and scenarios. Integrating these\nfoundation models intorobotic navigationsystems opens pathways toward\nbuilding general-purpose robots. Yet, evaluating these models' navigation\ncapabilities remains constrained by costly real-world trials, overly simplified\nsimulations, and limited benchmarks. We introduceNaviTrace, a high-qualityVisual Question Answeringbenchmark where a model receives an instruction and\nembodiment type (human, legged robot, wheeled robot, bicycle) and must output a\n2D navigation trace in image space. Across 1000 scenarios and more than 3000\nexpert traces, we systematically evaluate eight state-of-the-art VLMs using a\nnewly introducedsemantic-aware trace score. This metric combines Dynamic Time\nWarping distance,goal endpoint error, andembodiment-conditioned penaltiesderived fromper-pixel semanticsand correlates with human preferences. Our\nevaluation reveals consistent gap to human performance caused by poor spatial\ngrounding and goal localization.NaviTraceestablishes a scalable and\nreproducible benchmark for real-worldrobotic navigation. The benchmark and\nleaderboard can be found at\nhttps://leggedrobotics.github.io/navitrace_webpage/. NaviTrace is a novel VQA benchmark for evaluating vision–language models (VLMs) on their embodiment-specific understanding of navigation across diverse real-world scenarios. Given a natural-language instruction and an embodiment type (human, legged robot, wheeled robot, bicycle), a model must output a 2D navigation path in image space, which we call a trace. The benchmark includes 1000 scenarios with 3000+ expert traces, divided into: The dataset is available on Hugging Face. We provide ready-to-use evaluation scripts for API-based model inference and scoring, along with a leaderboard where users can compute scores on the test split and optionally submit their models for public comparison. ·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2545051",
    "title": "NaviTrace: Evaluating Embodied Navigation of Vision-Language Models",
    "authors": [
      "Tim Windecker",
      "Manthan Patel",
      "Moritz Reuss"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/leggedrobotics/navitrace_evaluation",
    "huggingface_url": "https://huggingface.co/papers/2510.26909",
    "upvote": 13
  }
}