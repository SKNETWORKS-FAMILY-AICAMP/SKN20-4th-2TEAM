{
  "context": "Research investigates how and when value alignment occurs during the post-training phase of LLMs, finding that supervised fine-tuning establishes values, while preference optimization has limited impact. AsLLMsoccupy an increasingly important role in society, they are more and\nmore confronted with questions that require them not only to draw on their\ngeneral knowledge but also to align with certain human value systems.\nTherefore, studying the alignment ofLLMswith human values has become a\ncrucial field of inquiry. Prior work, however, mostly focuses on evaluating the\nalignment of fully trained models, overlooking the training dynamics by which\nmodels learn to express human values. In this work, we investigate how and at\nwhich stagevalue alignmentarises during the course of a model'spost-training. Our analysis disentangles the effects ofpost-trainingalgorithms and datasets, measuring both the magnitude and time ofvalue driftsduring training. Experimenting with Llama-3 and Qwen-3 models of different\nsizes and popularsupervised fine-tuning(SFT) andpreference optimizationdatasets and algorithms, we find that the SFT phase generally establishes a\nmodel's values, and subsequentpreference optimizationrarely re-aligns these\nvalues. Furthermore, using a synthetic preference dataset that enables\ncontrolled manipulation of values, we find that different preference\noptimization algorithms lead to differentvalue alignmentoutcomes, even when\npreference data is held constant. Our findings provide actionable insights into\nhow values are learned duringpost-trainingand help to inform data curation,\nas well as the selection of models and algorithms forpreference optimizationto improve model alignment to human values. How do LLMs acquire human values? We often point to preference optimization. In our new work, we trace how and when model values shift during post-training and find surprising dynamics. We ask: How do data, algorithms, and their interaction shape model values?  Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2545056",
    "title": "Value Drifts: Tracing Value Alignment During LLM Post-Training",
    "authors": [],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2510.26707",
    "upvote": 12
  }
}