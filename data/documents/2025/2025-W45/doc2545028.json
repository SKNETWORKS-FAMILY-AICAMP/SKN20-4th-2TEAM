{
  "context": "MotionStream enables real-time video generation with sub-second latency and up to 29 FPS by distilling a text-to-video model with motion control into a causal student using Self Forcing with Distribution Matching Distillation and sliding-window causal attention with attention sinks. Currentmotion-conditioned video generationmethods suffer from prohibitive\nlatency (minutes per video) and non-causal processing that prevents real-time\ninteraction. We presentMotionStream, enabling sub-second latency with up to 29\nFPS streaming generation on a single GPU. Our approach begins by augmenting atext-to-video modelwithmotion control, which generates high-quality videos\nthat adhere to the global text prompt and local motion guidance, but does not\nperform inference on the fly. As such, we distill this bidirectional teacher\ninto acausal studentthrough Self Forcing with Distribution Matching\nDistillation, enabling real-time streaming inference. Several key challenges\narise when generating videos of long, potentially infinite time-horizons: (1)\nbridging the domain gap from training on finite length and extrapolating to\ninfinite horizons, (2) sustaining high quality by preventing error\naccumulation, and (3) maintaining fast inference, without incurring growth in\ncomputational cost due to increasing context windows. A key to our approach is\nintroducing carefully designedsliding-window causal attention, combined withattention sinks. By incorporatingself-rolloutwithattention sinksand KV\ncache rolling during training, we properly simulate inference-time\nextrapolations with a fixed context window, enabling constant-speed generation\nof arbitrarily long videos. Our models achieve state-of-the-art results inmotion followingandvideo qualitywhile being two orders of magnitude faster,\nuniquely enabling infinite-length streaming. WithMotionStream, users can paint\ntrajectories, control cameras, or transfer motion, and see results unfold in\nreal-time, delivering a truly interactive experience. MotionStream is a streaming (real-time, long-duration) video generation system with motion controls. Checkout more videos inhttps://joonghyuk.com/motionstream-web/! This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend  Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2545028",
    "title": "MotionStream: Real-Time Video Generation with Interactive Motion\n  Controls",
    "authors": [
      "Joonghyuk Shin"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/alex4727/motionstream",
    "huggingface_url": "https://huggingface.co/papers/2511.01266",
    "upvote": 28
  }
}