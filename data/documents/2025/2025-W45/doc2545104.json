{
  "context": "A novel approach combining GT-Pair and Reg-DPO enhances video generation quality by addressing data construction, training stability, and memory consumption challenges. Recent studies have identifiedDirect Preference Optimization(DPO) as an\nefficient and reward-free approach to improving video generation quality.\nHowever, existing methods largely follow image-domain paradigms and are mainly\ndeveloped on small-scale models (approximately 2B parameters), limiting their\nability to address the unique challenges of video tasks, such as costly data\nconstruction, unstable training, and heavy memory consumption. To overcome\nthese limitations, we introduce aGT-Pairthat automatically builds\nhigh-quality preference pairs by using real videos as positives and\nmodel-generated videos as negatives, eliminating the need for any external\nannotation. We further presentReg-DPO, which incorporates theSFT lossas a\nregularization term into theDPOobjective to enhance training stability and\ngeneration fidelity. Additionally, by combining theFSDP frameworkwith\nmultiple memory optimization techniques, our approach achieves nearly three\ntimes higher training capacity than using FSDP alone. Extensive experiments on\nboth I2V andT2V tasksacross multiple datasets demonstrate that our method\nconsistently outperforms existing approaches, delivering superior video\ngeneration quality. Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2545104",
    "title": "Reg-DPO: SFT-Regularized Direct Preference Optimization with GT-Pair for\n  Improving Video Generation",
    "authors": [
      "Jie Du"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2511.01450",
    "upvote": 1
  }
}