{
  "context": "MME-CC is a vision-grounded benchmark that evaluates multimodal large language models' cognitive capacity across spatial, geometric, and knowledge-based reasoning tasks, revealing weaknesses in spatial and geometric reasoning. As reasoning models scale rapidly, the essential role of multimodality in\nhuman cognition has come into sharp relief, driving a growing need to probevision-centric cognitive behaviors. Yet, existingmultimodal benchmarkseither\noveremphasize textual reasoning or fall short of systematically capturingvision-centric cognitive behaviors, leaving thecognitive capacityofMLLMsinsufficiently assessed. To address this limitation, we introduceMME-CC(Multi-Modal Evaluation benchmark ofCognitive Capacity), a vision-grounded\nbenchmark that organizes 11 representative reasoning tasks into three\nfundamental categories ofvisual information: spatial, geometric, andknowledge-based reasoning, and provides fine-grained analyses ofMLLMs'cognitive capacityacross these dimensions. Based onMME-CC, we conduct\nextensive experiments over 16 representativeMLLMs. Our study reveals thatclosed-source modelscurrently lead overall (e.g., 42.66 for Gemini-2.5-Pro vs.\n30.45 for GLM-4.5V), while spatial andgeometric reasoningremain broadly weak\n(less than or equal to 30%). We further identify common error patterns,\nincludingorientation mistakes, fragilecross-view identity persistence, and\npoor adherence tocounterfactual instructions, and observe thatChain-of-Thoughttypically follows a three-stage process (extract -> reason ->\nverify) with heavy reliance on visual extraction. We hope this work catalyzes a\nshift toward treating thecognitive capacityofMLLMsas central to both\nevaluation and model design. As reasoning models scale rapidly, the essential role of multimodality in human cognition has come into sharp relief, driving a growing need to probe vision-centric cognitive behaviors. Yet, existing multimodal benchmarks either overemphasize textual reasoning or fall short of systematically capturing vision-centric cognitive behaviors, leaving the cognitive capacity of MLLMs insufficiently assessed. To address this limitation, we introduce MME-CC (Multi-Modal Evaluation benchmark of Cognitive Capacity), a vision-grounded benchmark that organizes 11 representative reasoning tasks into three fundamental categories of visual information: spatial, geometric, and knowledge-based reasoning, and provides fine-grained analyses of MLLMs' cognitive capacity across these dimensions. Based on MME-CC, we conduct extensive experiments over 16 representative MLLMs. Our study reveals that closed-source models currently lead overall (e.g., 42.66 for Gemini-2.5-Pro vs. 30.45 for GLM-4.5V), while spatial and geometric reasoning remain broadly weak (less than or equal to 30%). We further identify common error patterns, including orientation mistakes, fragile cross-view identity persistence, and poor adherence to counterfactual instructions, and observe that Chain-of-Thought typically follows a three-stage process (extract -> reason -> verify) with heavy reliance on visual extraction. We hope this work catalyzes a shift toward treating the cognitive capacity of MLLMs as central to both evaluation and model design. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2545067",
    "title": "MME-CC: A Challenging Multi-Modal Evaluation Benchmark of Cognitive\n  Capacity",
    "authors": [
      "Ge Zhang"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2511.03146",
    "upvote": 7
  }
}