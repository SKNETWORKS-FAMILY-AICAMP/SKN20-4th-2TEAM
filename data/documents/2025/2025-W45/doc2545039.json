{
  "context": "A comprehensive analysis of multimodal Rotary Positional Embedding (RoPE) leads to the proposal of Multi-Head RoPE (MHRoPE) and MRoPE-Interleave (MRoPE-I), which improve multimodal understanding in vision-language models. Multimodal position encodingis essential for vision-language models, yet\nthere has been little systematic investigation into multimodal position\nencoding. We conduct a comprehensive analysis of multimodal Rotary Positional\nEmbedding (RoPE) by examining its two core components:position designandfrequency allocation. Through extensive experiments, we identify three key\nguidelines:positional coherence,full frequency utilization, and preservation\nof textual priors-ensuring unambiguous layout, rich representation, and\nfaithful transfer from the pre-trained LLM. Based on these insights, we proposeMulti-Head RoPE (MHRoPE)andMRoPE-Interleave (MRoPE-I), two simple and\nplug-and-play variants that require no architectural changes. Our methods\nconsistently outperform existing approaches across diverse benchmarks, with\nsignificant improvements in both general and fine-grained multimodal\nunderstanding. Code will be avaliable at\nhttps://github.com/JJJYmmm/Multimodal-RoPEs.  This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2545039",
    "title": "Revisiting Multimodal Positional Encoding in Vision-Language Models",
    "authors": [
      "Jie Huang",
      "Xuejing Liu"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/JJJYmmm/Multimodal-RoPEs",
    "huggingface_url": "https://huggingface.co/papers/2510.23095",
    "upvote": 20
  }
}