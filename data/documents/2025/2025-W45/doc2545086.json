{
  "context": "ConvRec-R1, a two-stage framework, enhances LLM-based conversational recommender systems by using behavioral cloning and Rank-GRPO to improve recommendation quality and convergence. Large language models (LLMs) are reshaping therecommender systemparadigm by\nenabling users to express preferences and receive recommendations through\nconversations. Yet, aligningLLMsto the recommendation task remains\nchallenging: pretrainedLLMsoften generate out-of-catalog items, violate\nrequired output formats, and their ranking quality degrades sharply toward the\nend of the generated list. To this end, we propose ConvRec-R1, a two-stage\nframework for end-to-end training of LLM-based conversational recommender\nsystems. In Stage 1, we construct abehavioral-cloningdataset with aRemap-Reflect-Adjustpipeline, which produces high-quality, catalog-grounded\ndemonstrations from powerful blackboxLLMsto warm-start the RL training. In\nStage 2, we proposeRank-GRPO, a principled extension of group relative policy\noptimization (GRPO) tailored to tasks with rank-style outputs.Rank-GRPOtreats\neach rank in the recommendation list as the unit instead of token (too\nfine-grained) or sequence (too coarse), redefining rewards to remove non-causal\ncredit assignment and introducing arank-level importance ratiobased on the\ngeometric mean of rank-wise token probabilities to stabilize policy updates.\nExperiments on the publicReddit-v2 datasetshow that ConvRec-R1 converges\nfaster and achieves higherRecallandNDCGthanGRPO-style baselines. Code and\ndatasets are released at https://github.com/yaochenzhu/Rank-GRPO. Large language models (LLMs) are reshaping the recommender system paradigm by enabling users to express preferences and receive recommendations through conversations. Yet, aligning LLMs to the recommendation task remains challenging: pretrained LLMs often generate out-of-catalog items, violate required output formats, and their ranking quality degrades sharply toward the end of the generated list. To this end, we propose ConvRec-R1, a two-stage framework for end-to-end training of LLM-based conversational recommender systems. In Stage 1, we construct a behavioral-cloning dataset with a Remap-Reflect-Adjust pipeline, which produces high-quality, catalog-grounded demonstrations from powerful blackbox LLMs to warm-start the RL training. In Stage 2, we propose Rank-GRPO, a principled extension of group relative policy optimization (GRPO) tailored to tasks with rank-style outputs. Rank-GRPO treats each rank in the recommendation list as the unit instead of token (too fine-grained) or sequence (too coarse), redefining rewards to remove non-causal credit assignment and introducing a rank-level importance ratio based on the geometric mean of rank-wise token probabilities to stabilize policy updates. Experiments on the public Reddit-v2 dataset show that ConvRec-R1 converges faster and achieves higher Recall and NDCG than GRPO-style baselines. Code and datasets are released athttps://github.com/yaochenzhu/Rank-GRPO. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2545086",
    "title": "Rank-GRPO: Training LLM-based Conversational Recommender Systems with\n  Reinforcement Learning",
    "authors": [
      "Yaochen Zhu"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/yaochenzhu/Rank-GRPO",
    "huggingface_url": "https://huggingface.co/papers/2510.20150",
    "upvote": 4
  }
}