{
  "context": "Cosmos-Predict2.5 and Cosmos-Transfer2.5 are advanced Physical AI models that unify text, image, and video generation, improve video quality and instruction alignment, and enable Sim2Real and Real2Real world translation with higher fidelity. We introduce [Cosmos-Predict2.5], the latest generation of the Cosmos World\nFoundation Models forPhysical AI. Built on aflow-based architecture,\n[Cosmos-Predict2.5] unifiesText2World,Image2World, andVideo2Worldgeneration\nin a single model and leverages [Cosmos-Reason1], aPhysical AIvision-language\nmodel, to provide richer text grounding and finer control of world simulation.\nTrained on 200M curated video clips and refined with reinforcement\nlearning-based post-training, [Cosmos-Predict2.5] achieves substantial\nimprovements over [Cosmos-Predict1] in video quality and instruction alignment,\nwith models released at 2B and 14B scales. These capabilities enable more\nreliablesynthetic data generation,policy evaluation, and closed-loop\nsimulation forroboticsandautonomous systems. We further extend the family\nwith [Cosmos-Transfer2.5], acontrol-netstyle framework forSim2RealandReal2Realworld translation. Despite being 3.5times smaller than\n[Cosmos-Transfer1], it delivers higher fidelity and robust long-horizon video\ngeneration. Together, these advances establish [Cosmos-Predict2.5] and\n[Cosmos-Transfer2.5] as versatile tools for scalingembodied intelligence. To\naccelerate research and deployment inPhysical AI, we release source code,\npretrained checkpoints, and curated benchmarks under the NVIDIA Open Model\nLicense at https://github.com/nvidia-cosmos/cosmos-predict2.5 and\nhttps://github.com/nvidia-cosmos/cosmos-transfer2.5. We hope these open\nresources lower the barrier to adoption and foster innovation in building the\nnext generation ofembodied intelligence. We introduce [Cosmos-Predict2.5], the latest generation of the Cosmos World Foundation Models for Physical AI. Built on a flow-based architecture, [Cosmos-Predict2.5] unifies Text2World, Image2World, and Video2World generation in a single model and leverages [Cosmos-Reason1], a Physical AI vision-language model, to provide richer text grounding and finer control of world simulation. Trained on 200M curated video clips and refined with reinforcement learning-based post-training, [Cosmos-Predict2.5] achieves substantial improvements over [Cosmos-Predict1] in video quality and instruction alignment, with models released at 2B and 14B scales. These capabilities enable more reliable synthetic data generation, policy evaluation, and closed-loop simulation for robotics and autonomous systems. We further extend the family with [Cosmos-Transfer2.5], a control-net style framework for Sim2Real and Real2Real world translation. Despite being 3.5times smaller than [Cosmos-Transfer1], it delivers higher fidelity and robust long-horizon video generation. Together, these advances establish [Cosmos-Predict2.5] and [Cosmos-Transfer2.5] as versatile tools for scaling embodied intelligence. To accelerate research and deployment in Physical AI, we release source code, pretrained checkpoints, and curated benchmarks under the NVIDIA Open Model License athttps://github.com/nvidia-cosmos/cosmos-predict2.5andhttps://github.com/nvidia-cosmos/cosmos-transfer2.5. We hope these open resources lower the barrier to adoption and foster innovation in building the next generation of embodied intelligence. Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2545018",
    "title": "World Simulation with Video Foundation Models for Physical AI",
    "authors": [
      "Yu-Wei Chao",
      "Yin Cui"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/nvidia-cosmos/cosmos-predict2.5",
    "huggingface_url": "https://huggingface.co/papers/2511.00062",
    "upvote": 40
  }
}