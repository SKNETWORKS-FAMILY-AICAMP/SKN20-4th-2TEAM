{
  "context": "MeasureBench evaluates vision-language models on reading measurements from images, revealing challenges in indicator localization and fine-grained spatial grounding. Reading measurement instruments is effortless for humans and requires\nrelatively little domain expertise, yet it remains surprisingly challenging for\ncurrentvision-language models(VLMs) as we find in preliminary evaluation. In\nthis work, we introduceMeasureBench, a benchmark on visual measurement reading\ncovering both real-world and synthesized images of various types of\nmeasurements, along with an extensible pipeline fordata synthesis. Our\npipeline procedurally generates a specified type ofgaugewith controllable\nvisual appearance, enabling scalable variation in key details such aspointers,scales,fonts,lighting, andclutter. Evaluation on popular proprietary and\nopen-weightVLMsshows that even the strongest frontierVLMsstruggle\nmeasurement reading in general. A consistent failure mode is indicator\nlocalization: models can read digits or labels but misidentify the key\npositions ofpointersor alignments, leading to big numeric errors despite\nplausible textual reasoning. We have also conducted preliminary experiments\nwithreinforcement learningover synthetic data, and find encouraging results\non in-domain synthetic subset but less promising for real-world images. Our\nanalysis highlights a fundamental limitation of currentVLMsin fine-grained\nspatial grounding. We hope this resource can help future advances on visually\ngrounded numeracy and precise spatial perception ofVLMs, bridging the gap\nbetween recognizing numbers and measuring the world. Vision-Language Models (VLMs) can already tackle many complex tasks, including some college-level exam questions. Frontier VLMs also show promising spatial reasoning ability and are being explored in domains such as autonomous driving and embodied intelligence. However, many studies point out that VLMs still struggle with fine-grained visual perception, with some even claiming that “VLMs are blind.”In this work, we seek a quantitative way to evaluate fine-grained visual perception with reasoning in VLMs. We focus on instrument-reading scenarios—such as reading clocks, ammeters, and measuring cylinders—which are realistic, meaningful, and surprisingly challenging for current models. To this end, we propose MeasureBench and a synthetic pipeline for generating measuring-instrument data, hoping to drive progress in fine-grained visual perception for VLMs. ·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2545058",
    "title": "Do Vision-Language Models Measure Up? Benchmarking Visual Measurement\n  Reading with MeasureBench",
    "authors": [
      "Haiyu Xu",
      "Zheqi He",
      "Miguel Hu Chen"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/flageval-baai/MeasureBench",
    "huggingface_url": "https://huggingface.co/papers/2510.26865",
    "upvote": 11
  }
}