{
  "context": "Phased DMD enhances multi-step distillation of score-based generative models by dividing SNR ranges and using progressive distribution matching, improving diversity and generative capabilities. Distribution Matching Distillation(DMD) distills score-based generative\nmodels into efficientone-step generators, without requiring a one-to-one\ncorrespondence with the sampling trajectories of their teachers. However,\nlimited model capacity causes one-step distilled models underperform on complex\ngenerative tasks, e.g., synthesizing intricate object motions in text-to-video\ngeneration. Directly extending DMD tomulti-step distillationincreases memory\nusage andcomputational depth, leading to instability and reduced efficiency.\nWhile prior works proposestochastic gradient truncationas a potential\nsolution, we observe that it substantially reduces thegeneration diversityof\nmulti-step distilled models, bringing it down to the level of their one-step\ncounterparts. To address these limitations, we propose Phased DMD, a multi-step\ndistillation framework that bridges the idea ofphase-wise distillationwithMixture-of-Experts(MoE), reducing learning difficulty while enhancing model\ncapacity. Phased DMD is built upon two key ideas: progressive distribution\nmatching andscore matchingwithinsubintervals. First, our model divides theSNR rangeintosubintervals, progressively refining the model to higher SNR\nlevels, to better capture complex distributions. Next, to ensure the training\nobjective within each subinterval is accurate, we have conducted rigorous\nmathematical derivations. We validate Phased DMD by distilling state-of-the-art\nimage and video generation models, includingQwen-Image(20B parameters) andWan2.2(28B parameters). Experimental results demonstrate that Phased DMD\npreserves output diversity better than DMD while retaining key generative\ncapabilities. We will release our code and models. AbstractDistribution Matching Distillation (DMD) distills score-based generative models into efficient one-step generators, without requiring a one-to-one correspondence with the sampling trajectories of their teachers. However, limited model capacity causes one-step distilled models underperform on complex generative tasks, e.g., synthesizing intricate object motions in text-to-video generation. Directly extending DMD to multi-step distillation increases memory usage and computational depth, leading to instability and reduced efficiency. While prior works propose stochastic gradient truncation as a potential solution, we observe that it substantially reduces the generation diversity of multi-step distilled models, bringing it down to the level of their one-step counterparts. To address these limitations, we propose Phased DMD, a multi-step distillation framework that bridges the idea of phase-wise distillation with Mixture-of-Experts (MoE), reducing learning difficulty while enhancing model capacity. Phased DMD is built upon two key ideas: progressive distribution matching and score matching within subintervals. First, our model divides the SNR range into subintervals, progressively refining the model to higher SNR levels, to better capture complex distributions. Next, to ensure the training objective within each subinterval is accurate, we have conducted rigorous mathematical derivations. We validate Phased DMD by distilling state-of-the-art image and video generation models, including Qwen-Image (20B parameters) and Wan2.2 (28B parameters). Experimental results demonstrate that Phased DMD preserves output diversity better than DMD while retaining key generative capabilities. We will release our code and models. Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2545034",
    "title": "Phased DMD: Few-step Distribution Matching Distillation via Score\n  Matching within Subintervals",
    "authors": [
      "Lei Yang"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/ModelTC/Wan2.2-Lightning",
    "huggingface_url": "https://huggingface.co/papers/2510.27684",
    "upvote": 22
  }
}