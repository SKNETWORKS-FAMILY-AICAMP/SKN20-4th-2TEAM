{
  "context": "PHUMA, a large-scale and physically reliable humanoid locomotion dataset, improves motion imitation by addressing physical artifacts in human video data. Motion imitationis a promising approach forhumanoid locomotion, enabling\nagents to acquire humanlike behaviors. Existing methods typically rely on\nhigh-qualitymotion capture datasetssuch asAMASS, but these are scarce and\nexpensive, limiting scalability and diversity. Recent studies attempt to scale\ndata collection by converting large-scale internet videos, exemplified byHumanoid-X. However, they often introducephysical artifactssuch asfloating,penetration, andfoot skating, which hinder stable imitation. In response, we\nintroduce PHUMA, a Physically-groundedHUMAnoid locomotiondataset that\nleverages human video at scale, while addressingphysical artifactsthrough\ncarefuldata curationandphysics-constrained retargeting. PHUMA enforces joint\nlimits, ensuresground contact, and eliminatesfoot skating, producing motions\nthat are both large-scale and physically reliable. We evaluated PHUMA in two\nsets of conditions: (i) imitation of unseen motion from self-recorded test\nvideos and (ii)path followingwithpelvis-only guidance. In both cases,\nPHUMA-trained policies outperformHumanoid-XandAMASS, achieving significant\ngains in imitating diverse motions. The code is available at\nhttps://davian-robotics.github.io/PHUMA. We introduce PHUMA: a Physically-Grounded Humanoid Locomotion Dataset! ✨ By using human video and physically-grounded retargeting methods,PHUMA is 3x larger than AMASS leading to 20% better motion tracking policy for unseen human video. Paper:https://arxiv.org/abs/2510.26236Project Page:https://davian-robotics.github.io/PHUMA/GitHub:https://github.com/davian-robotics/PHUMAHugging Face:https://huggingface.co/datasets/DAVIAN-Robotics/PHUMAX:https://x.com/lee_kyungmin21/status/1984335933907546567 This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend ·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2545026",
    "title": "PHUMA: Physically-Grounded Humanoid Locomotion Dataset",
    "authors": [
      "Kyungmin Lee"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/davian-robotics/PHUMA",
    "huggingface_url": "https://huggingface.co/papers/2510.26236",
    "upvote": 29
  }
}