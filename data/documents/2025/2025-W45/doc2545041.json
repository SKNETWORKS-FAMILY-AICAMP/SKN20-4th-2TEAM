{
  "context": "SemCoT enhances CoT efficiency by optimizing token-level generation speed and preserving semantic alignment with ground-truth reasoning using contrastive training and knowledge distillation. The verbosity ofChain-of-Thought(CoT) reasoning hinders its mass deployment\nin efficiency-critical applications. Recently,implicit CoTapproaches have\nemerged, which encode reasoning steps withinLLM'shidden embeddings(termed\n``implicit reasoning'') rather thanexplicit tokens. This approach accelerates\nCoT by reducing the reasoning length and bypassing someLLMcomponents.\nHowever, existingimplicit CoTmethods face two significant challenges: (1)\nthey fail to preserve thesemantic alignmentbetween theimplicit reasoning(when transformed to natural language) and the ground-truth reasoning,\nresulting in a significant CoT performance degradation, and (2) they focus on\nreducing the length of theimplicit reasoning; however, they neglect the\nconsiderable time cost for anLLMto generate one individualimplicit reasoningtoken. To tackle these challenges, we propose a novel semantically-alignedimplicit CoTframework termedSemCoT. In particular, for the first challenge,\nwe design a contrastively trainedsentence transformerthat evaluates semantic\nalignment between implicit and explicit reasoning, which is used to enforce\nsemantic preservation duringimplicit reasoningoptimization. To address the\nsecond challenge, we introduce an efficientimplicit reasoninggenerator by\nfinetuning a lightweight language model usingknowledge distillation. This\ngenerator is guided by oursentence transformerto distill ground-truth\nreasoning into semantically alignedimplicit reasoning, while also optimizing\nfor accuracy.SemCoTis the first approach that enhances CoT efficiency by\njointly optimizing token-level generation speed and preserving semantic\nalignment with ground-truth reasoning. Extensive experiments demonstrate the\nsuperior performance ofSemCoTcompared to state-of-the-art methods in both\nefficiency and effectiveness. Our code can be found at\nhttps://github.com/YinhanHe123/SemCoT/. The verbosity of Chain-of-Thought (CoT) reasoning hinders its mass deployment in efficiency-critical applications. Recently, implicit CoT approaches have emerged, which encode reasoning steps within LLM's hidden embeddings (termed ``implicit reasoning'') rather than explicit tokens. This approach accelerates CoT by reducing the reasoning length and bypassing some LLM components. However, existing implicit CoT methods face two significant challenges: (1) they fail to preserve the semantic alignment between the implicit reasoning (when transformed to natural language) and the ground-truth reasoning, resulting in a significant CoT performance degradation, and (2) they focus on reducing the length of the implicit reasoning; however, they neglect the considerable time cost for an LLM to generate one individual implicit reasoning token. To tackle these challenges, we propose a novel semantically-aligned implicit CoT framework termed SemCoT. In particular, for the first challenge, we design a contrastively trained sentence transformer that evaluates semantic alignment between implicit and explicit reasoning, which is used to enforce semantic preservation during implicit reasoning optimization. To address the second challenge, we introduce an efficient implicit reasoning generator by finetuning a lightweight language model using knowledge distillation. This generator is guided by our sentence transformer to distill ground-truth reasoning into semantically aligned implicit reasoning, while also optimizing for accuracy. SemCoT is the first approach that enhances CoT efficiency by jointly optimizing token-level generation speed and preserving semantic alignment with ground-truth reasoning. Extensive experiments demonstrate the superior performance of SemCoT compared to state-of-the-art methods in both efficiency and effectiveness. Our code can be found athttps://github.com/YinhanHe123/SemCoT/. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2545041",
    "title": "SemCoT: Accelerating Chain-of-Thought Reasoning through\n  Semantically-Aligned Implicit Tokens",
    "authors": [
      "Wendy Zheng",
      "Yaochen Zhu",
      "Sriram Vasudevan"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/YinhanHe123/SemCoT",
    "huggingface_url": "https://huggingface.co/papers/2510.24940",
    "upvote": 17
  }
}