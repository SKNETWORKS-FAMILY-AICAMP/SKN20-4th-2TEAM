{
  "context": "ROVER is a benchmark that evaluates reciprocal cross-modal reasoning in unified multimodal models, showing that cross-modal interactions significantly impact visual generation quality and that models struggle with symbolic reasoning tasks. Unified multimodal models(UMMs) have emerged as a powerful paradigm for\nseamlessly unifying text and image understanding and generation. However,\nprevailing evaluations treat these abilities in isolation, such that tasks with\nmultimodal inputs and outputs are scored primarily through unimodal reasoning,\ni.e., textual benchmarks emphasize language-based reasoning, while visual\nbenchmarks emphasize reasoning outcomes manifested in the pixels. We introduceROVERto address this pressing need to testreciprocal cross-modal reasoning,\nthe use of one modality to guide, verify, or refine outputs in the other, an\nability central to the vision of unified multimodal intelligence.ROVERis a\nhuman-annotated benchmark that explicitly targets reciprocal cross-modal\nreasoning, which contains 1312 tasks grounded in 1876 images, spanning two\ncomplementary settings. Verbally-augmented reasoning for visual generation\nevaluates whether models can useverbal promptsandreasoning chainsto guide\nfaithfulimage synthesis. Visually-augmented reasoning for verbal generation\nevaluates whether models can generateintermediate visualizationsthat\nstrengthen their own reasoning processes forquestion answering. Experiments on\n17 unified models reveal two key findings: (i)Cross-modal reasoningdeterminesvisual generation quality, withinterleaved modelssignificantly outperforming\nnon-interleaved ones; notably, combining strongunimodal modelsfails to\nachieve comparable reasoning. (ii) Models show dissociation between physical\nandsymbolic reasoning: they succeed at interpretingperceptual conceptsliterally but fail to constructvisual abstractionsfor symbolic tasks, where\nfaulty reasoning harms performance. These results highlight reciprocalcross-modal reasoningas a critical frontier for enabling true omnimodal\ngeneration. ROVER evaluates UMMs through reciprocal cross-modal reasoning: ROVER-IG requires generating images with language-augmented reasoning, while ROVER-TG requires generating text answers with visually-augmented reasoning.https://roverbench.github.io/ Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2545023",
    "title": "ROVER: Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal\n  Generation",
    "authors": [
      "Yongyuan Liang",
      "Ziqiao Ma",
      "Xiyao Wang",
      "Jiatao Gu"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/cheryyunl/ROVER",
    "huggingface_url": "https://huggingface.co/papers/2511.01163",
    "upvote": 31
  }
}