{
  "context": "The \"Thinking with Video\" paradigm enhances multimodal reasoning by integrating video generation models, demonstrated through the Video Thinking Benchmark and improved performance on both vision and text tasks. \"Thinking with Text\" and \"Thinking with Images\" paradigm significantly\nimprove the reasoning ability oflarge language models(LLMs) and Vision\nLanguage Models (VLMs). However, these paradigms have inherent limitations. (1)\nImages capture only single moments and fail to represent dynamic processes or\ncontinuous changes, and (2) The separation of text and vision as distinct\nmodalities, hindering unified multimodal understanding and generation. To\novercome these limitations, we introduce \"Thinking with Video\", a new paradigm\nthat leveragesvideo generation models, such as Sora-2, to bridge visual and\ntextual reasoning in a unified temporal framework. To support this exploration,\nwe developed theVideo Thinking Benchmark(VideoThinkBench). VideoThinkBench\nencompasses two task categories: (1)vision-centric tasks(e.g., Eyeballing\nPuzzles), and (2)text-centric tasks(e.g., subsets ofGSM8K,MMMU). Our\nevaluation establishes Sora-2 as a capable reasoner. Onvision-centric tasks,\nSora-2 is generally comparable to state-of-the-art (SOTA) VLMs, and even\nsurpasses VLMs on several tasks, such as Eyeballing Games. On text-centric\ntasks, Sora-2 achieves 92% accuracy on MATH, and 75.53% accuracy onMMMU.\nFurthermore, we systematically analyse the source of these abilities. We also\nfind thatself-consistencyandin-context learningcan improve Sora-2's\nperformance. In summary, our findings demonstrate that the video generation\nmodel is the potential unified multimodal understanding and generation model,\npositions \"thinking with video\" as a unified multimodal reasoning paradigm. We introduce \"Thinking with Video\", a new paradigm that leverages video generation models, such as Sora-2, to bridge visual and textual reasoning in a unified temporal framework. To support this exploration, we developed the Video Thinking Benchmark (VideoThinkBench). Our findings demonstrate that the video generation model is the potential unified multimodal understanding and generation model, positioning \"Thinking with Video\" as a unified multimodal reasoning paradigm. Website & Demo:https://thinking-with-video.github.io/ This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend arXiv lens breakdown of this paper ðŸ‘‰https://arxivlens.com/PaperView/Details/thinking-with-video-video-generation-as-a-promising-multimodal-reasoning-paradigm-2944-57971210 Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2545001",
    "title": "Thinking with Video: Video Generation as a Promising Multimodal\n  Reasoning Paradigm",
    "authors": [
      "Mingzhe Li",
      "Yongzhuo Yang"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/tongjingqi/Thinking-with-Video",
    "huggingface_url": "https://huggingface.co/papers/2511.04570",
    "upvote": 211
  }
}