{
  "context": "This paper develops a semantic information theory framework for large language models, focusing on token-level semantic embedding and information-theoretic measures to understand and analyze model architectures and performance. Large language models (LLMs) have demonstrated remarkable capabilities in\nnumerous real-world applications. While the vast majority of research conducted\nfrom an experimental perspective is progressing rapidly, it demands substantial\ncomputational power, data, and other resources. Therefore, how to open the\nblack-box of LLMs from a theoretical standpoint has become a critical\nchallenge. This paper takes the theory ofrate-distortion function, directed\ninformation, andGranger causalityas its starting point to investigate the\ninformation-theoretic principles behind LLMs, leading to the development ofsemantic information theoryfor LLMs, where the fundamental unit istoken,\nrather than bits that lacks any semantic meaning. By defining the probabilistic\nmodel of LLMs, we discuss structure-agnostic information-theoretic measures,\nsuch as thedirected rate-distortion functionin pre-training, the directed\nrate-reward function in post-training, and thesemantic information flowin\ninference phase. This paper also delves deeply into the theory oftoken-level\nsemantic embedding and theinformation-theoretically optimal vectorizationmethod. Thereafter, we propose a general definition ofautoregression LLM,\nwhere theTransformer architectureand its performance such asELBO,generalization error bound,memory capacity, andsemantic information measurescan be derived theoretically. Other architectures, such asMamba/Mamba2andLLaDA, are also discussed in our framework. Consequently, this paper provides a\ntheoretical framework for understanding LLMs from the perspective of semantic\ninformation theory, which also offers the necessary theoretical tools for\nfurther in-depth research. This paper takes the theory of rate-distortion function, directed information, and Granger causality as its starting point to investigate the information-theoretic principles behind LLMs, leading to the development of semantic information theory for LLMs, where the fundamental unit is token, rather than bits that lacks any semantic meaning. Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2545076",
    "title": "Forget BIT, It is All about TOKEN: Towards Semantic Information Theory\n  for LLMs",
    "authors": [],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2511.01202",
    "upvote": 5
  }
}