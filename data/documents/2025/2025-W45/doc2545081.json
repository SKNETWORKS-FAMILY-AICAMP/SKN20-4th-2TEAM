{
  "context": "A novel affective cues-guided reasoning framework using video emotion foundation models and a fine-grained dataset achieves competitive performance in emotion understanding tasks. Understanding and predicting emotion from videos has gathered significant\nattention in recent studies, driven by advancements in video large language\nmodels (VideoLLMs). While advanced methods have made progress in video emotion\nanalysis, the intrinsic nature of emotions poses significant challenges.\nEmotions are characterized by dynamic and cues-dependent properties, making it\ndifficult to understand complex and evolving emotional states with reasonable\nrationale. To tackle these challenges, we propose a novel affective cues-guided\nreasoning framework that unifies fundamental attribute perception, expression\nanalysis, and high-level emotional understanding in a stage-wise manner. At the\ncore of our approach is a family ofvideo emotion foundation models(VidEmo),\nspecifically designed for emotion reasoning and instruction-following. These\nmodels undergo a two-stage tuning process: first,curriculum emotion learningfor injecting emotion knowledge, followed by affective-tree reinforcement\nlearning for emotion reasoning. Moreover, we establish a foundational data\ninfrastructure and introduce aemotion-centric fine-grained dataset(Emo-CFG)\nconsisting of 2.1M diverse instruction-based samples.Emo-CFGincludesexplainable emotional question-answering,fine-grained captions, and associated\nrationales, providing essential resources for advancing emotion understanding\ntasks. Experimental results demonstrate that our approach achieves competitive\nperformance, setting a new milestone across 15face perception tasks. Understanding and predicting emotion from videos has gathered significant attention in recent studies, driven by advancements in video large language models (VideoLLMs). While advanced methods have made progress in video emotion analysis, the intrinsic nature of emotions poses significant challenges. Emotions are characterized by dynamic and cues-dependent properties, making it difficult to understand complex and evolving emotional states with reasonable rationale. To tackle these challenges, we propose a novel affective cues-guided reasoning framework that unifies fundamental attribute perception, expression analysis, and high-level emotional understanding in a stage-wise manner. At the core of our approach is a family of video emotion foundation models (VidEmo), specifically designed for emotion reasoning and instruction-following. These models undergo a two-stage tuning process: first, curriculum emotion learning for injecting emotion knowledge, followed by affective-tree reinforcement learning for emotion reasoning. Moreover, we establish a foundational data infrastructure and introduce a emotion-centric fine-grained dataset (Emo-CFG) consisting of 2.1M diverse instruction-based samples. Emo-CFG includes explainable emotional question-answering, fine-grained captions, and associated rationales, providing essential resources for advancing emotion understanding tasks. Experimental results demonstrate that our approach achieves competitive performance, setting a new milestone across 15 face perception tasks. Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2545081",
    "title": "VidEmo: Affective-Tree Reasoning for Emotion-Centric Video Foundation\n  Models",
    "authors": [],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2511.02712",
    "upvote": 4
  }
}