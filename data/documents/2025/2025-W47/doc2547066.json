{
  "context": "A surgical video segmentation model SAM2S enhances interactive video object segmentation through robust memory, temporal learning, and ambiguity handling, achieving high performance and real-time inference on a comprehensive surgical benchmark. Surgical video segmentation is crucial for computer-assisted surgery, enabling precise localization and tracking of instruments and tissues.Interactive Video Object Segmentation(iVOS) models such asSegment Anything Model 2(SAM2) provide prompt-based flexibility beyond methods with predefined categories, but face challenges in surgical scenarios due to the domain gap and limited long-term tracking. To address these limitations, we constructSA-SV, the largest surgicaliVOSbenchmark with instance-level spatio-temporal annotations (masklets) spanning eight procedure types (61k frames, 1.6k masklets), enabling comprehensive development and evaluation for long-term tracking andzero-shot generalization. Building onSA-SV, we proposeSAM2S, a foundation model enhancingSAM2for SurgicaliVOSthrough: (1)DiveMem, a trainable diverse memory mechanism for robust long-term tracking; (2)temporal semantic learningfor instrument understanding; and (3)ambiguity-resilient learningto mitigate annotation inconsistencies across multi-source datasets. Extensive experiments demonstrate that fine-tuning onSA-SVenables substantial performance gains, withSAM2improving by 12.99 average J\\&F over vanillaSAM2.SAM2S further advances performance to 80.42 average J\\&F, surpassing vanilla and fine-tunedSAM2by 17.10 and 4.11 points respectively, while maintaining 68 FPSreal-time inferenceand strongzero-shot generalization. Code and dataset will be released at https://jinlab-imvr.github.io/SAM2S. We are excited to share our latest work: \"SAM2S: Segment Anything in Surgical Videos via Semantic Long-term Tracking\".https://arxiv.org/abs/2511.16618 We introduce SAM2S, enhancing SAM2 for surgical video segmentation through semantic long-term tracking and domain-specific adaptations. #SurgicalAI #SurgicalDataScience #SAM #SAM2 #VideoObjectSegmentation This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2547066",
    "title": "SAM2S: Segment Anything in Surgical Videos via Semantic Long-term Tracking",
    "authors": [
      "Haofeng Liu",
      "Alex Y. W. Kong"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/jinlab-imvr/SAM2S",
    "huggingface_url": "https://huggingface.co/papers/2511.16618",
    "upvote": 7
  }
}