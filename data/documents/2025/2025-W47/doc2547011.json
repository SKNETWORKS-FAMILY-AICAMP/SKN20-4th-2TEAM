{
  "context": "VR-Bench evaluates video models' spatial reasoning capabilities through maze-solving tasks, demonstrating that these models excel in spatial perception and reasoning, outperforming VLMs and benefiting from diverse sampling during inference. Video Modelshave achieved remarkable success in high-fidelityvideo generationwithcoherent motion dynamics. Analogous to the development from text generation to text-based reasoning in language modeling, the development ofvideo modelsmotivates us to ask: Canvideo modelsreason viavideo generation? Compared with the discrete text corpus, video grounds reasoning in explicit spatial layouts andtemporal continuity, which serves as an ideal substrate forspatial reasoning. In this work, we explore the reasoning via video paradigm and introduceVR-Bench-- a comprehensive benchmark designed to systematically evaluatevideo models' reasoning capabilities. Grounded inmaze-solving tasksthat inherently requirespatial planningandmulti-step reasoning,VR-Benchcontains 7,920 procedurally generated videos across five maze types and diverse visual styles. Our empirical analysis demonstrates thatSFTcan efficiently elicit the reasoning ability of video model.Video modelsexhibit stronger spatial perception during reasoning, outperformingleading VLMsand generalizing well across diverse scenarios, tasks, and levels of complexity. We further discover atest-time scaling effect, where diverse sampling during inference improves reasoning reliability by 10--20%. These findings highlight the unique potential and scalability of reasoning via video forspatial reasoningtasks. Video Models have achieved remarkable success in high-fidelity video generation with coherent motion dynamics. Analogous to the development from text generation to text-based reasoning in language modeling, the development of video models motivates us to ask: Can video models reason via video generation? Compared with the discrete text corpus, video grounds reasoning in explicit spatial layouts and temporal continuity, which serves as an ideal substrate for spatial reasoning. In this work, we explore the reasoning via video paradigm and introduce VR-Benchâ€”a comprehensive benchmark designed to systematically evaluate video models' reasoning capabilities. Grounded in maze-solving tasks that inherently require spatial planning and multi-step reasoning, VR-Bench contains 7,920 procedurally generated videos across five maze types and diverse visual styles. Our empirical analysis demonstrates that SFT can efficiently elicit the reasoning ability of video model. % In spatial reasoning task, Video model outperforms leading VLMs and generalize well across diverse scenarios, tasks, and levels of complexity. Video models exhibit stronger spatial perception during reasoning, outperforming leading VLMs and generalizing well across diverse scenarios, tasks, and levels of complexity. We further discover a test-time scaling effect, where diverse sampling during inference improves reasoning reliability by 10â€“20%. These findings highlight the unique potential and scalability of reasoning via video for spatial reasoning tasks. We have constructed a comprehensive evaluation system that facilitates objective and quantifiable assessment by tracking target trajectories in generated videos and comparing them against the ground truth. A distinctive feature of our benchmark is its ability to provide explicit reward signals for future paradigms, such as Video RL. As posited by Jason's 'Verifier Rule,' the feasibility of training AI for a task is directly proportional to the task's verifiability. Characterized by its objectivity, rapid and scalable verification, and low noise, VR-Bench serves as a highly effective benchmark within this framework. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend arXiv explained breakdown of this paper ðŸ‘‰https://arxivexplained.com/papers/reasoning-via-video-the-first-evaluation-of-video-models-reasoning-abilities-through-maze-solving-tasks Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2547011",
    "title": "Reasoning via Video: The First Evaluation of Video Models' Reasoning Abilities through Maze-Solving Tasks",
    "authors": [
      "Cheng Yang",
      "Haiyuan Wan",
      "Zhaoyang Yu",
      "Jiayi Zhang",
      "Xinlei Yu"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/ImYangC7/VR-Bench",
    "huggingface_url": "https://huggingface.co/papers/2511.15065",
    "upvote": 75
  }
}