{
  "context": "Denoising Positional Encoding (DoPE) enhances length generalization in Transformer models by detecting and mitigating noisy frequency bands in positional embeddings, improving retrieval accuracy and reasoning stability. Rotary Position Embedding (RoPE)inTransformer modelshas inherent limits that weaken length extrapolation. We reinterpret theattention mapwithpositional encodingas a noisy feature map, and proposeDenoising Positional Encoding (DoPE), a training-free method based ontruncated matrix entropyto detect outlier frequency bands in the feature map. Leveraging the noise characteristics of the feature map, we further reparameterize it with a parameter-freeGaussian distributionto achieve robust extrapolation. Our method theoretically reveals the underlying cause of theattention sink phenomenonand its connection totruncated matrix entropy. Experiments onneedle-in-a-haystackandmany-shot in-context learningtasks demonstrate that DoPE significantly improvesretrieval accuracyandreasoning stabilityacross extended contexts (up to 64K tokens). The results show that the denoising strategy for positional embeddings effectively mitigates attention sinks and restores balanced attention patterns, providing a simple yet powerful solution for improvinglength generalization. Our project page is Project: https://The-physical-picture-of-LLMs.github.io Good paper Good paper Good comment! Very insightful! Insightful paper, everyone should read it! Respect Nice paper! good paperÔºÅ super formula heavy, but a good read. Definitely should do some literature review to understand the difference between other developments in ROPE. The main idea I was impressed by was mitigation of the attention sink in a theoretical and then empirical manner This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Good paper Good comment! Good user! arXiv explained breakdown of this paper üëâhttps://arxivexplained.com/papers/dope-denoising-rotary-position-embedding ¬∑Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2547010",
    "title": "DoPE: Denoising Rotary Position Embedding",
    "authors": [
      "Zunhai Su"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2511.09146",
    "upvote": 95
  }
}