{
  "context": "A framework integrates human priors into end-to-end generative recommenders, enhancing accuracy and beyond-accuracy objectives by leveraging lightweight adapter heads and hierarchical composition strategies. Optimizingrecommender systemsfor objectives beyond accuracy, such asdiversity,novelty, andpersonalization, is crucial for long-term user satisfaction. To this end, industrial practitioners have accumulated vast amounts of structured domain knowledge, which we termhuman priors(e.g.,item taxonomies,temporal patterns). This knowledge is typically applied through post-hoc adjustments during ranking or post-ranking. However, this approach remains decoupled from the core model learning, which is particularly undesirable as the industry shifts toend-to-end generative recommendation foundation models. On the other hand, many methods targeting these beyond-accuracy objectives often require architecture-specific modifications and discard these valuablehuman priorsby learninguser intentin a fully unsupervised manner.\n  Instead of discarding thehuman priorsaccumulated over years of practice, we introduce abackbone-agnostic frameworkthat seamlessly integrates thesehuman priorsdirectly into the end-to-end training of generative recommenders. With lightweight,prior-conditioned adapter headsinspired byefficient LLM decoding strategies, our approach guides the model to disentangleuser intentalong human-understandable axes (e.g.,interaction types, long- vs. short-term interests). We also introduce ahierarchical composition strategyfor modeling complex interactions across different prior types. Extensive experiments on three large-scale datasets demonstrate that our method significantly enhances both accuracy and beyond-accuracy objectives. We also show thathuman priorsallow the backbone model to more effectively leverage longer context lengths and larger model sizes. While the scaling law shows immense promise, we believe that, before the arrival of AGI, it is unwise to ignore the structured human priors accumulated over years of practice. For generative recommenders, the timely challenge is how to integrate this knowledge into foundation models without resorting to brittle, post-hoc fixes. Inspired by efficient LLM decoding, this paper introduces a framework  that uses lightweight, prior-conditioned adapter heads to inject human expertise directly into E2E training. This guides the model to disentangle user intent along interpretable axes, resulting in recommendations that are more accurate, diverse, and inherently controllable. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2547077",
    "title": "Don't Waste It: Guiding Generative Recommenders with Structured Human Priors via Multi-head Decoding",
    "authors": [
      "Yunkai Zhang"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/zhykoties/Multi-Head-Recommendation-with-Human-Priors",
    "huggingface_url": "https://huggingface.co/papers/2511.10492",
    "upvote": 5
  }
}