{
  "context": "SAM 3D is a generative model that reconstructs 3D objects from single images using a multi-stage training framework that includes synthetic pretraining and real-world alignment, achieving high performance in human preference tests. We present SAM 3D, agenerative modelfor visually grounded3D object reconstruction, predictinggeometry,texture, andlayoutfrom a single image. SAM 3D excels in natural images, where occlusion and scene clutter are common and visual recognition cues from context play a larger role. We achieve this with a human- and model-in-the-loop pipeline for annotatingobject shape,texture, and pose, providing visually grounded 3D reconstruction data at unprecedented scale. We learn from this data in a modern,multi-stage trainingframework that combinessynthetic pretrainingwithreal-world alignment, breaking the 3D \"data barrier\". We obtain significant gains over recent work, with at least a 5:1 win rate inhuman preference testson real-world objects and scenes. We will release our code and model weights, an online demo, and a new challengingbenchmarkfor in-the-wild3D object reconstruction. We present SAM 3D, a generative model for visually grounded 3D object reconstruction, predicting geometry, texture, and layout from a single image. SAM 3D excels in natural images, where occlusion and scene clutter are common and visual recognition cues from context play a larger role. We achieve this with a human- and model-in-the-loop pipeline for annotating object shape, texture, and pose, providing visually grounded 3D reconstruction data at unprecedented scale. We learn from this data in a modern, multi-stage training framework that combines synthetic pretraining with real-world alignment, breaking the 3D \"data barrier\". We obtain significant gains over recent work, with at least a 5:1 win rate in human preference tests on real-world objects and scenes. We will release our code and model weights, an online demo, and a new challenging benchmark for in-the-wild 3D object reconstruction. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend arXiv explained breakdown of this paper ðŸ‘‰https://arxivexplained.com/papers/sam-3d-3dfy-anything-in-images arXiv lens breakdown of this paper ðŸ‘‰https://arxivlens.com/PaperView/Details/sam-3d-3dfy-anything-in-images-9667-03d581e7 Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2547006",
    "title": "SAM 3D: 3Dfy Anything in Images",
    "authors": [
      "Pierre Gleize",
      "Kevin J Liang",
      "Alexander Sax",
      "Thibaut Hardin",
      "Anushka Sagar",
      "Bowen Zhang",
      "Matt Feiszli"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/facebookresearch/sam-3d-objects",
    "huggingface_url": "https://huggingface.co/papers/2511.16624",
    "upvote": 110
  }
}