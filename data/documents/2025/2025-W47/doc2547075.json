{
  "context": "HI-TransPA, an instruction-driven audio-visual personal assistant, uses Omni-Model paradigm to translate and dialogue by fusing speech with lip dynamics, achieving state-of-the-art performance in assistive communication for hearing-impaired individuals. To provide a unified and flexible solution for daily communication among hearing-impaired individuals, we introduce theOmni-Modelparadigm into assistive technology and presentHI-TransPA, aninstruction-drivenaudio-visual personal assistant. The model fusesindistinct speechwithhigh-frame-rate lip dynamics, enabling both translation and dialogue within a single multimodal framework. To tackle the challenges of noisy and heterogeneous raw data and the limited adaptability of existingOmni-Models to hearing-impaired speech, we construct a comprehensivepreprocessingandcuration pipelinethat detectsfacial landmarks, isolates and stabilizes the lip region, and quantitatively assessesmultimodal sample quality. These quality scores guide acurriculum learningstrategy that first trains on clean, high-confidence samples and progressively incorporates harder cases to strengthen model robustness. We further adopt aSigLIP encodercombined with aUnified 3D-Resamplerto efficiently encode high-frame-rate lip motion. Experiments on our purpose-builtHI-Dialogue datasetshow thatHI-TransPAachieves state-of-the-art performance in bothliteral accuracyandsemantic fidelity. This work establishes a foundation for applyingOmni-Models to assistive communication technology, providing an end-to-end modeling framework and essential processing tools for future research. Hearing-impaired individuals often face significant barriers in daily communication due to the inherent challenges of producing clear speech. To address this, we introduce the Omni-Model paradigm into assistive technology and present HI-TransPA, an instruction-driven audio-visual personal assistant. The model fuses indistinct speech with lip dynamics, enabling both translation and dialogue within a single multimodal framework. To address the distinctive pronunciation patterns of hearing-impaired speech and the limited adaptability of existing models, we develop a multimodal preprocessing and curation pipeline that detects facial landmarks, stabilizes the lip region, and quantitatively evaluates sample quality. These quality scores guide a curriculum learning strategy that first trains on clean, high-confidence samples and progressively incorporates harder cases to strengthen model robustness. Architecturally, we employs a novel unified 3D-Resampler to efficiently encode the lip dynamics, which is critical for accurate interpretation. Experiments on purpose-built HI-Dialogue dataset show that HI-TransPA achieves state-of-the-art performance in both literal accuracy and semantic fidelity. Our work establishes a foundation for applying Omni-Models to assistive communication technology, providing an end-to-end modeling framework and essential processing tools for future research. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2547075",
    "title": "HI-TransPA: Hearing Impairments Translation Personal Assistant",
    "authors": [
      "Zhiming Ma"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2511.09915",
    "upvote": 6
  }
}