{
  "context": "Self-Referential Policy Optimization (SRPO) uses latent world representations to assign progress-wise rewards to failed trajectories, improving efficiency and effectiveness in vision-language-action robotic manipulation tasks without external demonstrations. Vision-Language-Action (VLA) models excel in robotic manipulation but are constrained by their heavy reliance on expert demonstrations, leading to demonstration bias and limiting performance.Reinforcement learning(RL) is a vital post-training strategy to overcome these limits, yet current VLA-RL methods, including group-based optimization approaches, are crippled by severe reward sparsity. Relying on binary success indicators wastes valuable information in failed trajectories, resulting in low training efficiency. To solve this, we proposeSelf-Referential Policy Optimization(SRPO), a novel VLA-RL framework.SRPOeliminates the need for external demonstrations or manual reward engineering by leveraging the model's own successful trajectories, generated within the current training batch, as a self-reference. This allows us to assign a progress-wise reward to failed attempts. A core innovation is the use oflatent world representationsto measure behavioral progress robustly. Instead of relying on raw pixels or requiring domain-specific fine-tuning, we utilize the compressed, transferable encodings from a world model's latent space. These representations naturally capture progress patterns across environments, enabling accurate, generalized trajectory comparison. Empirical evaluations on theLIBERO benchmarkdemonstrateSRPO's efficiency and effectiveness. Starting from a supervised baseline with 48.9% success,SRPOachieves a new state-of-the-art success rate of 99.2% in just 200 RL steps, representing a 103% relative improvement without any extra supervision. Furthermore,SRPOshows substantial robustness, achieving a 167% performance improvement on theLIBERO-Plus benchmark. SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2547042",
    "title": "SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models",
    "authors": [
      "Siyin Wang"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/sii-research/siiRL/blob/main/docs/examples/embodied_grpo_example.rst",
    "huggingface_url": "https://huggingface.co/papers/2511.15605",
    "upvote": 23
  }
}