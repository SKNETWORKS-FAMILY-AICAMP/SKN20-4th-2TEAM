{
  "context": "Video generation is evolving towards foundation models that integrate world simulation and rendering to produce physically plausible and interactive videos. The landscape of video generation is shifting, from a focus on generating visually appealing clips to building virtual environments that support interaction and maintainphysical plausibility. These developments point toward the emergence ofvideo foundation modelsthat function not only as visual generators but also asimplicit world models, models that simulate thephysical dynamics,agent-environment interactions, andtask planningthat govern real or imagined worlds. This survey provides a systematic overview of this evolution, conceptualizing modernvideo foundation modelsas the combination of two core components: an implicit world model and avideo renderer. The world model encodes structured knowledge about the world, including physical laws, interaction dynamics, and agent behavior. It serves as alatent simulationengine that enables coherentvisual reasoning, long-termtemporal consistency, andgoal-driven planning. Thevideo renderertransforms thislatent simulationinto realistic visual observations, effectively producing videos as a \"window\" into the simulated world. We trace the progression of video generation through four generations, in which the core capabilities advance step by step, ultimately culminating in a world model, built upon a video generation model, that embodies intrinsicphysical plausibility, real-timemultimodal interaction, and planning capabilities spanning multiplespatiotemporal scales. For each generation, we define its core characteristics, highlight representative works, and examine their application domains such as robotics, autonomous driving, and interactive gaming. Finally, we discuss open challenges and design principles for next-generation world models, including the role ofagent intelligencein shaping and evaluating these systems. An up-to-date list of related works is maintained at this link. The landscape of video generation is shifting, from a focus on generating visually appealing clips to building virtual environments that support interaction and maintain physical plausibility. These developments point toward the emergence of video foundation models that function not only as visual generators but also as implicit world models, models that simulate the physical dynamics, agent-environment interactions, and task planning that govern real or imagined worlds. This survey provides a systematic overview of this evolution, conceptualizing modern video foundation models as the combination of two core components: an implicit world model and a video renderer. The world model encodes structured knowledge about the world, including physical laws, interaction dynamics, and agent behavior. It serves as a latent simulation engine that enables coherent visual reasoning, long-term temporal consistency, and goal-driven planning. The video renderer transforms this latent simulation into realistic visual observations, effectively producing videos as a \"window\" into the simulated world. We trace the progression of video generation through four generations, in which the core capabilities advance step by step, ultimately culminating in a world model, built upon a video generation model, that embodies intrinsic physical plausibility, real-time multimodal interaction, and planning capabilities spanning multiple spatiotemporal scales. For each generation, we define its core characteristics, highlight representative works, and examine their application domains such as robotics, autonomous driving, and interactive gaming. Finally, we discuss open challenges and design principles for next-generation world models, including the role of agent intelligence in shaping and evaluating these systems. An up-to-date list of related works is maintained at this link:https://github.com/ziqihuangg/Awesome-From-Video-Generation-to-World-Model. looks like another deep research survey bait...impressive how your lab consistently produces papers with no intellectual or technical novelty whatsoever. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2547035",
    "title": "Simulating the Visual World with Artificial Intelligence: A Roadmap",
    "authors": [
      "Jingtong Yue",
      "Ziqi Huang"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/ziqihuangg/Awesome-From-Video-Generation-to-World-Model",
    "huggingface_url": "https://huggingface.co/papers/2511.08585",
    "upvote": 29
  }
}