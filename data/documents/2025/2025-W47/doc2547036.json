{
  "context": "Nemotron Elastic reduces training costs and memory usage by embedding multiple submodels within a single large language model, optimized for various deployment configurations and budgets without additional training or fine-tuning. Training a family oflarge language modelstargeting multiple scales and deployment objectives is prohibitively expensive, requiring separate training runs for each different size. Recent work onmodel compressionthroughpruningandknowledge distillationhas reduced this cost; however, this process still incurs hundreds of billions of tokens worth of training cost per compressed model. In this paper, we present Nemotron Elastic, a framework for building reasoning-oriented LLMs, includinghybrid Mamba-Attention architectures, that embed multiplenested submodelswithin a singleparent model, each optimized for different deployment configurations and budgets. Each of these submodels shares weights with theparent modeland can be extracted zero-shot during deployment without additional training or fine-tuning. We enable this functionality through anend-to-end trained router, tightly coupled to atwo-stage training curriculumdesigned specifically for reasoning models. We additionally introducegroup-aware SSM elastificationthat preserves Mamba's structural constraints,heterogeneous MLP elastification,normalized MSE-based layer importancefor improved depth selection, andknowledge distillationenabling simultaneousmulti-budget optimization. We apply Nemotron Elastic to theNemotron Nano V2 12B model, simultaneously producing a 9B and a 6B model using only 110B training tokens; this results in over 360x cost reduction compared to training model families from scratch, and around 7x compared to SoTA compression techniques. Each of the nested models performs on par or better than the SoTA in accuracy. Moreover, unlike other compression methods, the nested capability of our approach allows having a many-in-one reasoning model that has constant deployment memory against the number of models in the family. Training a family of large language models targeting multiple scales and deployment objectives is prohibitively expensive, requiring separate training runs for each different size. Recent work on model compression through pruning and knowledge distillation has reduced this cost; however, this process still incurs hundreds of billions of tokens worth of training cost per compressed model. In this paper, we present Nemotron Elastic, a framework for building reasoning-oriented LLMs, including hybrid Mamba-Attention architectures, that embed multiple nested submodels within a single parent model, each optimized for different deployment configurations and budgets. Each of these submodels shares weights with the parent model and can be extracted zero-shot during deployment without additional training or fine-tuning. We enable this functionality through an end-to-end trained router, tightly coupled to a two-stage training curriculum designed specifically for reasoning models. We additionally introduce group-aware SSM elastification that preserves Mamba's structural constraints, heterogeneous MLP elastification, normalized MSE-based layer importance for improved depth selection, and knowledge distillation enabling simultaneous multi-budget optimization. We apply Nemotron Elastic to the Nemotron Nano V2 12B model, simultaneously producing a 9B and a 6B model using only 110B training tokens; this results in over 360x cost reduction compared to training model families from scratch, and around 7x compared to SoTA compression techniques. Each of the nested models performs on par or better than the SoTA in accuracy. Moreover, unlike other compression methods, the nested capability of our approach allows having a many-in-one reasoning model that has constant deployment memory against the number of models in the family. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend arXiv explained breakdown of this paper ðŸ‘‰https://arxivexplained.com/papers/nemotron-elastic-towards-efficient-many-in-one-reasoning-llms Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2547036",
    "title": "Nemotron Elastic: Towards Efficient Many-in-One Reasoning LLMs",
    "authors": [],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2511.16664",
    "upvote": 26
  }
}