{
  "context": "In-context learning (ICL) -- the capacity of a model to infer and apply abstract patterns from examples provided within its input -- has been extensively studied in large language models trained for next-token prediction on human text. In fact, prior work often attributes this emergent behavior to distinctive statistical properties in human language. This raises a fundamental question: can ICL arise organically in other sequence domains purely through large-scale predictive training?\n  To explore this, we turn to genomic sequences, an alternative symbolic domain rich in statistical structure. Specifically, we study the Evo2 genomic model, trained predominantly on next-nucleotide (A/T/C/G) prediction, at a scale comparable to mid-sized LLMs. We develop a controlled experimental framework comprising symbolic reasoning tasks instantiated in both linguistic and genomic forms, enabling direct comparison of ICL across genomic and linguistic models. Our results show that genomic models, like their linguistic counterparts, exhibit log-linear gains in pattern induction as the number of in-context demonstrations increases. To the best of our knowledge, this is the first evidence of organically emergent ICL in genomic sequences, supporting the hypothesis that ICL arises as a consequence of large-scale predictive modeling over rich data. These findings extend emergent meta-learning beyond language, pointing toward a unified, modality-agnostic view of in-context learning. For years since the GPT-2 paper, emergent in-context learning (ICL) has been treated as something deeply tied to human language. But â€¦ what if ICL isnâ€™t uniquely tied human lang? Here is our finding: ğ—šğ—²ğ—»ğ—¼ğ—ºğ—¶ğ—°ğŸ§¬ ğ—ºğ—¼ğ—±ğ—²ğ—¹ğ˜€ ğ˜ğ—¿ğ—®ğ—¶ğ—»ğ—²ğ—± ğ™¤ğ™£ğ™¡ğ™® ğ—¼ğ—» 'ğ—»ğ—²ğ˜…ğ˜-ğ—»ğ˜‚ğ—°ğ—¹ğ—²ğ—¼ğ˜ğ—¶ğ—±ğ—² ğ—½ğ—¿ğ—²ğ—±ğ—¶ğ—°ğ˜ğ—¶ğ—¼ğ—»' ğ—²ğ˜…ğ—µğ—¶ğ—¯ğ—¶ğ˜ ğ—œğ—–ğ—Ÿ! What's remarkable is that their overall pattern closely mirrors LLMs:â†’ similar few-shot pattern inductionâ†’ similar log-linear gains with more shotsâ†’ similar improvement with model scale... all learned purely from DNA (nucleotide) sequences. ğ—›ğ—¼ğ˜„ ğ—±ğ—¶ğ—± ğ˜„ğ—² ğ—°ğ—¼ğ—ºğ—½ğ—®ğ—¿ğ—² ğ—´ğ—²ğ—»ğ—¼ğ—ºğ—¶ğ—° ğ˜ƒğ˜€ ğ—¹ğ—®ğ—»ğ—´ğ˜‚ğ—®ğ—´ğ—² ğ—ºğ—¼ğ—±ğ—²ğ—¹ğ˜€? We built a suite of symbolic bitstring-reasoning tasks and encoded them two ways: (1) genomic alphabet (A/T/C/G) and (2) linguistic alphabet (digits). This lets us compare Evo2 (genomic) vs Qwen3 (language) under matched few-shot prompts. ğ—ªğ—µğ˜† ğ—¶ğ˜ ğ—ºğ—®ğ˜ğ˜ğ—²ğ—¿ğ˜€: To our knowledge, this is the first evidence of emergent ICL in non-[human]language symbolic sequences. It suggests that ICL is modality-agnostic, and a general consequence of large-scale autoregressive training on rich data distributions. ğ——ğ—¼ğ—²ğ˜€ ğ—œğ—–ğ—Ÿ ğ—¶ğ—» ğ—´ğ—²ğ—»ğ—¼ğ—ºğ—¶ğ—° ğ˜ƒğ˜€ ğ—¹ğ—®ğ—»ğ—´ğ˜‚ğ—®ğ—´ğ—² ğ—ºğ—¼ğ—±ğ—²ğ—¹ğ˜€ ğ—®ğ—°ğ˜ ğ—¶ğ—±ğ—²ğ—»ğ˜ğ—¶ğ—°ğ—®ğ—¹ğ—¹ğ˜†? No! They share macro-level ICL trends, but each shows domain-specific inductive biases traceable to properties of DNA vs human language. ğ——ğ—¼ğ—²ğ˜€ ğ˜ğ—µğ—¶ğ˜€ ğ—ºğ—²ğ—®ğ—» ğ—µğ˜‚ğ—ºğ—®ğ—» ğ—¹ğ—®ğ—»ğ—´ğ˜‚ğ—®ğ—´ğ—² ğ˜€ğ˜ğ—¿ğ˜‚ğ—°ğ˜ğ˜‚ğ—¿ğ—² ğ—¶ğ˜€ ğ—¶ğ—¿ğ—¿ğ—²ğ—¹ğ—²ğ˜ƒğ—®ğ—»ğ˜? No! But it suggests there may be universal distributional properties across different languages (human, DNA, etc.) that yield ICL. It remains an open question what these properties are. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2547069",
    "title": "Genomic Next-Token Predictors are In-Context Learners",
    "authors": [
      "Aayush Mishra"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2511.12797",
    "upvote": 7
  }
}