{
  "context": "RBTransformer, a Transformer-based model, enhances EEG-based emotion recognition by capturing inter-cortical neural dynamics in latent space, outperforming existing methods on multiple datasets and dimensions. Human emotions are difficult to convey through words and are often abstracted in the process; however, electroencephalogram (EEG) signals can offer a more direct lens into emotional brain activity. Recent studies show that deep learning models can process these signals to perform emotion recognition with high accuracy. However, many existing approaches overlook the dynamic interplay between distinct brain regions, which can be crucial to understanding how emotions unfold and evolve over time, potentially aiding in more accurate emotion recognition. To address this, we proposeRBTransformer, aTransformer-based neural networkarchitecture that modelsinter-cortical neural dynamicsof the brain inlatent spaceto better capture structured neural interactions for effective EEG-based emotion recognition. First, the EEG signals are converted intoBand Differential Entropy (BDE) tokens, which are then passed throughElectrode Identity embeddingsto retain spatial provenance. These tokens are processed through successiveinter-cortical multi-head attention blocksthat construct an electrode x electrode attention matrix, allowing the model to learn the inter-cortical neural dependencies. The resulting features are then passed through aclassification headto obtain the final prediction. We conducted extensive experiments, specifically under subject-dependent settings, on the SEED, DEAP, andDREAMER datasets, over all three dimensions,Valence,Arousal, andDominance(for DEAP and DREAMER), under both binary and multi-class classification settings. The results demonstrate that the proposedRBTransformeroutperforms all previous state-of-the-art methods across all three datasets, over all three dimensions under both classification settings. The source code is available at: https://github.com/nnilayy/RBTransformer. We release RBTransformer, a Transformer-based architecture for EEG-based emotion recognition that models inter-cortical neural interactions using our proposed Inter-Cortical Multi-Head Self-Attention (Inter-Cortical MHSA) mechanism, capturing structured communication among distributed cortical regions during affective processing. RBTransformer achieves state-of-the-art performance across all three major EEG benchmarks: SEED, DEAP, and DREAMER, across all dimensions (Valence, Arousal, Dominance) and under both binary and multi-class settings, consistently surpassing prior deep learning and attention-based models. Alongside the paper, we provide a full open-source ecosystem for reproducibility GitHub Codebase:https://github.com/nnilayy/RBTransformerComplete training pipeline, preprocessing framework, model architecture & ablation scripts. Hugging Face ðŸ¤— Model Weights Collection:https://huggingface.co/nnilayy/collectionsAll trained checkpoints, ablation variants, and dataset-specific model collections for SEED, DEAP, and DREAMER. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2547083",
    "title": "A Brain Wave Encodes a Thousand Tokens: Modeling Inter-Cortical Neural Interactions for Effective EEG-based Emotion Recognition",
    "authors": [
      "Nilay Kumar"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/nnilayy/rbtransformer",
    "huggingface_url": "https://huggingface.co/papers/2511.13954",
    "upvote": 3
  }
}