{
  "context": "Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse domains, but their training remains resource- and time-intensive, requiring massive compute power and careful orchestration of training procedures. Model souping-the practice of averaging weights from multiple models of the same architecture-has emerged as a promising pre- and post-training technique that can enhance performance without expensive retraining. In this paper, we introduce Soup Of Category Experts (SoCE), a principled approach for model souping that utilizes benchmark composition to identify optimal model candidates and applies non-uniform weighted averaging to maximize performance. Contrary to previous uniform-averaging approaches, our method leverages the observation that benchmark categories often exhibit low inter-correlations in model performance. SoCE identifies \"expert\" models for each weakly-correlated category cluster and combines them using optimized weighted averaging rather than uniform weights. We demonstrate that the proposed method improves performance and robustness across multiple domains, including multilingual capabilities, tool calling, and math and achieves state-of-the-art results on the Berkeley Function Calling Leaderboard. outperforming uniform averaging is really hard. congrats, this is huge!! This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend arXiv explained breakdown of this paper ðŸ‘‰https://arxivexplained.com/papers/souper-model-how-simple-arithmetic-unlocks-state-of-the-art-llm-performance arXiv lens breakdown of this paper ðŸ‘‰https://arxivlens.com/PaperView/Details/souper-model-how-simple-arithmetic-unlocks-state-of-the-art-llm-performance-5226-3c3d929c Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2547003",
    "title": "Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance",
    "authors": [
      "Shalini Maiti",
      "Amar Budhiraja",
      "Bhavul Gauri",
      "Gaurav Chaurasia"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/facebookresearch/llm_souping",
    "huggingface_url": "https://huggingface.co/papers/2511.13254",
    "upvote": 136
  }
}