{
  "context": "ATLAS, a large-scale, cross-disciplinary evaluation suite, addresses the limitations of existing benchmarks by providing high-difficulty, original, and high-fidelity scientific problems to assess the reasoning capabilities of Large Language Models. The rapid advancement ofLarge Language Models(LLMs) has led to performance saturation on many established benchmarks, questioning their ability to distinguish frontier models. Concurrently, existing high-difficulty benchmarks often suffer from narrow disciplinary focus, oversimplified answer formats, and vulnerability to data contamination, creating a fidelity gap with real-worldscientific inquiry. To address these challenges, we introduceATLAS(AGI-Oriented TestbedforLogical Applicationin Science), a large-scale, high-difficulty, and cross-disciplinary evaluation suite composed of approximately 800 original problems. Developed bydomain experts(PhD-level and above),ATLASspans seven core scientific fields: mathematics, physics, chemistry, biology, computer science, earth science, and materials science. Its key features include: (1) High Originality and Contamination Resistance, with all questions newly created or substantially adapted to prevent test data leakage; (2) Cross-Disciplinary Focus, designed to assess models' ability to integrate knowledge and reason across scientific domains; (3) High-Fidelity Answers, prioritizing complex, open-ended answers involvingmulti-step reasoningandLaTeX-formatted expressionsover simple multiple-choice questions; and (4) Rigorous Quality Control, employing a multi-stage process ofexpert peer reviewandadversarial testingto ensure question difficulty, scientific value, and correctness. We also propose a robust evaluation paradigm using a panel ofLLM judgesfor automated, nuanced assessment of complex answers. Preliminary results on leading models demonstrateATLAS's effectiveness in differentiating their advanced scientific reasoning capabilities. We plan to developATLASinto a long-term, open, community-driven platform to provide a reliable \"ruler\" for progress toward Artificial General Intelligence. The rapid advancement of Large Language Models (LLMs) has led to performance saturation on many established benchmarks, questioning their ability to distinguish frontier models. Concurrently, existing high-difficulty benchmarks often suffer from narrow disciplinary focus, oversimplified answer formats, and vulnerability to data contamination, creating a fidelity gap with real-world scientific inquiry. To address these challenges, we introduce ATLAS (AGI-Oriented Testbed for Logical Application in Science), a large-scale, high-difficulty, and cross-disciplinary evaluation suite composed of approximately 800 original problems. Developed by domain experts (PhD-level and above), ATLAS spans seven core scientific fields: mathematics, physics, chemistry, biology, computer science, earth science, and materials science. Its key features include: (1) High Originality and Contamination Resistance, with all questions newly created or substantially adapted to prevent test data leakage; (2) Cross-Disciplinary Focus, designed to assess models' ability to integrate knowledge and reason across scientific domains; (3) High-Fidelity Answers, prioritizing complex, open-ended answers involving multi-step reasoning and LaTeX-formatted expressions over simple multiple-choice questions; and (4) Rigorous Quality Control, employing a multi-stage process of expert peer review and adversarial testing to ensure question difficulty, scientific value, and correctness. We also propose a robust evaluation paradigm using a panel of LLM judges for automated, nuanced assessment of complex answers. Preliminary results on leading models demonstrate ATLAS's effectiveness in differentiating their advanced scientific reasoning capabilities. We plan to develop ATLAS into a long-term, open, community-driven platform to provide a reliable \"ruler\" for progress toward Artificial General Intelligence. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2547051",
    "title": "ATLAS: A High-Difficulty, Multidisciplinary Benchmark for Frontier Scientific Reasoning",
    "authors": [
      "Junnan Liu",
      "Shudong Liu",
      "Xinyu Fang",
      "Zihan Ma"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2511.14366",
    "upvote": 16
  }
}