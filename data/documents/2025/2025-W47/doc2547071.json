{
  "context": "EntroPIC, a novel reinforcement learning method, adaptsively tunes loss coefficients to stabilize entropy during long-term training of large language models, ensuring efficient exploration and optimal training. Long-term training oflarge language models(LLMs) requires maintaining stable exploration to prevent the model from collapsing into sub-optimal behaviors.Entropyis crucial in this context, as it controls exploration and helps avoid premature convergence to sub-optimal solutions. However, existingreinforcement learningmethods struggle to maintain an appropriate level ofentropy, as the training process involves a mix ofpositive and negative samples, each affectingentropyin different ways across steps. To address this, we proposeEntropystablilization viaProportional-Integral Control(EntroPIC), a novel method that adaptively adjusts the influence ofpositive and negative samplesby dynamically tuning their loss coefficients. This approach stabilizesentropythroughout training, ensuring efficient exploration and steady progress. We provide a comprehensive theoretical analysis for both on-policy andoff-policy learningsettings, demonstrating that EntroPIC is effective at controllingentropyin large-scale LLM training. Experimental results show that our method successfully maintains desiredentropylevels, enabling stable and optimal RL training for LLMs. Long-term training of large language models (LLMs) requires maintaining stable exploration to prevent the model from collapsing into sub-optimal behaviors. Entropy is crucial in this context, as it controls exploration and helps avoid premature convergence to sub-optimal solutions. However, existing reinforcement learning methods struggle to maintain an appropriate level of entropy, as the training process involves a mix of positive and negative samples, each affecting entropy in different ways across steps. To address this, we propose Entropy stablilization via Proportional-Integral Control (EntroPIC), a novel method that adaptively adjusts the influence of positive and negative samples by dynamically tuning their loss coefficients. This approach stabilizes entropy throughout training, ensuring efficient exploration and steady progress. We provide a comprehensive theoretical analysis for both on-policy and off-policy learning settings, demonstrating that EntroPIC is effective at controlling entropy in large-scale LLM training. Experimental results show that our method successfully maintains desired entropy levels, enabling stable and optimal RL training for LLMs. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2547071",
    "title": "EntroPIC: Towards Stable Long-Term Training of LLMs via Entropy Stabilization with Proportional-Integral Control",
    "authors": [
      "Kai Yang",
      "Xin Xu",
      "Zichuan Lin"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/yk7333/EntroPIC",
    "huggingface_url": "https://huggingface.co/papers/2511.15248",
    "upvote": 6
  }
}