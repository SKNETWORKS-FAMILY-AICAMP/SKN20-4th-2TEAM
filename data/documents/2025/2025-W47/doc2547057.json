{
  "context": "MarsRL enhances multi-agent reasoning systems by optimizing all agents jointly, improving accuracy in complex reasoning tasks. Recent progress in large language models (LLMs) has been propelled byreinforcement learning with verifiable rewards(RLVR) andtest-time scaling. However, the limited output length of LLMs constrains the depth of reasoning attainable in a single inference process.Multi-agent reasoning systemsoffer a promising alternative by employing multiple agents includingSolver,Verifier, andCorrector, to iteratively refine solutions. While effective in closed-source models like Gemini 2.5 Pro, they struggle to generalize to open-source models due to insufficient critic and correction capabilities. To address this, we propose MarsRL, a novel reinforcement learning framework withagentic pipeline parallelism, designed to jointly optimize all agents in the system. MarsRL introducesagent-specific reward mechanismsto mitigate reward noise and employspipeline-inspired trainingto enhance efficiency in handling long trajectories. Applied to Qwen3-30B-A3B-Thinking-2507, MarsRL improvesAIME2025accuracy from 86.5% to 93.3% andBeyondAIMEfrom 64.9% to 73.8%, even surpassing Qwen3-235B-A22B-Thinking-2507. These findings highlight the potential of MarsRL to advancemulti-agent reasoning systemsand broaden their applicability across diverse reasoning tasks. Recent progress in large language models (LLMs) has been propelled by reinforcement learning with verifiable rewards (RLVR) and test-time scaling. However, the limited output length of LLMs constrains the depth of reasoning attainable in a single inference process. Multi-agent reasoning systems offer a promising alternative by employing multiple agents including Solver, Verifier, and Corrector, to iteratively refine solutions. While effective in closed-source models like Gemini 2.5 Pro, they struggle to generalize to open-source models due to insufficient critic and correction capabilities. To address this, we propose MarsRL, a novel reinforcement learning framework with agentic pipeline parallelism, designed to jointly optimize all agents in the system. MarsRL introduces agent-specific reward mechanisms to mitigate reward noise and employs pipeline-inspired training to enhance efficiency in handling long trajectories. Applied to Qwen3-30B-A3B-Thinking-2507, MarsRL improves AIME2025 accuracy from 86.5% to 93.3% and BeyondAIME from 64.9% to 73.8%, even surpassing Qwen3-235B-A22B-Thinking-2507. These findings highlight the potential of MarsRL to advance multi-agent reasoning systems and broaden their applicability across diverse reasoning tasks. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend what does this model specialize in The model in this work was trained for math reasoning, but the approach can be used in general reasoning tasks . Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2547057",
    "title": "MarsRL: Advancing Multi-Agent Reasoning System via Reinforcement Learning with Agentic Pipeline Parallelism",
    "authors": [
      "Dong Du"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/liushulinle/MarsRL",
    "huggingface_url": "https://huggingface.co/papers/2511.11373",
    "upvote": 12
  }
}