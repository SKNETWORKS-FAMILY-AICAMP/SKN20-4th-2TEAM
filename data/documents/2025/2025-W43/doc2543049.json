{
  "context": "3DThinker is a framework that enhances multimodal reasoning by integrating 3D spatial understanding from images without requiring 3D prior input or labeled data. Though recent advances invision-language models(VLMs) have achieved\nremarkable progress across a wide range of multimodal tasks, understanding 3D\nspatial relationships from limited views remains a significant challenge.\nPrevious reasoning methods typically rely on pure text (e.g., topological\ncognitive maps) or on 2D visual cues. However, their limited representational\ncapacity hinders performance in specific tasks that require 3D spatial\nimagination. To address this limitation, we propose 3DThinker, a framework that\ncan effectively exploits the rich geometric information embedded within images\nwhile reasoning, like humans do. Our framework is the first to enable 3D\nmentaling during reasoning without any 3D prior input, and it does not rely on\nexplicitly labeled 3D data for training. Specifically, our training consists of\ntwo stages. First, we perform supervised training to align the3D latentgenerated by VLM while reasoning with that of a3D foundation model(e.g.,VGGT). Then, we optimize the entire reasoning trajectory solely based on\noutcome signals, thereby refining the underlying3D mentaling. Extensive\nexperiments across multiple benchmarks show that 3DThinker consistently\noutperforms strong baselines and offers a new perspective toward unifying 3D\nrepresentations intomultimodal reasoning. Our code will be available at\nhttps://github.com/zhangquanchen/3DThinker. Though recent advances in vision-language models (VLMs) have achieved remarkable progress across a wide range of multimodal tasks, understanding 3D spatial relationships from limited views remains a significant challenge. Previous reasoning methods typically rely on pure text (e.g., topological cognitive maps) or on 2D visual cues. However, their limited representational capacity hinders performance in specific tasks that require 3D spatial imagination. To address this limitation, we propose 3DThinker, a framework that can effectively exploits the rich geometric information embedded within images while reasoning, like humans do. Our framework is the first to enable 3D mentaling during reasoning without any 3D prior input, and it does not rely on explicitly labeled 3D data for training. Specifically, our training consists of two stages. First, we perform supervised training to align the 3D latent generated by VLM while reasoning with that of a 3D foundation model (e.g., VGGT). Then, we optimize the entire reasoning trajectory solely based on outcome signals, thereby refining the underlying 3D mentaling. Extensive experiments across multiple benchmarks show that 3DThinker consistently outperforms strong baselines and offers a new perspective toward unifying 3D representations into multimodal reasoning. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2543049",
    "title": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from\n  Limited Views",
    "authors": [
      "Zhangquan Chen",
      "Xinlei Yu",
      "Zihao Pan"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/zhangquanchen/3DThinker",
    "huggingface_url": "https://huggingface.co/papers/2510.18632",
    "upvote": 21
  }
}