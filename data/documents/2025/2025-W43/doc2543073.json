{
  "context": "ssToken, a self-modulated and semantic-aware token selection approach, enhances supervised fine-tuning of large language models by adaptively selecting tokens and providing complementary semantic information, outperforming existing methods. Data quality plays a critical role in enhancingsupervised fine-tuning(SFT)\nforlarge language models(LLMs), and token-level data selection has emerged as\na promising direction for its fine-grained nature. Despite their strong\nempirical performance, existingtoken-level selectionmethods share two key\nlimitations: (1) requiring training or accessing an additional reference model,\nand (2) relying solely on loss information for token selection, which cannot\nwell preserve semantically important tokens that are not favored by loss-based\nmetrics. To address these challenges, we propose ssToken, a Self-modulated andSemantic-awareToken Selection approach. ssToken leverages readily accessiblehistory modelsto compute theper-token loss differencewith the current model,\nwhich serves as aself-modulated signalthat enables the model to adaptively\nselect tokens along its optimization trajectory, rather than relying on excess\nloss from an offline-trained reference model as in prior works. We further\nintroduce asemantic-aware,attention-basedtoken importance estimationmetric,\northogonal to loss-based selection and providing complementary semantic\ninformation for more effective filtering. Extensive experiments across\ndifferent model families and scales demonstrate that both self-modulated\nselection andsemantic-awareselection alone outperformfull-data fine-tuning,\nwhile their integration--ssToken--achievessynergistic gainsand further\nsurpasses priortoken-level selectionmethods, delivering performance\nimprovements while maintaining training efficiency. Data quality plays a critical role in enhancing supervised fine-tuning (SFT) for large language models (LLMs), and token-level data selection has emerged as a promising direction for its fine-grained nature. Despite their strong empirical performance, existing token-level selection methods share two key limitations: (1) requiring training or accessing an additional reference model, and (2) relying solely on loss information for token selection, which cannot well preserve semantically important tokens that are not favored by loss-based metrics. To address these challenges, we propose ssToken, a Self-modulated and Semantic-aware Token Selection approach. ssToken leverages readily accessible history models to compute the per-token loss difference with the current model, which serves as a self-modulated signal that enables the model to adaptively select tokens along its optimization trajectory, rather than relying on excess loss from an offline-trained reference model as in prior works. We further introduce a semantic-aware, attention-based token importance estimation metric, orthogonal to loss-based selection and providing complementary semantic information for more effective filtering. Extensive experiments across different model families and scales demonstrate that both self-modulated selection and semantic-aware selection alone outperform full-data fine-tuning, while their integration--ssToken--achieves synergistic gains and further surpasses prior token-level selection methods, delivering performance improvements while maintaining training efficiency. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2543073",
    "title": "ssToken: Self-modulated and Semantic-aware Token Selection for LLM\n  Fine-tuning",
    "authors": [
      "Xiaohan Qin"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/jianke0604/ssToken",
    "huggingface_url": "https://huggingface.co/papers/2510.18250",
    "upvote": 13
  }
}