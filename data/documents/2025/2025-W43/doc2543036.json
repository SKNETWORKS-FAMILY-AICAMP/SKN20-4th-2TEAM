{
  "context": "A unified reinforcement and imitation learning algorithm creates efficient, lightweight vision-language models that match or exceed leading VLMs in performance. Vision-Language Models (VLMs)have achieved remarkable progress, yet their\nlarge scale often renders them impractical for resource-constrained\nenvironments. This paper introduces Unified Reinforcement and Imitation\nLearning (RIL), a novel and efficient training algorithm designed to create\npowerful, lightweight VLMs. RIL distinctively combines the strengths ofreinforcement learningwithadversarial imitation learning. This enables\nsmaller student VLMs not only to mimic the sophisticatedtext generationof\nlarge teacher models but also to systematically improve their generative\ncapabilities through reinforcement signals. Key to our imitation framework is\nanLLM-based discriminatorthat adeptly distinguishes between student and\nteacher outputs, complemented by guidance from multiple large teacher VLMs to\nensure diverse learning. This unified learning strategy, leveraging both\nreinforcement and imitation, empowers student models to achieve significant\nperformance gains, making them competitive with leading closed-source VLMs.\nExtensive experiments on diversevision-language benchmarksdemonstrate that\nRIL significantly narrows the performance gap with state-of-the-art open- and\nclosed-source VLMs and, in several instances, surpasses them. ArXiv:https://arxiv.org/abs/2510.19307Project page:https://byungkwanlee.github.io/RIL-page/ Unified Learning: Combines reinforcement learning (GRPO) and imitation learning (GAIL) to help small VLMs mimic both how and what to generate from larger teacher models. Dual Reward System: Integrates a discriminator-based similarity reward with LLM-as-a-Judge accuracy feedback, ensuring responses are both stylistically aligned and factually correct. Teacher Diversity: Learns from multiple large teacher VLMs (e.g., Qwen2.5-VL-72B and InternVL3-78B), improving robustness and generalization. No “think” phase: RIL-trained models keep the same fast inference speed as standard models — ideal for deployment in mobile and resource-constrained environments. Hi, very impressive results.Do you have the ablation study of the following: Also, what the metric of the discriminator? A1. If my understanding is correct, then LLM judge using Dr.GRPO is already reported in Table 1 and Table 5(d).Please note that LLM judge + discriminator using Dr. GRPO is just our proposed method. A2.  We didn't directly compare token-wise distillation with our RIL method, because it is different training mechanism where distillation is closer to SFT, but RIL is closer to RL. However, in Table 5(f), we show that employing RIL method to the distillation-based models works also well. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend ·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2543036",
    "title": "Unified Reinforcement and Imitation Learning for Vision-Language Models",
    "authors": [
      "Byung-Kwan Lee"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2510.19307",
    "upvote": 30
  }
}