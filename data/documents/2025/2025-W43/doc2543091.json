{
  "context": "PokeeResearch-7B, a 7B-parameter deep research agent, achieves state-of-the-art performance using reinforcement learning and chain-of-thought reasoning to enhance robustness and alignment. Tool-augmented large language models (LLMs) are emerging as deep research\nagents, systems that decompose complex queries, retrieve external evidence, and\nsynthesize grounded responses. Yet current agents remain limited by shallow\nretrieval, weak alignment metrics, and brittle tool-use behavior. We introduce\nPokeeResearch-7B, a 7B-parameter deep research agent built under a unifiedreinforcement learningframework for robustness, alignment, and scalability.\nPokeeResearch-7B is trained by an annotation-freeReinforcement Learningfrom\nAI Feedback (RLAIF) framework to optimize policies using LLM-based reward\nsignals that capture factual accuracy, citation faithfulness, and instruction\nadherence. A chain-of-thought-driven multi-call reasoning scaffold further\nenhances robustness throughself-verificationandadaptive recoveryfrom tool\nfailures. Among 10 populardeep research benchmarks, PokeeResearch-7B achieves\nstate-of-the-art performance among 7B-scaledeep research agents. This\nhighlights that carefulreinforcement learningand reasoning design can produce\nefficient, resilient, and research-grade AI agents. The model and inference\ncode is open-sourced under MIT license at\nhttps://github.com/Pokee-AI/PokeeResearchOSS. Tool-augmented large language models (LLMs) are emerging as deep research agents, systems that decompose complex queries, retrieve external evidence, and synthesize grounded responses. Yet current agents remain limited by shallow retrieval, weak alignment metrics, and brittle tool-use behavior. We introduce PokeeResearch-7B, a 7B-parameter deep research agent built under a unified reinforcement learning framework for robustness, alignment, and scalability. PokeeResearch-7B is trained by an annotation-free Reinforcement Learning from AI Feedback (RLAIF) framework to optimize policies using LLM-based reward signals that capture factual accuracy, citation faithfulness, and instruction adherence. A chain-of-thought-driven multi-call reasoning scaffold further enhances robustness through self-verification and adaptive recovery from tool failures. Among 10 popular deep research benchmarks, PokeeResearch-7B achieves state-of-the-art performance among 7B-scale deep research agents. This highlights that careful reinforcement learning and reasoning design can produce efficient, resilient, and research-grade AI agents. The model and inference code is open-sourced under Apache 2.0 license at this https URL. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2543091",
    "title": "PokeeResearch: Effective Deep Research via Reinforcement Learning from\n  AI Feedback and Robust Reasoning Scaffold",
    "authors": [
      "Jiuqi Wang"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/Pokee-AI/PokeeResearchOSS",
    "huggingface_url": "https://huggingface.co/papers/2510.15862",
    "upvote": 9
  }
}