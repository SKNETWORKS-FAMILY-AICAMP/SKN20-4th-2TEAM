{
  "context": "BAlanced Policy Optimization with Adaptive Clipping (BAPO) addresses challenges in off-policy reinforcement learning by dynamically adjusting clipping bounds to improve sample efficiency, stability, and performance in large language models. Reinforcement learning(RL) has recently become the core paradigm for\naligning and strengtheninglarge language models(LLMs). Yet, applying RL inoff-policy settings--where stale data from past policies are used for\ntraining--improves sample efficiency, but remains challenging:policy entropydeclines sharply, optimization often becomes unstable and may even collapse.\nThrough theoretical and empirical analysis, we identify two key insights: (i)\nan imbalance in optimization, where negative-advantage samples dominate thepolicy gradient, suppressing useful behaviors and riskinggradient explosions;\nand (ii) the derivedEntropy-Clip Rule, which reveals that the fixed clipping\nmechanism inPPO-like objectivessystematically blocks entropy-increasing\nupdates, thereby driving the policy toward over-exploitation at the expense of\nexploration. Building on these insights, we propose BAlanced Policy\nOptimization with Adaptive Clipping (BAPO), a simple yet effective method that\ndynamically adjusts clipping bounds to adaptively re-balance positive and\nnegative contributions, preserve entropy, and stabilize RL optimization. Across\ndiverse off-policy scenarios--includingsample replayandpartial rollout--BAPO\nachieves fast, stable, and data-efficient training. OnAIME 2024andAIME 2025benchmarks, our 7B BAPO model surpasses open-source counterparts such asSkyWork-OR1-7B, while our 32B BAPO model not only achieves state-of-the-art\nresults among models of the same scale but also outperforms leading proprietary\nsystems likeo3-miniandGemini-2.5-Flash-Thinking. BAlanced Policy Optimization with Adaptive Clipping (BAPO) addresses challenges in off-policy reinforcement learning by dynamically adjusting clipping bounds to improve sample efficiency, stability, and performance in large language models. Hi, thanks for the great paper In the paper, Proposition 2 (Eq. 6) in the appendix writes the logit difference without the pi term, while Proposition 1 includes it - unlike the derivation in Entropy Mechanism (https://arxiv.org/pdf/2505.22617), where pi appears explicitly. Is this an intentional simplification (e.g., assuming natural gradient), or just a notational omission? This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2543008",
    "title": "BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via\n  Balanced Policy Optimization with Adaptive Clipping",
    "authors": [
      "Junrui Shen",
      "Honglin Guo",
      "Miao Zheng",
      "Guoteng Wang",
      "Shuo Zhang",
      "Peng Sun"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/WooooDyy/BAPO",
    "huggingface_url": "https://huggingface.co/papers/2510.18927",
    "upvote": 83
  }
}