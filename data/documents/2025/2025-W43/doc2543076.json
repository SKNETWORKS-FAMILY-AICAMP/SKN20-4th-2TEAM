{
  "context": "Decomposed Attention Fusion (DecAF) enhances video object segmentation by refining attention maps from multimodal large language models without retraining. Multimodal large language models(MLLMs) demonstrate strong video\nunderstanding by attending to visual tokens relevant to textual queries. To\ndirectly adapt this for localization in a training-free manner, we cast video\nreasoning segmentation as avideo QA taskand extractattention mapsviarollout mechanism. However, rawattention mapsare noisy and poorly aligned\nwith object regions. We propose Decomposed Attention Fusion (DecAF), which\nrefines these maps through two mechanisms: (1) contrastive object-background\nfusion and (2)complementary video-frame fusion. This method suppresses\nirrelevant activations and enhances object-focused cues, enabling direct\nconversion ofattention mapsinto coarse segmentation masks. In addition, we\nintroduceattention-guided SAM2 promptingfor obtaining fine-grained masks.\nUnlike existing methods that jointly train MLLMs with SAM, our method operates\nentirely without retraining. DecAF outperforms training-free methods and\nachieves performance comparable to training-based methods on both referring andreasoning VOS benchmarks. The code will be available at\nhttps://github.com/HYUNJS/DecAF. We introduce DecAF (Decomposed Attention Fusion) — a training-free framework that transforms MLLMs’ attention maps into video segmentation. DecAF refines noisy attention through two attention fusion mechanisms: (1) Contrastive object-background fusion and (2) Complementary video-frame fusion.Additionally, with our attention-guided SAM2 prompting strategy, DecAF obtains fine-grained masks and achieves performance comparable to training-based methods — all without retraining.  This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend ·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2543076",
    "title": "Decomposed Attention Fusion in MLLMs for Training-Free Video Reasoning\n  Segmentation",
    "authors": [
      "Jeongseok Hyun"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/HYUNJS/DecAF",
    "huggingface_url": "https://huggingface.co/papers/2510.19592",
    "upvote": 12
  }
}