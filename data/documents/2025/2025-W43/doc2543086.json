{
  "context": "A vision-guided 3D layout generation system uses an image generation model and scene graphs to produce rich and coherent 3D scenes from prompts. Generating artistic and coherent 3D scene layouts is crucial in digital\ncontent creation. Traditional optimization-based methods are often constrained\nby cumbersome manual rules, while deep generative models face challenges in\nproducing content with richness and diversity. Furthermore, approaches that\nutilize large language models frequently lack robustness and fail to accurately\ncapture complex spatial relationships. To address these challenges, this paper\npresents a novel vision-guided3D layout generationsystem. We first construct\na high-quality asset library containing 2,037 scene assets and 147 3D scene\nlayouts. Subsequently, we employ animage generation modelto expand prompt\nrepresentations into images, fine-tuning it to align with our asset library. We\nthen develop a robustimage parsing moduleto recover the 3D layout of scenes\nbased onvisual semanticsandgeometric information. Finally, we optimize the\nscene layout usingscene graphsand overallvisual semanticsto ensure logical\ncoherence and alignment with the images. Extensive user testing demonstrates\nthat our algorithm significantly outperforms existing methods in terms of\nlayout richness and quality. The code and dataset will be available at\nhttps://github.com/HiHiAllen/Imaginarium. Generating artistic and coherent 3D scene layouts is crucial in digital content creation. Traditional optimization-based methods are often constrained by cumbersome manual rules, while deep generative models face challenges in producing content with richness and diversity. Furthermore, approaches that utilize large language models frequently lack robustness and fail to accurately capture complex spatial relationships. To address these challenges, this paper presents a novel vision-guided 3D layout generation system. We first construct a high-quality asset library containing 2,037 scene assets and 147 3D scene layouts. Subsequently, we employ an image generation model to expand prompt representations into images, fine-tuning it to align with our asset library. We then develop a robust image parsing module to recover the 3D layout of scenes based on visual semantics and geometric information. Finally, we optimize the scene layout using scene graphs and overall visual semantics to ensure logical coherence and alignment with the images. Extensive user testing demonstrates that our algorithm significantly outperforms existing methods in terms of layout richness and quality. Github Page:https://github.com/HiHiAllen/Imaginarium This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2543086",
    "title": "Imaginarium: Vision-guided High-Quality 3D Scene Layout Generation",
    "authors": [],
    "publication_year": 2025,
    "github_url": "https://github.com/HiHiAllen/Imaginarium",
    "huggingface_url": "https://huggingface.co/papers/2510.15564",
    "upvote": 10
  }
}