{
  "context": "DaMo, a trainable network optimizing data mixtures for Multimodal Large Language Models, enhances performance across various mobile phone tasks and benchmarks. Mobile Phone Agents (MPAs) have emerged as a promising research direction due\nto their broad applicability across diverse scenarios. While Multimodal Large\nLanguage Models (MLLMs) serve as the foundation for MPAs, their effectiveness\nin handling multiple mobile phone tasks simultaneously remains limited.\nAlthoughmultitask supervised fine-tuning(SFT) is widely adopted for multitask\nlearning, existing approaches struggle to determine optimal training data\ncompositions for peak performance. To address this challenge, we proposeDaMo(Data Mixture Optimizer) - a novel solution employing a trainable network that\npredicts optimal data mixtures by forecasting downstream task performance for\nany given dataset ratio. To support comprehensive evaluation, we introducePhoneAgentBench, the first specialized benchmark to evaluate MLLMs on\nmultimodal mobile phone tasks, comprising 1235 QA pairs spanning diverse\nreal-world industrial mobile application scenarios. Demonstrating strong\npredictive capability (R^2=0.81) in small-scale pilot experiments,DaMoefficiently extrapolates optimal data mixing configurations. Our results showDaMoachieves a 3.38% performance improvement onPhoneAgentBenchcompared to\nalternative methods. Furthermore, extensive experiments across established\nbenchmarks includingBFCL-v3,MME-Reasoning,MME-Perception, andOCRBenchrevealDaMo's superior generalization, outperforming other approaches by 2.57%\nin terms of average score. When used solely for MLLM optimization on theBFCL-v3task,DaMoimproves the metrics by 12.47% than other methods. Notably,DaMomaintains robust scalability, preserving its effectiveness when applied to\nother model architectures. The code and dataset are available at\nhttps://github.com/OPPO-Mente-Lab/DaMo.git One paper to explore the data mixing optimizer for fine-tuning multimodal LLMs is proposed and  PhoneAgentBench for mobile agent evaluation is designed. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2543066",
    "title": "DaMo: Data Mixing Optimizer in Fine-tuning Multimodal LLMs for Mobile\n  Phone Agents",
    "authors": [],
    "publication_year": 2025,
    "github_url": "https://github.com/OPPO-Mente-Lab/DaMo",
    "huggingface_url": "https://huggingface.co/papers/2510.19336",
    "upvote": 16
  }
}