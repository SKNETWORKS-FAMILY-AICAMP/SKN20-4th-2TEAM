{
  "context": "CAD, a technique for long-context large language model training, improves throughput and balance by decoupling and distributing core attention computations. We presentcore attention disaggregation(CAD), a technique that improves\nlong-context large language model training by decoupling thecore attentioncomputation,softmax(QK^T)V, from the rest of the model and executing it on a\nseparate pool of devices. In existing systems,core attentionis colocated with\nother layers; at long context lengths, its quadratic compute growth compared to\nthe near-linear growth of other components causes load imbalance and stragglers\nacross data andpipeline parallelgroups. CAD is enabled by two observations.\nFirst,core attentionis stateless: it has no trainable parameters and only\nminimal transient data, so balancing reduces to scheduling compute-bound tasks.\nSecond, it is composable: modern attention kernels retain high efficiency when\nprocessing fused batches oftoken-level shardswith arbitrary lengths. CAD\npartitionscore attentioninto token-level tasks and dispatches them to\ndedicatedattention servers, which dynamically rebatch tasks to equalize\ncompute without sacrificing kernel efficiency. We implement CAD in a system\ncalledDistCA, which uses aping-pong executionscheme to fully overlap\ncommunication with computation andin-place executiononattention serversto\nreduce memory use. On 512 H200 GPUs and context lengths up to 512k tokens,DistCAimproves end-to-end training throughput by up to 1.35x, eliminates data\nandpipeline parallelstragglers, and achieves near-perfect compute and memory\nbalance. We present core attention disaggregation (CAD), a technique that improves long-context large language model training by decoupling the core attention computation, softmax(QK^T)V, from the rest of the model and executing it on a separate pool of devices. In existing systems, core attention is colocated with other layers; at long context lengths, its quadratic compute growth compared to the near-linear growth of other components causes load imbalance and stragglers across data and pipeline parallel groups. CAD is enabled by two observations. First, core attention is stateless: it has no trainable parameters and only minimal transient data, so balancing reduces to scheduling compute-bound tasks. Second, it is composable: modern attention kernels retain high efficiency when processing fused batches of token-level shards with arbitrary lengths. CAD partitions core attention into token-level tasks and dispatches them to dedicated attention servers, which dynamically rebatch tasks to equalize compute without sacrificing kernel efficiency. We implement CAD in a system called DistCA, which uses a ping-pong execution scheme to fully overlap communication with computation and in-place execution on attention servers to reduce memory use. On 512 H200 GPUs and context lengths up to 512k tokens, DistCA improves end-to-end training throughput by up to 1.35x, eliminates data and pipeline parallel stragglers, and achieves near-perfect compute and memory balance. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Hi, your paper mentioned that you have integrated DistCA into Megatron-LM. Would you be submitting a corresponding PR to Megatron-LM? @larekrowyes, we'll try once the code is more ready arXiv explained breakdown of this paper ðŸ‘‰https://arxivexplained.com/papers/efficient-long-context-language-model-training-by-core-attention-disaggregation Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2543002",
    "title": "Efficient Long-context Language Model Training by Core Attention\n  Disaggregation",
    "authors": [
      "Yonghao Zhuang",
      "Junda Chen",
      "Bo Pang",
      "Yi Gu"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2510.18121",
    "upvote": 122
  }
}