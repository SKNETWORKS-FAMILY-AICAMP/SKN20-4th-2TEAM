{
  "context": "A novel AutoRegressive Generation-based paradigm for image segmentation leverages multimodal large language models and VQ-VAE for dense mask generation, achieving high performance and fast inference. We propose a novelAutoRegressive Generation-based paradigm for image\nSegmentation (ARGenSeg), achieving multimodal understanding and pixel-level\nperception within a unified framework. Prior works integrating image\nsegmentation intomultimodal large language models(MLLMs) typically employ\neitherboundary points representationor dedicatedsegmentation heads. These\nmethods rely ondiscrete representationsorsemantic promptsfed intotask-specific decoders, which limits the ability of the MLLM to capture\nfine-grained visual details. To address these challenges, we introduce a\nsegmentation framework for MLLM based on image generation, which naturally\nproduces dense masks for target objects. We leverage MLLM to output visual\ntokens and detokenize them into images using anuniversal VQ-VAE, making the\nsegmentation fully dependent on the pixel-level understanding of the MLLM. To\nreduce inference latency, we employ anext-scale-prediction strategyto\ngenerate requiredvisual tokensin parallel. Extensive experiments demonstrate\nthat our method surpasses prior state-of-the-art approaches on multiple\nsegmentation datasets with a remarkable boost in inference speed, while\nmaintaining strong understanding capabilities. We propose a novel AutoRegressive Generation-based paradigm for image Segmentation (ARGenSeg), achieving multimodal understanding and pixel-level perception within a unified framework. Prior works integrating image segmentation into multimodal large language models (MLLMs) typically employ either boundary points representation or dedicated segmentation heads. These methods rely on discrete representations or semantic prompts fed into task-specific decoders, which limits the ability of the MLLM to capture fine-grained visual details. To address these challenges, we introduce a segmentation framework for MLLM based on image generation, which naturally produces dense masks for target objects. We leverage MLLM to output visual tokens and detokenize them into images using an universal VQ-VAE, making the segmentation fully dependent on the pixel-level understanding of the MLLM. To reduce inference latency, we employ a next-scale-prediction strategy to generate required visual tokens in parallel. Extensive experiments demonstrate that our method surpasses prior state-of-the-art approaches on multiple segmentation datasets with a remarkable boost in inference speed, while maintaining strong understanding capabilities. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2543088",
    "title": "ARGenSeg: Image Segmentation with Autoregressive Image Generation Model",
    "authors": [
      "Xiaolong Wang",
      "Ziyuan Huang",
      "Kaixiang Ji",
      "Dandan Zheng"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2510.20803",
    "upvote": 9
  }
}