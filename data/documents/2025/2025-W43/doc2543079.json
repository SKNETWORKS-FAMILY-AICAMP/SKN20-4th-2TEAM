{
  "context": "KORE is a method for injecting new knowledge into large multimodal models while preserving old knowledge, using structured augmentations and covariance matrix constraints to minimize catastrophic forgetting. Large Multimodal Models encode extensive factual knowledge in their\npre-trained weights. However, its knowledge remains static and limited, unable\nto keep pace with real-world developments, which hinders continuous knowledge\nacquisition. Effective knowledge injection thus becomes critical, involving two\ngoals:knowledge adaptation(injecting new knowledge) andknowledge retention(preserving old knowledge). Existing methods often struggle to learn new\nknowledge and suffer fromcatastrophic forgetting. To address this, we proposeKORE, a synergistic method ofKnOwledge-oRientEd augmentationsand constraints\nfor injecting new knowledge into large multimodal models while preserving old\nknowledge. Unlike general text or image data augmentation,KOREautomatically\nconverts individual knowledge items into structured and comprehensive knowledge\nto ensure that the model accurately learns new knowledge, enabling accurate\nadaptation. Meanwhile,KOREstores previous knowledge in thecovariance matrixof LMM'slinear layer activationsand initializes theadapterby projecting the\noriginal weights into the matrix's null space, defining afine-tuning directionthat minimizes interference with previous knowledge, enabling powerful\nretention. Extensive experiments on variousLMMs, includingLLaVA-v1.5-7B,LLaVA-v1.5-13B, andQwen2.5-VL-7B, show thatKOREachieves superior new\nknowledge injection performance and effectively mitigates catastrophic\nforgetting. Knowledge-Oriented Control, Accurate Adaptation and Powerful Retention!  This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend ğŸ‰ We are excited to introduce KORE: a new knowledge injection framework for LMMs!\"Knowledge-Oriented Control, Accurate Adaptation and Powerful Retention! ---Evolving Knowledge Injection\" ğŸ”¥ LMMs encode vast but static and limited knowledge, making them unable to keep up with real-world developments. This not only leads to outdated responses but also prevents them from continuously acquiring new information. Therefore, efficient knowledge injection becomes crucial. ğŸ§ Core Challenge: How to enable LMMs to efficientlylearn new knowledge (knowledge adaptation), while simultaneouslyretaining their vast existing knowledge and capabilities (knowledge retention)? ğŸ¯The KORE Solution: We propose a synergistic method of \"Knowledge-Oriented Augmentation and Constraint\" (KORE)! ğŸ“šKORE-Augmentation: Automatically augments single knowledge points into structured multi-turn dialogues and instruction tasks, ensuring the model\"learns accurately\" (Accurate Adaptation). ğŸ›¡ï¸KORE-Constraint: Stores existing knowledge in a covariance matrix and performs constrained fine-tuning in its \"null-space,\" ensuring the model\"forgets less\" (Powerful Retention). ğŸš€ Significant Results: KORE demonstrates exceptional performance on mainstream LMMs such as LLaVA-v1.5 (7B/13B) and Qwen2.5-VL (7B).â—Knowledge Adaptation Evaluation: KORE's CEM (30.65) and F1-Score (41.26) on the EVOKE benchmark (multimodal evolving knowledge injection dataset and benchmark) are more than double those of LoRA (15.23, 18.31);â—Knowledge Retention Evaluation: KORE performs robustly across 12 benchmarks spanning 7 capability dimensions (e.g., MME, SEEDBench2-Plus, ScienceQA, MIA-Bench, etc.), surpassing numerous previous continual learning methods;â—Overall Performance: KORE successfully balances both aspects, with its comprehensive average score on LLaVA-v1.5 (7B \"8.41 â†‘\" / 13B \"10.74 â†‘\") and Qwen2.5-VL (7B \"3.40 â†‘\") significantly surpassing all baselines, demonstrating its SOTA performance and strong generalizability across different architectures and scales. ğŸ“‘Arxivï¼šhttps://arxiv.org/abs/2510.19316ğŸŒWebsiteï¼šhttps://kore-lmm.github.io/ğŸ“¦Githubï¼šhttps://github.com/KORE-LMM/KORE If you find our work interesting, please help us by ğŸ¤—UpvotingandStarring! ğŸ’¬ We welcome everyone to follow and discuss! Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2543079",
    "title": "KORE: Enhancing Knowledge Injection for Large Multimodal Models via\n  Knowledge-Oriented Augmentations and Constraints",
    "authors": [
      "Kailin Jiang"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/KORE-LMM/KORE",
    "huggingface_url": "https://huggingface.co/papers/2510.19316",
    "upvote": 11
  }
}