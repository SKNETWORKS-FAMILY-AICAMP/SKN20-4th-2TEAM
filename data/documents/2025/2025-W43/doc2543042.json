{
  "context": "A new benchmark, IF-VidCap, evaluates video captioning models on instruction-following capabilities, revealing that top-tier open-source models are closing the performance gap with proprietary models. AlthoughMultimodal Large Language Models(MLLMs) have demonstrated\nproficiency invideo captioning, practical applications require captions that\nfollow specific user instructions rather than generating exhaustive,\nunconstrained descriptions. Currentbenchmarks, however, primarily assess\ndescriptive comprehensiveness while largely overlookinginstruction-followingcapabilities. To address this gap, we introduce IF-VidCap, a newbenchmarkfor\nevaluating controllablevideo captioning, which contains 1,400 high-quality\nsamples. Distinct from existingvideo captioningor generalinstruction-followingbenchmarks, IF-VidCap incorporates a systematic framework\nthat assesses captions on two dimensions:format correctnessand content\ncorrectness. Our comprehensive evaluation of over 20 prominent models reveals a\nnuanced landscape: despite the continued dominance of proprietary models, the\nperformance gap is closing, with top-tier open-source solutions now achieving\nnear-parity. Furthermore, we find that models specialized fordense captioningunderperformgeneral-purpose MLLMson complex instructions, indicating that\nfuture work should simultaneously advance both descriptive richness andinstruction-followingfidelity. üéØ First Instruction-Following Video Captioning Benchmark: 1,400 complex, compositional instructions aligned with real-world downstream applicationsüîç Robust Evaluation Protocol: Multi-dimensional evaluation combining rule-based and LLM-based checksüìä Comprehensive Analysis: Evaluation of 20+ state-of-the-art models with detailed insightsüìö Training Dataset: Curated dataset for fine-grained instruction-based control This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend ¬∑Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2543042",
    "title": "IF-VidCap: Can Video Caption Models Follow Instructions?",
    "authors": [
      "Shihao Li"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/NJU-LINK/IF-VidCap",
    "huggingface_url": "https://huggingface.co/papers/2510.18726",
    "upvote": 25
  }
}