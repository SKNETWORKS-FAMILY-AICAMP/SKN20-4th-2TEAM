{
  "context": "Glyph compresses long textual inputs into images using vision-language models, achieving significant token compression and improved performance in long-context tasks. Large language models(LLMs) increasingly rely onlong-context modelingfor\ntasks such asdocument understanding, code analysis, and multi-step reasoning.\nHowever, scaling context windows to the million-token level brings prohibitive\ncomputational and memory costs, limiting the practicality of long-context LLMs.\nIn this work, we take a different perspective-visual context scaling-to tackle\nthis challenge. Instead of extendingtoken-based sequences, we proposeGlyph, a\nframework that renders long texts into images and processes them withvision-language models(VLMs). This approach substantially compresses textual\ninput while preserving semantic information, and we further design an\nLLM-drivengenetic searchto identify optimal visual rendering configurations\nfor balancing accuracy and compression. Through extensive experiments, we\ndemonstrate that our method achieves 3-4xtoken compressionwhile maintaining\naccuracy comparable to leading LLMs such as Qwen3-8B on various long-context\nbenchmarks. This compression also leads to around 4x fasterprefillinganddecoding, and approximately 2x fasterSFT training. Furthermore, under extreme\ncompression, a 128K-context VLM could scale to handle 1M-token-level text\ntasks. In addition, the rendered text data benefits real-world multimodal\ntasks, such asdocument understanding. Our code and model are released at\nhttps://github.com/thu-coai/Glyph. ü™∂ Glyph: Scaling Context Windows via Visual-Text Compression Glyph introduces a new paradigm for long-context LLMs throughvisual-text compression‚Äî rendering text as images and processing them with VLMs to boost information density. üß©Efficient compression:Achieves3‚Äì4√ó token reductionwith minimal accuracy loss on long-context benchmarks. ‚ö°Faster inference & training:Up to4√ó faster inferenceand2√ó faster SFT training. üìèExtreme compression capability:Enables128K-context models to handle 1M-token tasksthrough highly compact visual representations. üåêOpen-source soon:Code and models are on the way to foster vision-driven context scaling. Is this not exactly what the Deepseek OCR paper was about? ~3-4√ó token compression with comparable accuracy on long-context benchmarks; faster inference/training; aim to handle up to ~1 M token-level tasks via extreme compression.Hugging Face Primary domain: more general ‚Äúdocument understanding / long text‚Äù rather than strictly OCR or converting arbitrary scanned docs into text. The image representation is a strategy for long-context LLMs. similar ideas, but this implementation is more practical for inference and training and practical tasks. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend üòÅWe are honored to have invited the author to deliver an online special report on November 13 at 10:30 Beijing Time. Click here to reserve your spot for the live stream  üì∫https://event.baai.ac.cn/activities/970üìöMore awsome presentations , you can click:https://hub.baai.ac.cn/papers ¬∑Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2543014",
    "title": "Glyph: Scaling Context Windows via Visual-Text Compression",
    "authors": [],
    "publication_year": 2025,
    "github_url": "https://github.com/thu-coai/Glyph",
    "huggingface_url": "https://huggingface.co/papers/2510.17800",
    "upvote": 67
  }
}