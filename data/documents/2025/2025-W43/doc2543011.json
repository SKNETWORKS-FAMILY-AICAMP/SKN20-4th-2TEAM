{
  "context": "Ring-1T, a trillion-parameter open-source thinking model, addresses training challenges with IcePop, C3PO++, and ASystem, achieving top results across benchmarks and democratizing large-scale reasoning intelligence. We present Ring-1T, the first open-source, state-of-the-art thinking model\nwith a trillion-scale parameter. It features 1 trillion total parameters and\nactivates approximately 50 billion per token. Training such models at a\ntrillion-parameter scale introduces unprecedented challenges, including\ntrain-inference misalignment, inefficiencies inrollout processing, and\nbottlenecks in theRL system. To address these, we pioneer three interconnected\ninnovations: (1)IcePopstabilizes RL training via token-level discrepancy\nmasking andclipping, resolving instability from training-inference mismatches;\n(2)C3PO++improvesresource utilizationfor long rollouts under a token budget\nby dynamically partitioning them, thereby obtaining high time efficiency; and\n(3)ASystem, a high-performance RL framework designed to overcome the systemic\nbottlenecks that impedetrillion-parameter modeltraining. Ring-1T delivers\nbreakthrough results across critical benchmarks: 93.4 onAIME-2025, 86.72 onHMMT-2025, 2088 onCodeForces, and 55.94 onARC-AGI-v1. Notably, it attains a\nsilver medal-level result on theIMO-2025, underscoring its exceptional\nreasoning capabilities. By releasing the complete 1T parameterMoEmodel to the\ncommunity, we provide the research community with direct access to cutting-edge\nreasoning capabilities. This contribution marks a significant milestone in\ndemocratizing large-scale reasoning intelligence and establishes a new baseline\nfor open-source model performance. We present Ring-1T, the first open-source, state-of-the-art thinking model with a trillion-scale parameter. It features 1 trillion total parameters and activates approximately 50 billion per token. Training such models at a trillion-parameter scale introduces unprecedented challenges, including train-inference misalignment, inefficiencies in rollout processing, and bottlenecks in the RL system. To address these, we pioneer three interconnected innovations: (1) IcePop stabilizes RL training via token-level discrepancy masking and clipping, resolving instability from training-inference mismatches; (2) C3PO++ improves resource utilization for long rollouts under a token budget by dynamically partitioning them, thereby obtaining high time efficiency; and (3) ASystem, a high-performance RL framework designed to overcome the systemic bottlenecks that impede trillion-parameter model training. Ring-1T delivers breakthrough results across critical benchmarks: 93.4 on AIME-2025, 86.72 on HMMT-2025, 2088 on CodeForces, and 55.94 on ARC-AGI-v1. Notably, it attains a silver medal-level result on the IMO-2025, underscoring its exceptional reasoning capabilities. By releasing the complete 1T parameter MoE model to the community, we provide the research community with direct access to cutting-edge reasoning capabilities. This contribution marks a significant milestone in democratizing large-scale reasoning intelligence and establishes a new baseline for open-source model performance. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Thanks a lot, very interesting Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2543011",
    "title": "Every Step Evolves: Scaling Reinforcement Learning for Trillion-Scale\n  Thinking Model",
    "authors": [],
    "publication_year": 2025,
    "github_url": "https://github.com/inclusionAI/Ring-V2",
    "huggingface_url": "https://huggingface.co/papers/2510.18855",
    "upvote": 71
  }
}