{
  "context": "LoongRL, a data-driven reinforcement learning method, enhances long-context reasoning by transforming short multi-hop QA into high-difficulty tasks, improving accuracy and generalization in large language models. Reasoning over long contexts is essential for large language models. Whilereinforcement learning(RL) enhances short-context reasoning by inducing \"Aha\"\nmoments in chain-of-thought, the advanced thinking patterns required forlong-context reasoningremain largely unexplored, and high-difficulty RL data\nare scarce. In this paper, we introduceLoongRL, a data-driven RL method for\nadvancedlong-context reasoning. Central toLoongRLisKeyChain, a synthesis\napproach that transforms shortmulti-hop QAinto high-difficulty long-context\ntasks by inserting UUID chains that hide the true question among large\ncollections of distracting documents. Solving these tasks requires the model to\ntrace the correct chain step-by-step, identify the true question, retrieve\nrelevant facts and reason over them to answer correctly. RL training onKeyChaindata induces an emergentplan-retrieve-reason-recheckreasoning\npattern that generalizes far beyond training length. Models trained at 16K\neffectively solve 128K tasks without prohibitive full-lengthRL rolloutcosts.\nOnQwen2.5-7Band 14B,LoongRLsubstantially improves long-contextmulti-hop QAaccuracy by +23.5% and +21.1% absolute gains. The resultingLoongRL-14B reaches\na score of 74.2, rivaling much larger frontier models such aso3-mini(74.5)\nandDeepSeek-R1(74.9). It also improves long-context retrieval, passes all\n128Kneedle-in-a-haystack stress tests, and preserves short-context reasoning\ncapabilities. We introduce LoongRL to explore how models think over long contexts. LoongRL uses a data-driven RL approach that induces emergent plan–retrieve–reason–recheck patterns and matches frontier models with only 14B parameters. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Very Similar to what I am trying to look into Structured Reasoning and multi perspective comparison At a quick glance I could see an overlap between the benchmarks in your train recipe and the ones you evaluated on. Could you explain why such choice and the strategies you introduced to prevent overlap between train and evaluation curriculum? Hi Alberto. Thank you for your feedback. The three datasets — HotpotQA, 2WikiMultiHopQA and MuSiQue — each of them already comes with a standard split of training, validation and test sets in their original form. In our work we only used part of the training set for model training. We did not use their validation sets nor the test sets for training or development. When we evaluate on the benchmark, we use the evaluation data as constructed in the downstream benchmark (e.g., LongBench). Therefore, there is no overlap between our training data and the evaluation curriculum. ·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2543018",
    "title": "LoongRL:Reinforcement Learning for Advanced Reasoning over Long Contexts",
    "authors": [
      "Siyuan Wang",
      "Li Lyna Zhang"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2510.19363",
    "upvote": 61
  }
}