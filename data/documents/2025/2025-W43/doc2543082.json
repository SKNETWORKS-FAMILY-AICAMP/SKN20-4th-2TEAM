{
  "context": "DRIFT, a lightweight method, enhances multimodal large language models' reasoning ability by transferring knowledge in gradient space, outperforming naive merging and supervised fine-tuning with reduced computational cost. Multimodal large language models(MLLMs) are rapidly advancing, yet theirreasoning abilityoften lags behind that of strong text-only counterparts.\nExisting methods to bridge this gap rely onsupervised fine-tuningover\nlarge-scale multimodal reasoning data orreinforcement learning, both of which\nare resource-intensive. A promising alternative ismodel merging, which\ninterpolates parameters betweenreasoning-enhanced LLMsand multimodal\nvariants. However, our analysis shows that naive merging is not always a \"free\nlunch\": its effectiveness varies drastically across model families, with some\n(e.g., LLaVA, Idefics) benefiting while others (e.g., Qwen) suffer performance\ndegradation. To address this, we proposeDirectional Reasoning Injectionfor\nFine-Tuning (DRIFT) MLLMs, a lightweight method that transfers reasoning\nknowledge in thegradient space, without destabilizingmultimodal alignment.\nDRIFT precomputes areasoning prioras theparameter-spacedifference between\nreasoning andmultimodal variants, then uses it to bias gradients during\nmultimodal fine-tuning. This approach preserves the simplicity of standardsupervised fine-tuningpipelines while enabling efficient reasoning transfer.\nExtensive experiments on multimodal reasoning benchmarks, includingMathVistaandMathVerse, demonstrate that DRIFT consistently improves reasoning\nperformance over naive merging andsupervised fine-tuning, while matching or\nsurpassing training-heavy methods at a fraction of the cost. DRIFT transfers reasoning from DeepSeekR1 into QwenVL through gradient guidance. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Have you ever compared Skywork R1V's method with yours?Skywork R1V used CoT finetuning data to update MLP only on qwen 32B VL and then substitute the LLM with R1-distilled-qwen 32B. The results also look good.  I haven't tried fine-tuning the 32 B model yet, but it is on my agenda. Then I will compare it to the Skywork R1V with our results. Thank you for the suggestion. Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2543082",
    "title": "Directional Reasoning Injection for Fine-Tuning MLLMs",
    "authors": [
      "Chao Huang",
      "Zeliang Zhang"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/WikiChao/DRIFT",
    "huggingface_url": "https://huggingface.co/papers/2510.15050",
    "upvote": 11
  }
}