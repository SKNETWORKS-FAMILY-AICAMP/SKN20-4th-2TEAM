{
  "context": "The Ring-linear model series, including Ring-mini-linear-2.0 and Ring-flash-linear-2.0, uses a hybrid architecture combining linear and softmax attention to reduce inference costs and improve training efficiency. In this technical report, we present the Ring-linear model series,\nspecifically including Ring-mini-linear-2.0 and Ring-flash-linear-2.0.\nRing-mini-linear-2.0 comprises 16B parameters and 957M activations, while\nRing-flash-linear-2.0 contains 104B parameters and 6.1B activations. Both\nmodels adopt ahybrid architecturethat effectively integrateslinear attentionandsoftmax attention, significantly reducing I/O and computational overhead in\nlong-context inference scenarios. Compared to a 32 billion parameter dense\nmodel, this series reduces inference cost to 1/10, and compared to the original\nRing series, the cost is also reduced by over 50%. Furthermore, through\nsystematic exploration of the ratio between different attention mechanisms in\nthehybrid architecture, we have identified the currently optimal model\nstructure. Additionally, by leveraging our self-developed high-performance FP8\noperator library-linghe, overall training efficiency has been improved by 50%.\nBenefiting from the high alignment between the training and inference engine\noperators, the models can undergo long-term, stable, and highly efficient\noptimization during thereinforcement learningphase, consistently maintainingSOTA performanceacross multiple challenging complex reasoning benchmarks. In this technical report, we present the Ring-linear model series, specifically including Ring-mini-linear-2.0 and Ring-flash-linear-2.0. Ring-mini-linear-2.0 comprises 16B parameters and 957M activations, while Ring-flash-linear-2.0 contains 104B parameters and 6.1B activations. Both models adopt a hybrid architecture that effectively integrates linear attention and softmax attention, significantly reducing I/O and computational overhead in long-context inference scenarios. Compared to a 32 billion parameter dense model, this series reduces inference cost to 1/10, and compared to the original Ring series, the cost is also reduced by over 50%. Furthermore, through systematic exploration of the ratio between different attention mechanisms in the hybrid architecture, we have identified the currently optimal model structure. Additionally, by leveraging our self-developed high-performance FP8 operator library-linghe, overall training efficiency has been improved by 50%. Benefiting from the high alignment between the training and inference engine operators, the models can undergo long-term, stable, and highly efficient optimization during the reinforcement learning phase, consistently maintaining SOTA performance across multiple challenging complex reasoning benchmarks. high performance FP8 operators library linghe(ÁÅµÊ†∏):https://github.com/inclusionAI/linghe arXiv explained breakdown of this paper üëâhttps://arxivexplained.com/papers/every-attention-matters-an-efficient-hybrid-architecture-for-long-context-reasoning This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Check out the breakdown of this paper on arXiv Explained:https://arxivexplained.com/papers/every-attention-matters-an-efficient-hybrid-architecture-for-long-context-reasoning ¬∑Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2543003",
    "title": "Every Attention Matters: An Efficient Hybrid Architecture for\n  Long-Context Reasoning",
    "authors": [
      "Jingyu Hu",
      "Longfei Li",
      "Peijie Jiang",
      "Qian Zhao",
      "Qingyuan Yang",
      "Wenbo Shen",
      "Yankun Ren"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/inclusionAI/linghe",
    "huggingface_url": "https://huggingface.co/papers/2510.19338",
    "upvote": 114
  }
}