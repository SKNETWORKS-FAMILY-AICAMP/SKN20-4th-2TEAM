{
  "context": "Extracting alignment training data from post-trained models using embedding models reveals significant semantic similarities and potential risks in distillation practices. In this work, we show that it is possible to extract significant amounts of\nalignment training data from a post-trained model -- useful to steer the model\nto improve certain capabilities such aslong-context reasoning,safety,instruction following, andmaths. While the majority of related work on\nmemorisation has focused on measuring success of training data extraction\nthroughstring matching, we argue thatembedding modelsare better suited for\nour specific goals. Distances measured through a high quality embedding model\ncan identifysemantic similaritiesbetween strings that a different metric such\nasedit distancewill struggle to capture. In fact, in our investigation,\napproximatestring matchingwould have severely undercounted (by a conservative\nestimate of 10times) the amount of data that can be extracted due to trivial\nartifacts that deflate the metric. Interestingly, we find that models readily\nregurgitate training data that was used in post-training phases such asSFTorRL. We show that this data can be then used to train a base model, recovering a\nmeaningful amount of the original performance. We believe our work exposes a\npossibly overlooked risk towards extracting alignment data. Finally, our work\nopens up an interesting discussion on the downstream effects ofdistillationpractices: since models seem to be regurgitating aspects of their training set,distillationcan therefore be thought of as indirectly training on the model's\noriginal dataset. Turns out the usual metrics undercounted the effects of memorisation. We show that when you measure semantic memorisation for post training data e.g. RL you get significantly high memorisation rates. Maybe this is why di-steal-ation works? This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend \" (2) our generated dataset. For the synthetic dataset, we collect a synthetic dataset of a similar size of≈930k samples. We perform basic filtering and processing using Gemini 2.5. \"    May I ask: How are the processing steps carried out here? Will this change the original samples in any way? well, close model maker make a research to discredit open model maker try to convince open model is susceptible to the attack If your model is truly fully open (like the OLMo 2 models they used to validate their findings), then it's not a really issue.I think it should be the norm to have a way of auditing models' training data. That's why models such as Apertus, OLMo 2, SmolLM ( +others) are amazing. ·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2543089",
    "title": "Extracting alignment data in open models",
    "authors": [
      "Ilia Shumailov"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2510.18554",
    "upvote": 9
  }
}