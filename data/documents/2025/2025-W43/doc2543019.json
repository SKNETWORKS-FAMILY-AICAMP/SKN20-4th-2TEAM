{
  "context": "AdaSPEC enhances speculative decoding by selectively filtering tokens during knowledge distillation, improving token acceptance rates without sacrificing generation quality. Speculative Decoding(SD) accelerates large language model inference by\nemploying a small draft model to generate predictions, which are then verified\nby a larger target model. The effectiveness of SD hinges on the alignment\nbetween these models, which is typically enhanced byKnowledge Distillation(KD). However, conventional KD methods aim to minimize theKL divergencebetween the draft and target models across all tokens, a goal that is\nmisaligned with the true objective of SD, which is to maximize token acceptance\nrate. Therefore, draft models often struggle to fully assimilate the target\nmodel's knowledge due to capacity constraints, leading to suboptimal\nperformance. To address this challenge, we propose AdaSPEC, a novel method that\nincorporatesselective token filteringinto the KD process. AdaSPEC utilizes areference modelto identify and filter out difficult-to-fit tokens, enabling\nthe distillation of a draft model that better aligns with the target model on\nsimpler tokens. This approach improves the overalltoken acceptance ratewithout compromising generation quality. We evaluate AdaSPEC across diverse\ntasks, includingarithmetic reasoning,instruction-following,coding, andsummarization, using model configurations of 31M/1.4B and 350M/2.7B parameters.\nOur results demonstrate that AdaSPEC consistently outperforms the\nstate-of-the-art DistillSpec method, achieving higher acceptance rates across\nall tasks (up to 15\\%). The code is publicly available at\nhttps://github.com/yuezhouhu/adaspec. AdaSPEC introduces atwo-stage selective knowledge distillationframework to train draft models that better align with the target model in Speculative Decoding. Reference Model as a Difficulty Analyzer:A reference model (initialized identically to the draft model) is first distilled from the target model using standard knowledge distillation (e.g., forward KL divergence). This reference model serves not as the final draft, but as aproxy to estimate token-wise learning difficulty. Selective Token Filtering:During distillation of the actual draft model, AdaSPEC computes the KL divergence loss for each token from both the draft and reference models against the target. It then calculates theloss gapΔL = L_draft − L_ref. Tokens with alargerΔL are consideredeasier to learn, because higher ΔL indicates larger potential to optimize on those tokens. AdaSPECselects the top-k% of these \"easy\" tokensand trains the draft modelonlyon this filtered subset. By focusing the draft model’s limited capacity on tokens it can reliably learn, AdaSPEC achieveshigher alignmentwith the target model, leading toconsistently improved acceptance ratesacross diverse tasks—without sacrificing generation quality. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend ·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2543019",
    "title": "AdaSPEC: Selective Knowledge Distillation for Efficient Speculative\n  Decoders",
    "authors": [
      "Yuezhou Hu",
      "Jiaxin Guo"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/yuezhouhu/adaspec",
    "huggingface_url": "https://huggingface.co/papers/2510.19779",
    "upvote": 60
  }
}