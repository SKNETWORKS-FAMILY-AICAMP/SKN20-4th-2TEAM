{
  "context": "Reinforcement Learning enhances vision encoders in Multimodal Language Models, leading to better visual representations and performance compared to Supervised Fine-tuning. A dominant assumption inMultimodal Language Model(MLLM) research is that\nits performance is largely inherited from theLLM backbone, given its immense\nparameter scale and remarkable capabilities. This has created a void in the\nunderstanding of thevision encoder, which determines how MLLMs perceive\nimages. The recent shift in MLLM training paradigms, fromSupervised Finetuning(SFT) toReinforcement Learning(RL), magnifies this oversight-namely, the\nsignificant lack of analysis on how such training reshapes thevision encoderas well as the MLLM. To address this, we first investigate the impact of\ntraining strategies on MLLMs, where RL shows a clear advantage over SFT in\nstrongly vision-relatedVQA benchmarks. Motivated by this, we conduct a\ncritical yet under-explored analysis of thevision encoderof MLLMs through\ndiverse and in-depth experiments, ranging fromImageNet classificationandsegmentationtogradient visualization. Our results demonstrate that MLLM's\npost-training strategy (i.e., SFT or RL) not only leads to distinct outcomes on\nMLLM downstream tasks, but also fundamentally reshapes MLLM's underlying visual\nrepresentations. Specifically, the key finding of our study is that RL produces\nstronger and precisely localizedvisual representationscompared to SFT,\nboosting the ability of thevision encoderfor MLLM. We then reframe our\nfindings into a simple recipe for building strongvision encoders for MLLMs,Preference-Instructed Vision OpTimization(PIVOT). When integrated into MLLMs,\naPIVOT-trainedvision encoderoutperforms even larger and more heavily-trained\ncounterparts, despite requiring less than 1% of the computational cost of\nstandard vision pretraining. This result opens an effective and efficient path\nfor advancing the vision backbones of MLLMs. Project page available at\nhttps://june-page.github.io/pivot/ A dominant assumption in Multimodal Language Model (MLLM) research is that its performance is largely inherited from the LLM backbone, given its immense parameter scale and remarkable capabilities. This has created a void in the understanding of the vision encoder, which determines how MLLMs perceive images. The recent shift in MLLM training paradigms, from Supervised Finetuning (SFT) to Reinforcement Learning (RL), magnifies this oversight-namely, the significant lack of analysis on how such training reshapes the vision encoder as well as the MLLM. To address this, we first investigate the impact of training strategies on MLLMs, where RL shows a clear advantage over SFT in strongly vision-related VQA benchmarks. Motivated by this, we conduct a critical yet under-explored analysis of the vision encoder of MLLMs through diverse and in-depth experiments, ranging from ImageNet classification and segmentation to gradient visualization. Our results demonstrate that MLLM's post-training strategy (i.e., SFT or RL) not only leads to distinct outcomes on MLLM downstream tasks, but also fundamentally reshapes MLLM's underlying visual representations. Specifically, the key finding of our study is that RL produces stronger and precisely localized visual representations compared to SFT, boosting the ability of the vision encoder for MLLM. We then reframe our findings into a simple recipe for building strong vision encoders for MLLMs, Preference-Instructed Vision OpTimization (PIVOT). When integrated into MLLMs, a PIVOT-trained vision encoder outperforms even larger and more heavily-trained counterparts, despite requiring less than 1% of the computational cost of standard vision pretraining. This result opens an effective and efficient path for advancing the vision backbones of MLLMs. Project page available at this https URL This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2543025",
    "title": "RL makes MLLMs see better than SFT",
    "authors": [
      "Junha Song",
      "Byeongho Heo"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/june-page/pivot",
    "huggingface_url": "https://huggingface.co/papers/2510.16333",
    "upvote": 48
  }
}