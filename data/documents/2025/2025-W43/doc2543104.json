{
  "context": "Chart2Code is a hierarchical benchmark for evaluating the chart understanding and code generation capabilities of large multimodal models, featuring three levels of increasing complexity and diverse real-world scenarios. We introduceChart2Code, a new benchmark for evaluating the chart\nunderstanding and code generation capabilities of largemultimodal models(LMMs).Chart2Codeis explicitly designed from a user-driven perspective,\ncapturing diverse real-world scenarios and progressively increasing task\ndifficulty. It consists of three levels: Level 1 (Chart Reproduction)\nreproduces charts from a reference figure and user query; Level 2 (Chart\nEditing) involves complex modifications such as changing chart types or adding\nelements; and Level 3 (Long-Table to Chart Generation) requires models to\ntransform long, information-dense tables into faithful charts following user\ninstructions. To our knowledge, this is the firsthierarchical benchmarkthat\nreflects practicalchart2codeusage while systematically scaling task\ncomplexity. In total,Chart2Codecontains 2,023 tasks across 22 chart types,\npaired with multi-level evaluation metrics that assess bothcode correctnessand thevisual fidelityof rendered charts. We benchmark 25 state-of-the-art\n(SoTA)LMMs, including both proprietary and the latest open-source models such\nasGPT-5,Qwen2.5-VL,InternVL3/3.5,MiMo-VL, andSeed-1.6-VL. Experimental\nresults demonstrate that even the SoTA modelGPT-5averages only 0.57 on\ncode-based evaluation and 0.22 on chart-quality assessment across the editing\ntasks, underscoring the difficulty ofChart2Code. We anticipate this benchmark\nwill drive advances in multimodal reasoning and foster the development of more\nrobust and general-purposeLMMs. Our code and data are available onChart2Code. Chart2Code is a new benchmark for evaluating the chart understanding and code generation capabilities of large multimodal models (LMMs). Project Page:https://csu-jpg.github.io/Chart2Code.github.io/Code:https://github.com/CSU-JPG/Chart2CodeData:https://huggingface.co/datasets/CSU-JPG/Chart2Code This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2543104",
    "title": "From Charts to Code: A Hierarchical Benchmark for Multimodal Models",
    "authors": [
      "Dongxing Mao"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/CSU-JPG/Chart2Code",
    "huggingface_url": "https://huggingface.co/papers/2510.17932",
    "upvote": 7
  }
}