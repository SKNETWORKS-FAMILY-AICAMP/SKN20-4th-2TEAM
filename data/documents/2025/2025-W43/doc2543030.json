{
  "context": "Mixture-of-Groups Attention (MoGA) enables efficient long video generation by addressing the quadratic scaling issue of full attention in Diffusion Transformers. Long video generationwithDiffusion Transformers(DiTs) is bottlenecked by\nthe quadratic scaling offull attentionwithsequence length. Since attention\nis highly redundant, outputs are dominated by a small subset of query-key\npairs. Existing sparse methods rely on blockwise coarse estimation, whose\naccuracy-efficiency trade-offs are constrained by block size. This paper\nintroduces Mixture-of-Groups Attention (MoGA), an efficientsparse attentionthat uses a lightweight, learnabletoken routerto precisely match tokens\nwithout blockwise estimation. Throughsemantic-aware routing, MoGA enables\neffective long-range interactions. As akernel-free method, MoGA integrates\nseamlessly with modern attention stacks, includingFlashAttentionand sequence\nparallelism. Building on MoGA, we develop an efficientlong video generationmodel that end-to-end producesminute-level,multi-shot,480pvideos at24 fps,\nwith acontext lengthof approximately 580k. Comprehensive experiments on\nvarious video generation tasks validate the effectiveness of our approach. Long video generation with Diffusion Transformers (DiTs) is bottlenecked by the quadratic scaling of full attention with sequence length. Since attention is highly redundant, outputs are dominated by a small subset of query-key pairs. Existing sparse methods rely on blockwise coarse estimation, whose accuracy-efficiency trade-offs are constrained by block size. This paper introduces Mixture-of-Groups Attention (MoGA), an efficient sparse attention that uses a lightweight, learnable token router to precisely match tokens without blockwise estimation. Through semantic-aware routing, MoGA enables effective long-range interactions. As a kernel-free method, MoGA integrates seamlessly with modern attention stacks, including FlashAttention and sequence parallelism. Building on MoGA, we develop an efficient long video generation model that end-to-end produces minute-level, multi-shot, 480p videos at 24 fps, with a context length of approximately 580k. Comprehensive experiments on various video generation tasks validate the effectiveness of our approach. Thank you for the report. The official git repository of MoGA is available here:https://github.com/bytedance-fanqie-ai/MoGA It is still not available the github repo? Just the README file and 3 ong files. Any plans to do update it today? Appreciate. It is still not available the github repo? Just the README file and 3 png files. Any plans to do update it today? Appreciate. Thanks for your attention. The code is still undergoing the company's review process and may take some time before being updated to the repository. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2543030",
    "title": "MoGA: Mixture-of-Groups Attention for End-to-End Long Video Generation",
    "authors": [
      "Yuning Lu",
      "Nan Chen"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/bytedance-fanqie-ai/MoGA",
    "huggingface_url": "https://huggingface.co/papers/2510.18692",
    "upvote": 40
  }
}