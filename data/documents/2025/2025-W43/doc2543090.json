{
  "context": "A training framework for large-scale video generation models optimizes data processing, model architecture, training strategy, and infrastructure, resulting in a model that matches state-of-the-art performance and is open-sourced with Megatron-Core-based training code. In recent years, large-scalegenerative modelsfor visual content\n(e.g., images, videos, and 3D objects/scenes) have made remarkable\nprogress. However, training large-scalevideo generationmodels remains\nparticularly challenging and resource-intensive due to cross-modal text-video\nalignment, the long sequences involved, and the complex spatiotemporal\ndependencies. To address these challenges, we present a training framework that\noptimizes four pillars: (i)data processing, (ii)model architecture, (iii)training strategy, and (iv)infrastructurefor large-scalevideo generationmodels. These optimizations delivered significant efficiency gains and\nperformance improvements across all stages of data preprocessing, video\ncompression,parameter scaling,curriculum-based pretraining, andalignment-focused post-training. Our resulting model, MUG-V 10B, matches recent\nstate-of-the-art video generators overall and, on e-commerce-oriented video\ngeneration tasks, surpasses leading open-source baselines in human evaluations.\nMore importantly, we open-source the complete stack, including model weights,Megatron-Core-based large-scale training code, and inference pipelines forvideo generationand enhancement. To our knowledge, this is the first public\nrelease of large-scalevideo generationtraining code that exploitsMegatron-Coreto achieve high training efficiency and near-linear multi-node\nscaling, details are available in\nhttps://github.com/Shopee-MUG/MUG-V{our webpage}. In recent years, large-scale generative models for visual content (e.g., images, videos, and 3D objects/scenes) have made remarkable progress. However, training large-scale video generation models remains particularly challenging and resource-intensive due to cross-modal text-video alignment, the long sequences involved, and the complex spatiotemporal dependencies. To address these challenges, we present a training framework that optimizes four pillars: (i) data processing, (ii) model architecture, (iii) training strategy, and (iv) infrastructure for large-scale video generation models. These optimizations delivered significant efficiency gains and performance improvements across all stages of data preprocessing, video compression, parameter scaling, curriculum-based pretraining, and alignment-focused post-training. Our resulting model, MUG-V 10B, matches recent state-of-the-art video generators overall and, on e-commerce-oriented video generation tasks, surpasses leading open-source baselines in human evaluations. More importantly, we open-source the complete stack, including model weights, Megatron-Core-based large-scale training code, and inference pipelines for video generation and enhancement. To our knowledge, this is the first public release of large-scale video generation training code that exploits Megatron-Core to achieve high training efficiency and near-linear multi-node scaling, details are available inhttps://github.com/Shopee-MUG/MUG-V. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2543090",
    "title": "MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation\n  Models",
    "authors": [],
    "publication_year": 2025,
    "github_url": "https://github.com/Shopee-MUG/MUG-V",
    "huggingface_url": "https://huggingface.co/papers/2510.17519",
    "upvote": 9
  }
}