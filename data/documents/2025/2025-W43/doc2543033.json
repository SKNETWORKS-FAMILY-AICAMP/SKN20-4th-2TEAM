{
  "context": "QueST, a framework combining difficulty-aware graph sampling and fine-tuning, generates large-scale synthetic coding problems to enhance the performance of large language models in competitive coding and reasoning tasks. Large Language Models have achieved strong performance on reasoning tasks,\nsolving competition-level coding and math problems. However, their scalability\nis limited by human-labeled datasets and the lack of large-scale, challenging\ncoding problem training data. Existing competitive coding datasets contain only\nthousands to tens of thousands of problems. Previoussynthetic data generationmethods rely on either augmenting existing instruction datasets or selecting\nchallenging problems from human-labeled data. In this paper, we proposeQueST,\na novel framework which combinesdifficulty-aware graph samplinganddifficulty-aware rejection fine-tuningthat directly optimizes specialized\ngenerators to create challenging coding problems. Our trained generators\ndemonstrate superior capability compared to even GPT-4o at creating challenging\nproblems that benefit downstream performance. We leverageQueSTto generate\nlarge-scale synthetic coding problems, which we then use to distill from strong\nteacher models with longchain-of-thoughtor to conductreinforcement learningfor smaller models, proving effective in both scenarios. Ourdistillationexperiments demonstrate significant performance gains. Specifically, after\nfine-tuning Qwen3-8B-base on 100K difficult problems generated byQueST, we\nsurpass the performance of the original Qwen3-8B onLiveCodeBench. With an\nadditional 112K examples (i.e., 28K human-written problems paired with multiple\nsynthetic solutions), our 8B model matches the performance of the much largerDeepSeek-R1-671B. These findings indicate that generating complex problems viaQueSToffers an effective and scalable approach to advancing the frontiers of\ncompetitive coding and reasoning for large language models. Synthetic code problems help for RLVR and chain-of-thought distillation – and optimizing their quality can make a big difference! This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Thanks for this interesting paper. Though I now have a question. Here the difficulty of a question is dependent on the output consistency by GPT-4o for a coding problem. Isnt this easily impacted by the choice of the model? Also why is the output consistency based difficulty estimation preferred? Since we're working with this form of inconsistency, can't we say that the focus is not on difficulty as the sole contributor but is an unintended mix of novelty, difficulty and ambiguity/under-specified problems? Since the model can be uncertain in any of these conditions? Would love to have more discussion on this. ·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2543033",
    "title": "QueST: Incentivizing LLMs to Generate Difficult Problems",
    "authors": [
      "Hanxu Hu",
      "Xingxing Zhang",
      "Jannis Vamvas"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2510.17715",
    "upvote": 33
  }
}