{
  "context": "DLER, a reinforcement learning training recipe, improves the accuracy-efficiency trade-off in reasoning language models by addressing challenges in advantage estimation, entropy collapse, and sparse reward signals, leading to shorter outputs and better test-time scaling. Reasoning language models such as OpenAI-o1, DeepSeek-R1, and Qwen achieve\nstrong performance via extended chains of thought but often generate\nunnecessarily long outputs. Maximizing intelligence per token--accuracy\nrelative to response length--remains an open problem. We revisit reinforcement\nlearning (RL) with the simplest length penalty--truncation--and show that\naccuracy degradation arises not from the lack of sophisticated penalties but\nfrom inadequate RL optimization. We identify three key challenges: (i) large\nbias inadvantage estimation, (ii)entropy collapse, and (iii) sparse reward\nsignal. We address them withDoing Length pEnalty Right(DLER), a training\nrecipe combiningbatch-wise reward normalization, higher clipping, dynamic\nsampling, and a simple truncation length penalty. DLER achieves\nstate-of-the-art accuracy--efficiency trade-offs, cutting output length by over\n70 percent while surpassing all previous baseline accuracy. It also improves\ntest-time scaling: compared to DeepSeek-R1-7B, DLER-7B generates multiple\nconcise responses in parallel with 28 percent higher accuracy and lower\nlatency. We further introduceDifficulty-Aware DLER, which adaptively tightens\ntruncation on easier questions for additional efficiency gains. We also propose\nanupdate-selective mergingmethod that preserves baseline accuracy while\nretaining the concise reasoning ability of the DLER model, which is useful for\nscenarios where RL training data is scarce. Reasoning language models such as OpenAI-o1, DeepSeek-R1, and Qwen achieve strong performance via extended chains of thought but often generate unnecessarily long outputs. Maximizing intelligence per token--accuracy relative to response length--remains an open problem. We revisit reinforcement learning (RL) with the simplest length penalty--truncation--and show that accuracy degradation arises not from the lack of sophisticated penalties but from inadequate RL optimization. We identify three key challenges: (i) large bias in advantage estimation, (ii) entropy collapse, and (iii) sparse reward signal. We address them with Doing Length pEnalty Right (DLER), a training recipe combining batch-wise reward normalization, higher clipping, dynamic sampling, and a simple truncation length penalty. DLER achieves state-of-the-art accuracy--efficiency trade-offs, cutting output length by over 70 percent while surpassing all previous baseline accuracy. It also improves test-time scaling: compared to DeepSeek-R1-7B, DLER-7B generates multiple concise responses in parallel with 28 percent higher accuracy and lower latency. We further introduce Difficulty-Aware DLER, which adaptively tightens truncation on easier questions for additional efficiency gains. We also propose an update-selective merging method that preserves baseline accuracy while retaining the concise reasoning ability of the DLER model, which is useful for scenarios where RL training data is scarce. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Models are released! Check them out! Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2543068",
    "title": "DLER: Doing Length pEnalty Right - Incentivizing More Intelligence per\n  Token via Reinforcement Learning",
    "authors": [
      "Shih-Yang Liu",
      "Min-Hung Chen",
      "Hongxu Yin"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2510.15110",
    "upvote": 15
  }
}