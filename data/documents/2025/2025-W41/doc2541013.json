{
  "context": "MemMamba, a novel architecture integrating state summarization and cross-attention, improves long-range memory and efficiency in sequence modeling compared to Mamba and Transformers. With the explosive growth of data, long-sequence modeling has become\nincreasingly important in tasks such as natural language processing and\nbioinformatics. However, existing methods face inherent trade-offs between\nefficiency and memory.Recurrent neural networkssuffer fromgradient vanishingand explosion, making them hard to scale.Transformerscan model global\ndependencies but are constrained byquadratic complexity. Recently, selective\nstate-space models such asMambahave demonstrated high efficiency with O(n)\ntime and O(1) recurrent inference, yet their long-rangememory decays\nexponentially. In this work, we conduct mathematical derivations andinformation-theoretic analysisto systematically uncover thememory decaymechanism ofMamba, answering a fundamental question: what is the nature ofMamba's long-range memory and how does it retain information? To quantify key\ninformation loss, we further introducehorizontal-vertical memory fidelitymetrics that capture degradation both within and across layers. Inspired by how\nhumans distill and retain salient information when reading long documents, we\npropose MemMamba, a novel architectural framework that integrates state\nsummarization mechanism together with cross-layer andcross-token attention,\nwhich alleviates long-range forgetting while preserving linear complexity.\nMemMambaachieves significant improvements over existingMambavariants andTransformerson long-sequence benchmarks such asPG19andPasskey Retrieval,\nwhile delivering a 48% speedup in inference efficiency. Both theoretical\nanalysis and empirical results demonstrate that MemMambaachieves a\nbreakthrough in the complexity-memory trade-off, offering a new paradigm forultra-long sequence modeling. MemMamba: Rethinking Memory Patterns in State Space Model This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Baseline Evaluation Methodology Table 1 reports the results of the Megalodon model, which hasno publicly released checkpoints. Could the authors please confirm if the baselines were trained from scratch and, if so, provide the training details (e.g., did you useXuezheMax/megalodonunmodified, what were hyperparameters and convergence curves like)?  The reported Megalodon perplexity (PPL) score of ~64-66 appears quite.. poor in the base case, and inconsistent with its published claims of strong long-context performance. On top of the high numbers the score slightly.. improves with seq length? Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2541013",
    "title": "MemMamba: Rethinking Memory Patterns in State Space Model",
    "authors": [],
    "publication_year": 2025,
    "github_url": "https://github.com/XuezheMax/megalodon",
    "huggingface_url": "https://huggingface.co/papers/2510.03279",
    "upvote": 72
  }
}