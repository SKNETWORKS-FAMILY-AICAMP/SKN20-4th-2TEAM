{
  "context": "BlockRank optimizes in-context ranking by enforcing inter-document block sparsity and enhancing query-document relevance, improving efficiency and scalability in large-scale information retrieval. In-context Ranking(ICR) is an emerging paradigm for Information Retrieval\n(IR), which leverages contextual understanding ofLLMsby directly\nincorporating the task description, candidate documents, and the query into the\nmodel's input prompt and tasking the LLM to identify relevant document(s).\nWhile it is effective, efficiency is a significant challenge in this paradigm,\nespecially as the candidate list grows due to quadratic/super-linear scaling ofattention operationwith context length. To this end, this paper first\nidentifies inherent and exploitable structures in the attention ofLLMsfinetuned forICR: (1)inter-document block sparsity: attention is dense within\neach document block but sparse across different documents in the context; and\n(2)query-document block relevance: the attention scores from certain query\ntokens to a document block in middle layers strongly correlate with that\ndocument's actual relevance. Motivated by these observations, we introduceBlockRank(BlockwiseIn-context Ranking), a novel method that adapts theattention operationin an LLM by (a) architecturally enforcing the observedinter-document block sparsity, reducing attention complexity from quadratic to\nlinear without loss in performance, and (b) optimizing query-document block\nrelevance for true relevant documents during fine-tuning using an auxiliarycontrastive training objective, improving retrieval in attention. Experiments\nonBEIR,MSMarcoandNQwithMistral-7Bdemonstrate thatFLARE Mistralmatches\nor outperforms existing SOTAlistwise rankersand controlled fine-tuned\nbaseline while being significantly more efficient at inference (4.7x for 100MSMarcodocuments in context) and scaling gracefully to long-context\nshortlists, around 500 documents in-context (approximately 100K context length)\nwithin a second, presenting a scalable and effective solution forICR. We present, ‚ÄúScalable In-context Ranking with Generative Models‚Äù ‚Äî step toward retrieval-native LLMs ‚Äî models that understand & optimize retrieval internally, rather than as an external prompt-level task. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Hi! First off, thank you for your excellent work‚Äîit‚Äôs been really helpful for our research. Could you please let us know if there‚Äôs a timeline for releasing the code and model weights? We‚Äôd greatly appreciate an update whenever you have a chance! Hey@TankNee! Thanks for reaching out, right now I am working on converting our internal code to an openly supported codebase - expecting this will take a week or so ü§û. Will post an update here once this is done. Thanks again for interest in our work! Is this work in any way related to or inspired by the original BlockRank from 2003?https://nlp.stanford.edu/pubs/blockrank.pdf It's not but now that you mention there are parallels in the fundamental insight - gain efficiency using block structure in the connection patterns (LLM attention in our case and hyperlinks in the 2003 BlockRank paper). Makes me wonder if pagerank has applications in attention architecture? Update: we released the full blockrank training/eval code (https://github.com/nilesh2797/BlockRank) and mistral-7b model fine-tuned on 10% of msmarco data (quicktensor/blockrank-msmarco-mistral-7b) (in the paper we used 100% of msmarco but even this checkpoint should give you close to the reported numbers). ¬∑Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2541101",
    "title": "Scalable In-context Ranking with Generative Models",
    "authors": [
      "Nilesh Gupta"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/nilesh2797/BlockRank",
    "huggingface_url": "https://huggingface.co/papers/2510.05396",
    "upvote": 9
  }
}