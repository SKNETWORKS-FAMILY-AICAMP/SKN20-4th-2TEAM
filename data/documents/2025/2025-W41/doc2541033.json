{
  "context": "SHANKS, a general inference framework, enables spoken language models to generate unspoken reasoning while listening to user input, enhancing real-time interaction and task completion. Current large language models (LLMs) andspoken language models(SLMs) begin\nthinking and taking actions only after the user has finished their turn. This\nprevents the model from interacting during the user's turn and can lead to high\nresponse latency while it waits to think. Consequently, thinking after\nreceiving the full input is not suitable for speech-to-speech interaction,\nwhere real-time, low-latency exchange is important. We address this by noting\nthat humans naturally \"think while listening.\" In this paper, we proposeSHANKS, a general inference framework that enables SLMs to generate unspoken\nchain-of-thought reasoning while listening to the user input.SHANKSstreams\nthe input speech in fixed-duration chunks and, as soon as a chunk is received,\ngenerates unspoken reasoning based on all previous speech and reasoning, while\nthe user continues speaking.SHANKSuses this unspoken reasoning to decide\nwhether to interrupt the user and to maketool callsto complete the task. We\ndemonstrate thatSHANKSenhances real-time user-SLM interaction in two\nscenarios: (1) when the user is presenting a step-by-step solution to a math\nproblem,SHANKScan listen, reason, and interrupt when the user makes a\nmistake, achieving 37.1% higherinterruption accuracythan a baseline that\ninterrupts without thinking; and (2) in a tool-augmented dialogue,SHANKScan\ncomplete 56.9% of thetool callsbefore the user finishes their turn. Overall,SHANKSmoves toward models that keep thinking throughout the conversation, not\nonly after a turn ends. Animated illustrations ofShankscan be found at\nhttps://d223302.github.io/SHANKS/ SHANKS is a method to allow spoken language models (SLMs) to think while listening to the user input. Unlike LLMs that only start to think after the full input is received, our method allows the SLM to begin to reason about the user input as the user is speaking, enabling timely and well-founded interaction and reducing the response latency. Check out a brief introduction at the project page:  This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2541033",
    "title": "SHANKS: Simultaneous Hearing and Thinking for Spoken Language Models",
    "authors": [],
    "publication_year": 2025,
    "github_url": "https://github.com/d223302/SHANKS",
    "huggingface_url": "https://huggingface.co/papers/2510.06917",
    "upvote": 34
  }
}