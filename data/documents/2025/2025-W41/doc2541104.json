{
  "context": "A reinforcement mid-training framework (RMT) improves large language models by addressing inefficiencies and underutilization of token information, leading to significant performance gains. The development of state-of-the-art large language models is commonly\nunderstood as a two-stage process involving pre-training and post-training. We\npoint out the need for an additional intermediate stage called reinforcement\nmid-training with potential for strong performance gains. In this paper, we\nformally define the problem and identify three key challenges: (1) inefficient\ntraining due to excessive reasoning steps, (2) disregard of the imbalanced\ntoken entropy distribution, and (3) underutilization of token information. To\naddress these challenges, we propose RMT, a framework for efficient, adaptive,\nand unifiedreinforcement mid-trainingwith various innovative components. In\nparticular, we first introduce adynamic token budget mechanismthat constrains\nunnecessary reasoning steps and mitigates model overthinking. Next, we design acurriculum-based adaptive samplingmethod that fosters a progressive learning\ntrajectory from easy to hard tokens. Finally, we present a dual training\nstrategy that combinesreinforcement learningwithnext-token prediction,\nensuring targeted learning on key tokens and full exploitation of all token\ninformation. Extensive experiments demonstrate the superiority of RMT over\nstate-of-the-art methods, achieving up to +64.91% performance improvement with\nonly 21% of the reasoning length inlanguage modeling. We also show that\ncheckpoints obtained afterreinforcement mid-trainingcan benefit the\nsubsequent post-training, yielding up to +18.76% improvement in themathematical domain. Introducing Reinforcement Mid-Training, an additional intermediate stage with potential for strong performance gains for post-training. Code:https://github.com/Mid-Training/RMT This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2541104",
    "title": "Reinforcement Mid-Training",
    "authors": [
      "Shaoyu Chen"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/Mid-Training/RMT",
    "huggingface_url": "https://huggingface.co/papers/2509.24375",
    "upvote": 9
  }
}