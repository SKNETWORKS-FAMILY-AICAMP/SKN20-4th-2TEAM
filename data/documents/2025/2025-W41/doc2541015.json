{
  "context": "VideoCanvas addresses temporal ambiguity in latent video diffusion models to enable flexible spatio-temporal video completion using a hybrid conditioning strategy. We introduce the task ofarbitrary spatio-temporal video completion, where a\nvideo is generated from arbitrary, user-specified patches placed at any spatial\nlocation and timestamp, akin to painting on a video canvas. This flexible\nformulation naturally unifies many existing controllable video generation\ntasks--including first-frame image-to-video, inpainting, extension, and\ninterpolation--under a single, cohesive paradigm. Realizing this vision,\nhowever, faces a fundamental obstacle in modernlatent video diffusion models:\nthe temporal ambiguity introduced bycausal VAEs, where multiple pixel frames\nare compressed into a single latent representation, making precise frame-level\nconditioning structurally difficult. We address this challenge withVideoCanvas, a novel framework that adapts theIn-Context Conditioning (ICC)paradigm to this fine-grained control task with zero new parameters. We propose\na hybrid conditioning strategy that decouples spatial and temporal control:\nspatial placement is handled viazero-padding, while temporal alignment is\nachieved throughTemporal RoPE Interpolation, which assigns each condition a\ncontinuous fractional position within the latent sequence. This resolves the\nVAE's temporal ambiguity and enables pixel-frame-aware control on a frozen\nbackbone. To evaluate this new capability, we developVideoCanvasBench, the\nfirst benchmark forarbitrary spatio-temporal video completion, covering bothintra-scene fidelityandinter-scene creativity. Experiments demonstrate thatVideoCanvassignificantly outperforms existing conditioning paradigms,\nestablishing a new state of the art in flexible and unified video generation. We introduce the task of arbitrary spatio-temporal video completion, where a video is generated from arbitrary, user-specified patches placed at any spatial location and timestamp, akin to painting on a video canvas. This flexible formulation naturally unifies many existing controllable video generation tasks--including first-frame image-to-video, inpainting, extension, and interpolation--under a single, cohesive paradigm. Realizing this vision, however, faces a fundamental obstacle in modern latent video diffusion models: the temporal ambiguity introduced by causal VAEs, where multiple pixel frames are compressed into a single latent representation, making precise frame-level conditioning structurally difficult. We address this challenge with VideoCanvas, a novel framework that adapts the In-Context Conditioning (ICC) paradigm to this fine-grained control task with zero new parameters. We propose a hybrid conditioning strategy that decouples spatial and temporal control: spatial placement is handled via zero-padding, while temporal alignment is achieved through Temporal RoPE Interpolation, which assigns each condition a continuous fractional position within the latent sequence. This resolves the VAE's temporal ambiguity and enables pixel-frame-aware control on a frozen backbone. To evaluate this new capability, we develop VideoCanvasBench, the first benchmark for arbitrary spatio-temporal video completion, covering both intra-scene fidelity and inter-scene creativity. Experiments demonstrate that VideoCanvas significantly outperforms existing conditioning paradigms, establishing a new state of the art in flexible and unified video generation. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2541015",
    "title": "VideoCanvas: Unified Video Completion from Arbitrary Spatiotemporal\n  Patches via In-Context Conditioning",
    "authors": [
      "Minghong Cai"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/KwaiVGI/VideoCanvas",
    "huggingface_url": "https://huggingface.co/papers/2510.08555",
    "upvote": 63
  }
}