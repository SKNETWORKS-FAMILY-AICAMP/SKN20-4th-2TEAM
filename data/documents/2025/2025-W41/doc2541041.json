{
  "context": "Joint optimal scaling of model and dataset sizes in deep learning is governed by the operator norm of the output layer, a phenomenon termed norm transfer, which provides a necessary condition for optimal learning rate and batch size. Despite recent progress in optimal hyperparameter transfer under model and\ndataset scaling, no unifying explanatory principle has been established. Using\ntheScion optimizer, we discover that joint optimal scaling across model and\ndataset sizes is governed by a single invariant: theoperator normof theoutput layer. Across models with up to 1.3B parameters trained on up to 138B\ntokens, the optimallearning rate/batch sizepair (eta^{ast}, B^{ast})\nconsistently has the sameoperator normvalue - a phenomenon we term norm\ntransfer. This constant norm condition is necessary but not sufficient: while\nfor each dataset size, multiple (eta, B) reach the optimal norm, only a\nunique (eta^{ast}, B^{ast}) achieves the best loss. As a sufficient\ncondition, we provide the first measurement of (eta^{ast}, B^{ast})\nscaling with dataset size for Scion, and find that the scaling rules are\nconsistent with those of theAdam optimizer. Tuning per-layer-group learning\nrates also improves model performance, with theoutput layerbeing the most\nsensitive and hidden layers benefiting from lowerlearning rates. We provide\npractical insights on norm-guided optimal scaling and release our Distributed\nScion (Disco) implementation with logs from over two thousand runs to support\nresearch onLLM training dynamicsat scale. We show how to optimally scale LLM pretraining with norm-based optimizers (e.g. Scion),jointlyin data, model width and model depth. tldr; you need to watch out for your layer norms to stay in the optimal region (necessary condition) + use our scaling rules to transfer optimal hyperparameters across dataset sizes (sufficient condition): batch sizeB* ~ sqrt(D)and learning rateeta* ~ D^{-1/4}. Let us know your thoughts! This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2541041",
    "title": "Optimal Scaling Needs Optimal Norm",
    "authors": [],
    "publication_year": 2025,
    "github_url": "https://github.com/SDLAML/disco",
    "huggingface_url": "https://huggingface.co/papers/2510.03871",
    "upvote": 29
  }
}