{
  "context": "DreamOmni2 addresses limitations in instruction-based image editing and subject-driven generation by introducing multimodal instruction-based editing and generation tasks, utilizing feature mixing, index encoding, and joint training with a VLM. Recent advancements in instruction-based image editing and subject-driven\ngeneration have garnered significant attention, yet both tasks still face\nlimitations in meeting practical user needs.Instruction-based editingrelies\nsolely on language instructions, which often fail to capture specific editing\ndetails, making reference images necessary. Meanwhile, subject-driven\ngeneration is limited to combining concrete objects or people, overlooking\nbroader, abstract concepts. To address these challenges, we propose two novel\ntasks:multimodal instruction-based editingand generation. These tasks support\nboth text and image instructions and extend the scope to include both concrete\nand abstract concepts, greatly enhancing their practical applications. We\nintroduce DreamOmni2, tackling two primary challenges: data creation and model\nframework design. Our data synthesis pipeline consists of three steps: (1)\nusing afeature mixingmethod to create extraction data for both abstract and\nconcrete concepts, (2) generatingmultimodal instruction-based editingtraining\ndata using the editing and extraction models, and (3) further applying the\nextraction model to create training data for multimodal instruction-based\nediting. For the framework, to handle multi-image input, we propose an index\nencoding andposition encodingshift scheme, which helps the model distinguish\nimages and avoid pixel confusion. Additionally, we introducejoint trainingwith theVLMand our generation/editing model to better process complex\ninstructions. In addition, we have proposed comprehensive benchmarks for these\ntwo new tasks to drive their development. Experiments show that DreamOmni2 has\nachieved impressive results. Models and codes will be released. (1) Multimodal Instruction-based Generation For traditional subject-driven generation based on concrete objects, DreamOmni2 achieves the best results among open-source models, showing superior identity and pose consistency. Additionally, DreamOmni2 can reference abstract attributes (such as material, texture, makeup, hairstyle, posture, design style, artistic style, etc.), even surpassing commercial models in this area. (2) Multimodal Instruction-based Editing Beyond traditional instruction-based editing models, DreamOmni2 supports multimodal instruction editing. In everyday editing tasks, there are often elements that are difficult to describe purely with language and require reference images. Our model addresses this need, supporting references to any concrete objects and abstract attributes, with performance comparable to commercial models. (3) Unified Generation and Editing Model Building upon these two new tasks, we introduce DreamOmni2, which is capable of multimodal instruction-based editing and generation under any concrete or abstract concept guidance. Overall, DreamOmni2 is a more intelligent and powerful open-sourced unified generation and editing model, offering enhanced capabilities across a wide range of tasks. Project page:https://pbihao.github.io/projects/DreamOmni2/index.htmlPaper:https://arxiv.org/abs/2510.06679Code:https://github.com/dvlab-research/DreamOmni2  This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend  لباس تو این عکس رو بکن قرمز لباس تو این عکس رو سفید کن Set image color white ·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2541011",
    "title": "DreamOmni2: Multimodal Instruction-based Editing and Generation",
    "authors": [
      "Chengyao Wang"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/dvlab-research/DreamOmni2",
    "huggingface_url": "https://huggingface.co/papers/2510.06679",
    "upvote": 73
  }
}