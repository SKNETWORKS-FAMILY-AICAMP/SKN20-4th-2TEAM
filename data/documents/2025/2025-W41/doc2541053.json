{
  "context": "Low-precision training of transformer models with flash attention suffers from catastrophic loss explosions due to low-rank representations and biased rounding errors, which are addressed by a minimal modification to the flash attention mechanism. The pursuit of computational efficiency has driven the adoption oflow-precision formatsfor trainingtransformer models. However, this progress\nis often hindered by notorioustraining instabilities. This paper provides the\nfirst mechanistic explanation for a long-standing and unresolved failure case\nwhere training withflash attentionin low-precision settings leads to\ncatastrophic loss explosions. Our in-depth analysis reveals that the failure is\nnot a random artifact but caused by two intertwined phenomena: the emergence of\nsimilarlow-rank representationswithin the attention mechanism and the\ncompounding effect ofbiased rounding errorsinherent in low-precision\narithmetic. We demonstrate how these factors create a vicious cycle of error\naccumulation that corruptsweight updates, ultimately derailing the training\ndynamics. To validate our findings, we introduce a minimal modification to theflash attentionthat mitigates the bias in rounding errors. This simple change\nstabilizes the training process, confirming our analysis and offering a\npractical solution to this persistent problem. Training transformer models with low-precision formats promises substantial computational savings but often suffers from severe instability issues. This paper uncovers the mechanistic cause behind a long-standing failure mode of flash attention under low precision, revealing how low-rank representation collapse and biased rounding errors jointly trigger catastrophic loss explosion. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2541053",
    "title": "Why Low-Precision Transformer Training Fails: An Analysis on Flash\n  Attention",
    "authors": [
      "Haiquan Qiu"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/ucker/why-low-precision-training-fails",
    "huggingface_url": "https://huggingface.co/papers/2510.04212",
    "upvote": 23
  }
}