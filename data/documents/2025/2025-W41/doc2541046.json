{
  "context": "A language-mixed chain-of-thought reasoning approach improves performance in Korean-specific tasks by switching between English and Korean, achieving state-of-the-art results across various benchmarks. Recent frontier models employ longchain-of-thought reasoningto explore\nsolution spaces in context and achieve stonger performance. While many works\nstudy distillation to build smaller yet capable models, most focus on English\nand little is known about language-specific reasoning. To bridge this gap, we\nfirst introduct **Language-Mixed CoT**, a reasoning schema that switches\nbetween English and a target language, using English as an anchor to excel in\nreasoning while minimizing translation artificats. As a Korean case study, we\ncurate **Yi-Sang**: 5.79M native-Korean prompts from web Q&A, exams, STEM, and\ncode; 3.7M long reasoning traces generated fromQwen3-32B; and a targeted 260k\nhigh-yield subset. We train ninve models (4B-35B) across six families (Qwen2.5,\nLlama-3.1, Gemma-3, etc). Our best model, **KO-REAson-35B**, achieves\nstate-of-the-art performance, with the highest overall average score (64.0 \\pm\n25), ranking first on 5/9 benchmarks and second on the remainder. Samller and\nmid-sized models also benefit substantially, with an average improvement of\n+18.6 points across teh evaluated nine benchmarks. Ablations show\n**Language-Mixed CoT** is more effective thanmonolingual CoT, also resulting\nincross-lingualandmult-modalperformance gains. We release our data-curation\npipeline, evaluation system, datasets, and models to advance research on\nlanguage-specific reasoning. Data and model collection:\nhttps://huggingface.co/KOREAson. Link to Models:https://huggingface.co/collections/KOREAson/ko-reason-1009-68e7a925c3c41417dee4db3c This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2541046",
    "title": "Pushing on Multilingual Reasoning Models with Language-Mixed\n  Chain-of-Thought",
    "authors": [
      "Hyunwoo Ko",
      "Dasol Choi"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2510.04230",
    "upvote": 26
  }
}