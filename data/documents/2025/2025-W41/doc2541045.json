{
  "context": "A new quantization method, Micro-Rotated-GPTQ, addresses the challenges of 4-bit floating-point formats MXFP4 and NVFP4, achieving high performance and accuracy in large language model inference. The recent hardware-accelerated microscaling 4-bit floating-point formats\nsuch asMXFP4andNVFP4, supported on NVIDIA and AMD GPUs, promise to\nrevolutionize large language model (LLM) inference. Yet, their practical\nbenefits remain unproven. We present the first comprehensive study ofMXFP4andNVFP4forpost-training quantization, revealing gaps between their promise and\nreal-world performance. Our analysis shows that state-of-the-art methods\nstruggle with FP4, due to two key issues: (1)NVFP4's small group size provably\nneutralizes traditional outlier mitigation techniques; (2)MXFP4's power-of-two\nscale quantization severely degrades accuracy due to high induced error. To\nbridge this gap, we introduce Micro-Rotated-GPTQ(MR-GPTQ), a variant of the\nclassicGPTQquantization algorithm that tailors the quantization process to\nFP4's unique properties, by usingblock-wise Hadamard transformsandformat-specific optimizations. We support our proposal with a set of\nhigh-performanceGPU kernelsthat enable the MR-GPTQformat with negligible\noverhead, byrotation fusioninto the weights, and fast online computation of\nthe activations. This leads to speedups vs. FP16 of up to 3.6x layer-wise, and\n2.2x end-to-end on NVIDIA B200, and of 6x layer-wise and 4x end-to-end on\nRTX5090. Our extensive empirical evaluation demonstrates that MR-GPTQmatches\nor outperforms state-of-the-art accuracy, significantly boostingMXFP4, to the\npoint where it nears that ofNVFP4. We conclude that, while FP4 is not an\nautomatic upgrade over INT4, format-specialized methods like MR-GPTQcan unlock\na new frontier ofaccuracy-performance trade-offs. In this work we present a systematic study of FP4 quantization for large language models. We examine how scale quantization and rotations affect the performance of the quantized model, and we introduce Micro‑Rotated‑GPTQ (MR‑GPTQ)—a variant of the classic GPTQ algorithm that tailors the quantization process to FP4 and outperforms the previous state‑of‑the‑art methods. Links This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Great work! ·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2541045",
    "title": "Bridging the Gap Between Promise and Performance for Microscaling FP4\n  Quantization",
    "authors": [
      "Denis Kuznedelev"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/IST-DASLab/FP-Quant",
    "huggingface_url": "https://huggingface.co/papers/2509.23202",
    "upvote": 27
  }
}