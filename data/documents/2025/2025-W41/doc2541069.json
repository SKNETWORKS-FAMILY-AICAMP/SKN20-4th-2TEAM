{
  "context": "Co-Evolving Multi-Agent Systems (CoMAS) enable LLM-based agents to improve autonomously through inter-agent interactions and intrinsic rewards, achieving state-of-the-art performance. Self-evolutionis a central research topic in enabling large language model\n(LLM)-based agents to continually improve their capabilities after pretraining.\nRecent research has witnessed a transition from reinforcement learning\n(RL)-free to RL-based methods. Current RL-based methods either rely on dense\nexternal reward signals or extractintrinsic reward signalsfrom LLMs\nthemselves. However, these approaches diverge from theself-evolutionmechanisms observed in human intelligence, where individuals learn and improve\nthrough mutual discussion and collaboration. In this work, we introduceCo-Evolving Multi-Agent Systems (CoMAS), a novel framework that enables agents\nto improve autonomously by learning from inter-agent interactions without\nexternal supervision. CoMAS generates intrinsic rewards from rich discussion\ndynamics, employs anLLM-as-a-judgemechanism to formulate these rewards, and\noptimizes each agent's policy through RL, thereby enablingdecentralizedandscalable co-evolution. Experimental results demonstrate that CoMAS consistently\noutperforms untrained agents and achieves state-of-the-art performance across\nmost evaluation settings. Ablation studies confirm the necessity of\ninteraction-based reward signals and reveal promising scalability as the number\nand diversity of agents increase. These findings establish CoMAS as a novel and\neffective paradigm forself-evolutionin LLM-based agents. We’re excited to share our latest work,CoMAS: Co-Evolving Multi-Agent Systems via Interaction Rewards. In this study, we question whether LLM-based agents can continuously improve by learning from mutual interaction, rather than dense external or intrinsic supervision. Our proposed CoMAS framework addresses this by deriving intrinsic reward signals from inter‑agent collaboration and using them to guide reinforcement learning–based policy optimization. Our initial results show that CoMAS not only stabilizes self-learning but also improves transferability and multi-agent collaboration. In short, it’s a step toward more autonomous, collective intelligence in multi-agent systems. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend ·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2541069",
    "title": "CoMAS: Co-Evolving Multi-Agent Systems via Interaction Rewards",
    "authors": [
      "Xiangyuan Xue"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/xxyQwQ/CoMAS",
    "huggingface_url": "https://huggingface.co/papers/2510.08529",
    "upvote": 18
  }
}