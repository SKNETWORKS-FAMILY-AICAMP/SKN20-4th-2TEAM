{
  "context": "A novel dual-tower diffusion transformer with a Dual CrossAttention mechanism addresses challenges in Text-to-Sounding-Video generation by disentangling captions and enabling symmetric information exchange. This study focuses on a challenging yet promising task,\nText-to-Sounding-Video (T2SV) generation, which aims to generate a video with\nsynchronized audio from text conditions, meanwhile ensuring both modalities are\naligned with text. Despite progress in joint audio-video training, two critical\nchallenges still remain unaddressed: (1) a single, shared text caption where\nthe text for video is equal to the text for audio often creates modal\ninterference, confusing the pretrained backbones, and (2) the optimal mechanism\nfor cross-modal feature interaction remains unclear. To address these\nchallenges, we first propose theHierarchical Visual-Grounded Captioning(HVGC)\nframework that generates pairs of disentangled captions, a video caption, and\nan audio caption, eliminating interference at the conditioning stage. Based onHVGC, we further introduceBridgeDiT, a noveldual-tower diffusion transformer,\nwhich employs aDual CrossAttention(DCA) mechanism that acts as a robust\n``bridge\" to enable a symmetric, bidirectional exchange of information,\nachieving both semantic andtemporal synchronization. Extensive experiments on\nthree benchmark datasets, supported by human evaluations, demonstrate that our\nmethod achieves state-of-the-art results on most metrics. Comprehensive\nablation studies further validate the effectiveness of our contributions,\noffering key insights for the future T2SV task. All the codes and checkpoints\nwill be publicly released. This study focuses on a challenging yet promising task, Text-to-Sounding-Video (T2SV) generation, which aims to generate a video with synchronized audio from text conditions, meanwhile ensuring both modalities are aligned with text. Despite progress in joint audio-video training, two critical challenges still remain unaddressed: (1) a single, shared text caption where the text for video is equal to the text for audio often creates modal interference, confusing the pretrained backbones, and (2) the optimal mechanism for cross-modal feature interaction remains unclear. To address these challenges, we first propose the Hierarchical Visual-Grounded Captioning (HVGC) framework that generates pairs of disentangled captions, a video caption, and an audio caption, eliminating interference at the conditioning stage. Based on HVGC, we further introduce BridgeDiT, a novel dual-tower diffusion transformer, which employs a Dual CrossAttention (DCA) mechanism that acts as a robust ``bridge\" to enable a symmetric, bidirectional exchange of information, achieving both semantic and temporal synchronization. Extensive experiments on three benchmark datasets, supported by human evaluations, demonstrate that our method achieves state-of-the-art results on most metrics. Comprehensive ablation studies further validate the effectiveness of our contributions, offering key insights for the future T2SV task. All the codes and checkpoints will be publicly released. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2541094",
    "title": "Taming Text-to-Sounding Video Generation via Advanced Modality Condition\n  and Interaction",
    "authors": [
      "Kaisi Guan"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2510.03117",
    "upvote": 11
  }
}