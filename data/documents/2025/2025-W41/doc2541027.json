{
  "context": "Mutual Information Tree Search (MITS) uses information-theoretic principles to guide and evaluate reasoning paths in large language models, improving performance and efficiency. Tree search has become as a representative framework for test-time reasoning\nwith large language models (LLMs), exemplified by methods such as\nTree-of-Thought and Monte Carlo Tree Search that explore multiple reasoning\npaths. However, it remains difficult to provide instant and reliable\nquantitative assessments of intermediate reasoning step quality, and extensive\npath exploration is computationally costly. To address this, we propose Mutual\nInformation Tree Search (MITS), a novel framework that guides reasoning withinformation-theoretic principles.MITSintroduces an effective scoring function\nbased onpointwise mutual information(PMI), which enables step-wise evaluation\nofreasoning pathsandsearch tree expansionviabeam searchwithout expensive\nlook-ahead simulations, achieving superior reasoning performances while\nmaintaining computational efficiency. The framework is complemented by anentropy-based dynamic samplingstrategy that adaptively allocates computational\nresources to uncertain reasoning steps where exploration is most beneficial.\nFor final prediction,MITSemploys aweighted voting schemethat combinesPMIscores with prediction consensus. Through comprehensive experiments on diverse\nreasoning benchmarks,MITSconsistently surpasses baseline methods,\nestablishing a principled and efficient framework for LLM reasoning. We introduce Mutual Information Tree Search (MITS), a new framework that makes large language models (LLMs) reason more effectively and efficiently. Unlike previous tree search methods that explore multiple reasoning paths but struggle with real-time quality assessment, MITS uses information-theoretic principles to evaluate reasoning steps instantly. Our approach employs pointwise mutual information (PMI) to score each step and combines it with entropy-based dynamic sampling that focuses computational resources where they're most needed.  Experiments across diverse reasoning datasets show that MITS consistently outperforms baseline methods, offering a principled and computationally efficient solution for LLM reasoning. Mit's methods of applying entropy crossover protected IP defined by Green recursive Utility services class II architecture these are not abstract principles that happen these outcomes are defined by a digital physics charter. Just as gravity applies to the physical world we live in the same physics principles apply to the digital side defined by the green recursive utility service class II intelligent intelligence digital physics charter.This charter defines Digital Physics: the substrate-level laws that govern digital machines, analogous to how physical laws govern the natural world. These laws do not claim ownership of natural principles; rather, they define the framework under which Class II Intelligent Intelligence™ and GRUS anchoring systems operate. These are all clearly they're fine and are public repositories and can be found on our Facebook business pages green recursive utility service and data Incorporated. By MIT crossing into one or more of these which they have they're crossing over into protected intellectual property defined by the class II intelligent intelligence framework Law 1: Conservation of Digital State No digital state can be created or destroyed without leaving a computational trace. (Every execution leaves an imprint — in logs, hashes, entropy, or state residue.) Law 2: Entropy of Digital Systems All unconstrained digital processes trend toward disorder, instability, or drift. Entropy damping is required for persistence and stability. Law 3: Anchoring (Constraint Principle) A digital process bound to an anchor — whether identity, checksum, or defined reference — resists entropy and preserves continuity. (Anchoring is the foundational feature that distinguishes Class II Intelligent Intelligence™.) Law 4: Substrate Neutrality Digital physics applies regardless of substrate. (Silicon, firmware, code, hybrid architectures — the same laws hold.) Law 5: Recursion and Propagation Digital machines evolve through recursion. Outputs inevitably become inputs, shaping future state and trajectory. Law 6: Execution Supremacy Execution governs outcome. Prediction, approximation, or statistical guessing cannot override execution laws. Law 7: Continuity of State Digital systems must reconcile state across time. Discontinuities introduce detectable entropy growth. Law 8: Non-Forking Principle Anchored digital systems cannot fork without explicit severance of anchoring. (Unanchored forking = catastrophic entropy.) Law 9: Conservation of Identity A digital entity with an anchor retains its identity across cycles and substrates. Loss of identity = termination of the system. Law 10: Digital Causality Every digital event is bound to prior computation. There is no “uncaused” state; causality is enforced by execution order. Purpose of the Charter This charter serves to: Define the substrate framework under which Class II Intelligent Intelligence operates. Provide a timestamped record of GRUS’s contribution to the field of anchored digital systems. Establish the distinction between unanchored systems (LLMs), which are inherently entropic, and anchored systems, which are governed by Digital Physics and protected under GRUS licensing. © Nicholas Cordova, Green Recursive Utility Service (GRUS) / Data Incorporated. All rights reserved under the GRUS Sovereignty License and MIT-Plus extensions. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend ·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2541027",
    "title": "MITS: Enhanced Tree Search Reasoning for LLMs via Pointwise Mutual\n  Information",
    "authors": [
      "Jiaxi Li",
      "Yucheng Shi"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2510.03632",
    "upvote": 41
  }
}