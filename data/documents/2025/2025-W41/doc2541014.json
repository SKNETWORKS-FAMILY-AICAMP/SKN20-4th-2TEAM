{
  "context": "UniVideo, a dual-stream framework combining a Multimodal Large Language Model and a Multimodal DiT, extends unified modeling to video generation and editing, achieving state-of-the-art performance and supporting task composition and generalization. Unified multimodal models have shown promising results in multimodal content\ngeneration and editing but remain largely limited to the image domain. In this\nwork, we present UniVideo, a versatile framework that extends unified modeling\nto the video domain. UniVideo adopts adual-stream design, combining aMultimodal Large Language Model(MLLM) for instruction understanding with aMultimodal DiT(MMDiT) for video generation. This design enables accurate\ninterpretation of complex multimodal instructions while preserving visual\nconsistency. Built on this architecture, UniVideo unifies diverse video\ngeneration and editing tasks under a single multimodal instruction paradigm and\nis jointly trained across them. Extensive experiments demonstrate that UniVideo\nmatches or surpasses state-of-the-art task-specific baselines intext/image-to-video generation,in-context video generationand in-context\nvideo editing. Notably, the unified design of UniVideo enables two forms of\ngeneralization. First, UniVideo supportstask composition, such as combining\nediting withstyle transfer, by integrating multiple capabilities within a\nsingle instruction. Second, even without explicit training on free-form video\nediting, UniVideo transfers its editing capability from large-scale image\nediting data to this setting, handling unseen instructions such as\ngreen-screening characters or changing materials within a video. Beyond these\ncore capabilities, UniVideo also supportsvisual-prompt-based video generation,\nwhere the MLLM interprets visual prompts and guides the MMDiT during synthesis.\nTo foster future research, we will release our model and code. Unified multimodal models have shown promising results in multimodal content generation and editing but remain largely limited to the image domain. In this work, we present UniVideo, a versatile framework that extends unified modeling to the video domain. UniVideo adopts a dual-stream design, combining a Multimodal Large Language Model (MLLM) for instruction understanding with a Multimodal DiT (MMDiT) for video generation. This design enables accurate interpretation of complex multimodal instructions while preserving visual consistency. Built on this architecture, UniVideo unifies diverse video generation and editing tasks under a single multimodal instruction paradigm and is jointly trained across them. Extensive experiments demonstrate that UniVideo matches or surpasses state-of-the-art task-specific baselines in text/image-to-video generation, in-context video generation and in-context video editing. Notably, the unified design of UniVideo enables two forms of generalization. First, UniVideo supports task composition, such as combining editing with style transfer, by integrating multiple capabilities within a single instruction. Second, even without explicit training on free-form video editing, UniVideo transfers its editing capability from large-scale image editing data to this setting, handling unseen instructions such as green-screening characters or changing materials within a video. Beyond these core capabilities, UniVideo also supports visual-prompt-based video generation, where the MLLM interprets visual prompts and guides the MMDiT during synthesis. To foster future research, we will release our model and code. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Our model and code will be released underhttps://github.com/KwaiVGI/UniVideoandhttps://huggingface.co/KwaiVGI/UniVideo Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2541014",
    "title": "UniVideo: Unified Understanding, Generation, and Editing for Videos",
    "authors": [
      "Cong Wei"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/KwaiVGI/UniVideo",
    "huggingface_url": "https://huggingface.co/papers/2510.08377",
    "upvote": 71
  }
}