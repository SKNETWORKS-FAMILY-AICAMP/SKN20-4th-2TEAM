{
  "context": "Training-Free GRPO enhances LLM agent performance in specialized domains by learning experiential knowledge as a token prior without parameter updates, improving out-of-domain tasks with minimal data. Recent advances inLarge Language Model (LLM)agents have demonstrated their\npromising general capabilities. However, their performance in specialized\nreal-world domains often degrades due to challenges in effectively integrating\nexternal tools and specific prompting strategies. While methods like agentic\nreinforcement learning have been proposed to address this, they typically rely\non costly parameter updates, for example, through a process that usesSupervised Fine-Tuning (SFT)followed by aReinforcement Learning (RL)phase\nwithGroup Relative Policy Optimization (GRPO)to alter the output\ndistribution. However, we argue that LLMs can achieve a similar effect on the\noutput distribution by learning experiential knowledge as atoken prior, which\nis a far more lightweight approach that not only addresses practical data\nscarcity but also avoids the common issue of overfitting. To this end, we\npropose Training-Free Group Relative Policy Optimization (Training-Free GRPO),\na cost-effective solution that enhances LLM agent performance without any\nparameter updates. Our method leverages thegroup relative semantic advantageinstead of numerical ones within each group of rollouts, iteratively distilling\nhigh-quality experiential knowledge duringmulti-epoch learningon a minimal\nground-truth data. Such knowledge serves as the learnedtoken prior, which is\nseamlessly integrated during LLM API calls to guide model behavior. Experiments\nonmathematical reasoningandweb searching tasksdemonstrate thatTraining-Free GRPO, when applied toDeepSeek-V3.1-Terminus, significantly\nimproves out-of-domain performance. With just a few dozen training samples,Training-Free GRPOoutperforms fine-tuned small LLMs with marginal training\ndata and cost. Proposes Training-Free GRPO to boost LLM agent performance without parameter updates by distilling experiential knowledge as a token prior during limited-data, multi-epoch learning. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2541024",
    "title": "Training-Free Group Relative Policy Optimization",
    "authors": [
      "Yuzheng Cai",
      "Yuchen Shi",
      "Yulei Qin",
      "Xiaoyu Tan",
      "Xing Sun"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/TencentCloudADP/youtu-agent/tree/training_free_GRPO",
    "huggingface_url": "https://huggingface.co/papers/2510.08191",
    "upvote": 44
  }
}