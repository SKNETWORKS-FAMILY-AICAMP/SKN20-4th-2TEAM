{
  "context": "The Reactive Transformer (RxT) addresses the limitations of stateless Transformers in conversational AI by using an event-driven paradigm with a fixed-size Short-Term Memory (STM) system, achieving linear scaling and low latency. TheTransformer architecturehas become the de facto standard for Large\nLanguage Models (LLMs), demonstrating remarkable capabilities in language\nunderstanding and generation. However, its application in conversational AI is\nfundamentally constrained by itsstateless natureand the quadratic\ncomputational complexity (O(L^2)) with respect to sequence length L.\nCurrent models emulate memory by reprocessing an ever-expanding conversation\nhistory with each turn, leading to prohibitive costs and latency in long\ndialogues. This paper introduces theReactive Transformer (RxT), a novel\narchitecture designed to overcome these limitations by shifting from a\ndata-driven to anevent-driven paradigm. RxT processes each conversational turn\nas a discrete event in real-time, maintaining context in an integrated,\nfixed-sizeShort-Term Memory (STM)system. The architecture features a distinct\noperational cycle where agenerator-decoderproduces a response based on the\ncurrent query and the previous memory state, after which amemory-encoderand a\ndedicatedMemory Attention networkasynchronously update the STM with a\nrepresentation of the complete interaction. This design fundamentally alters\nthe scaling dynamics, reducing the total user-facing cost of a conversation\nfrom quadratic (O(N^2 cdot T)) to linear (O(N cdot T)) with respect to\nthe number of interactions N. By decoupling response generation from memory\nupdates, RxT achieves low latency, enabling truly real-time, stateful, and\neconomically viable long-form conversations. We validated our architecture with\na series of proof-of-concept experiments on synthetic data, demonstrating\nsuperior performance andconstant-time inference latencycompared to a baseline\nstateless model of comparable size. Paper is introducing Reactive Transfomer (RxT) architecture for stateful real-time processing, that's outperforming same size stateless decoder-only model in small-scale experiments. Architecture advantages: This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2541050",
    "title": "Reactive Transformer (RxT) -- Stateful Real-Time Processing for\n  Event-Driven Reactive Language Models",
    "authors": [
      "Adam Filipek"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/RxAI-dev/rxlm",
    "huggingface_url": "https://huggingface.co/papers/2510.03561",
    "upvote": 24
  }
}