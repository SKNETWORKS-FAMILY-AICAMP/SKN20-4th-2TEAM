{
  "context": "Cache-to-Cache (C2C) enables direct semantic communication between LLMs using neural network projections, improving accuracy and reducing latency compared to text-based communication. Multi-LLM systems harness the complementary strengths of diverse Large\nLanguage Models, achieving performance and efficiency gains unattainable by a\nsingle model. In existing designs, LLMs communicate through text, forcing\ninternal representations to be transformed into output token sequences. This\nprocess both loses rich semantic information and incurs token-by-token\ngenerationlatency. Motivated by these limitations, we ask: Can LLMs\ncommunicate beyond text? Oracle experiments show that enriching theKV-Cachesemantics can improve response quality without increasing cache size,\nsupportingKV-Cacheas an effective medium for inter-model communication. Thus,\nwe proposeCache-to-Cache(C2C), a new paradigm for direct semantic\ncommunication between LLMs. C2C uses aneural networkto project and fuse the\nsource model'sKV-cachewith that of the target model to enable direct semantic\ntransfer. A learnablegating mechanismselects the target layers that benefit\nfrom cache communication. Compared with text communication, C2C utilizes the\ndeep, specialized semantics from both models, while avoiding explicit\nintermediate text generation. Experiments show that C2C achieves 8.5-10.5%\nhigher averageaccuracythan individual models. It further outperforms the text\ncommunication paradigm by approximately 3.0-5.0%, while delivering an average\n2.0x speedup inlatency. Our code is available at\nhttps://github.com/thu-nics/C2C. Can LLMs communicate beyond text? We explore Cache-to-Cache (C2C) as a new multi-LLM communication paradigm. It directly projects and fuses KV-caches between models to transfer semantics, achieving ~8.5â€“10.5% average accuracy gains over single models, ~3.0â€“5.0% over text-based exchange, and ~2Ã— lower latency. Code is open-sourced.  This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend I love the concept of learnable gating mechanism for selective layer fusion. 2times latency reduction is great progress. Thank you so much for your interest and kind words about our work! ðŸ˜ŠWeâ€™re really excited to see how the community will build on Cache-to-Cache communication and push this direction further. The context length of the two models are the same. Why choose such a settingï¼ŸOr I missed something. Thank you so much for asking. Yes, we deliberately keep the context the same to exam the influence of context content itself and semantic understandings. For text to text communication, for receiver model, the context is different from sharer, but all context uses receiverâ€™s understanding. In contrast, for cache to cache communication, the context incorporates both sharer and receiver understandings, while making the context unchanged. Note that one can certainly try to combine both, where context is updated by T2T while use C2C to enhance the semantic understandings as well Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2541008",
    "title": "Cache-to-Cache: Direct Semantic Communication Between Large Language\n  Models",
    "authors": [
      "Tianyu Fu",
      "Zihan Min",
      "Hanling Zhang",
      "Jichao Yan"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/thu-nics/C2C",
    "huggingface_url": "https://huggingface.co/papers/2510.03215",
    "upvote": 97
  }
}