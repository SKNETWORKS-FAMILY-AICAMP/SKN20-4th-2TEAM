{
  "context": "UP2You reconstructs high-fidelity 3D clothed portraits from unconstrained 2D photos using a data rectifier and pose-correlated feature aggregation, achieving superior geometric and texture accuracy. We present UP2You, the first tuning-free solution for reconstructing\nhigh-fidelity 3D clothed portraits from extremely unconstrained in-the-wild 2D\nphotos. Unlike previous approaches that require \"clean\" inputs (e.g., full-body\nimages with minimal occlusions, or well-calibrated cross-view captures), UP2You\ndirectly processes raw, unstructured photographs, which may vary significantly\nin pose, viewpoint, cropping, and occlusion. Instead of compressing data into\ntokens for slow online text-to-3D optimization, we introduce adata rectifierparadigm that efficiently converts unconstrained inputs into clean, orthogonal\nmulti-view images in a single forward pass within seconds, simplifying the 3D\nreconstruction. Central to UP2You is apose-correlated feature aggregationmodule (PCFA), that selectively fuses information from multiple reference\nimages w.r.t. target poses, enabling better identity preservation and nearly\nconstant memory footprint, with more observations. We also introduce aperceiver-based multi-reference shape predictor, removing the need for\npre-captured body templates. Extensive experiments on 4D-Dress, PuzzleIOI, and\nin-the-wild captures demonstrate that UP2You consistently surpasses previous\nmethods in both geometric accuracy (Chamfer-15%,P2S-18% on PuzzleIOI) and\ntexture fidelity (PSNR-21%,LPIPS-46% on 4D-Dress). UP2You is efficient (1.5\nminutes per person), and versatile (supports arbitrary pose control, and\ntraining-free multi-garment3D virtual try-on), making it practical for\nreal-world scenarios where humans are casually captured. Both models and code\nwill be released to facilitate future research on this underexplored task.\nProject Page: https://zcai0612.github.io/UP2You UP2You reconstructs high-quality textured meshes from unconstrained photos. Our approach effectively handles extremely unconstrained photo collections by rectifying them into orthogonal multi-view images and corresponding normal maps, enabling the reconstruction of detailed 3D clothed portraits.  This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2541105",
    "title": "UP2You: Fast Reconstruction of Yourself from Unconstrained Photo\n  Collections",
    "authors": [
      "Zeyu Cai",
      "Zhenyu Zhang",
      "Yuliang Xiu"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/zcai0612/UP2You",
    "huggingface_url": "https://huggingface.co/papers/2509.24817",
    "upvote": 8
  }
}