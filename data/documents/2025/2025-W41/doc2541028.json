{
  "context": "RLinf-VLA is a unified framework for scalable reinforcement learning training of vision-language-action models, offering improved performance and generalization compared to supervised fine-tuning. Recent progress in vision and language foundation models has significantly\nadvanced multimodal understanding, reasoning, and generation, inspiring a surge\nof interest in extending such capabilities to embodied settings through\nvision-language-action (VLA) models. Yet, most VLA models are still trained\nwithsupervised fine-tuning(SFT), which struggles to generalize under\ndistribution shifts due to error accumulation.Reinforcement learning(RL)\noffers a promising alternative by directly optimizing task performance through\ninteraction, but existing attempts remain fragmented and lack a unified\nplatform for fair and systematic comparison across model architectures and\nalgorithmic designs. To address this gap, we introduceRLinf-VLA, a unified and\nefficient framework for scalable RL training of VLA models. The system adopts a\nhighly flexibleresource allocationdesign that addresses the challenge of\nintegrating rendering, training, and inference in RL+VLA training. In\nparticular, forGPU-parallelized simulators,RLinf-VLAimplements a novelhybrid fine-grained pipeline allocationmode, achieving a 1.61x-1.88x speedup\nin training. Through a unified interface,RLinf-VLAseamlessly supports diverse\nVLA architectures (e.g.,OpenVLA,OpenVLA-OFT), multiple RL algorithms (e.g.,PPO,GRPO), and various simulators (e.g.,ManiSkill,LIBERO). In simulation, a\nunified model achieves 98.11\\% across 130LIBEROtasks and 97.66\\% across 25ManiSkilltasks. Beyond empirical performance, our study distills a set of best\npractices for applying RL to VLA training and sheds light on emerging patterns\nin this integration. Furthermore, we present preliminary deployment on a\nreal-worldFranka robot, where RL-trained policies exhibit stronger\ngeneralization than those trained with SFT. We envisionRLinf-VLAas a\nfoundation to accelerate and standardize research on embodied intelligence. Recent progress in vision and language foundation models has significantly advanced multimodal understanding, reasoning, and generation, inspiring a surge of interest in extending such capabilities to embodied settings through vision-language-action (VLA) models. Yet, most VLA models are still trained with supervised fine-tuning (SFT), which struggles to generalize under distribution shifts due to error accumulation. Reinforcement learning (RL) offers a promising alternative by directly optimizing task performance through interaction, but existing attempts remain fragmented and lack a unified platform for fair and systematic comparison across model architectures and algorithmic designs. To address thisgap, we introduce RLinf-VLA, a unified and efficient framework for scalable RL training of VLA models. The system adopts a highly flexible resource allocation design that addresses the challenge of integrating rendering, training, and inference in RL+VLA training. In particular, for GPU-parallelized simulators,RLinf-VLA implements a novel hybrid fine-grained pipeline allocation mode, achieving a 1.61x-1.88x speedup in training. Through a unified interface, RLinfVLA seamlessly supports diverse VLA architectures (e.g., OpenVLA, OpenVLAOFT), multiple RL algorithms (e.g., PPO, GRPO), and various simulators (e.g.,ManiSkill, LIBERO). In simulation, a unified model achieves 98.11% across 130 LIBERO tasks and 97.66% across 25 ManiSkill tasks. Beyond empirical performance, our study distills a set of best practices for applying RL to VLA training and sheds light on emerging patterns in this integration. Furthermore, we present preliminary deployment on a real-world Franka robot, where RL-trained policies exhibit stronger generalization than those trained with SFT. We envision RLinfVLA as a foundation to accelerate and standardize research on embodied intelligence. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2541028",
    "title": "RLinf-VLA: A Unified and Efficient Framework for VLA+RL Training",
    "authors": [
      "Wenhao Tang"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/RLinf/RLinf",
    "huggingface_url": "https://huggingface.co/papers/2510.06710",
    "upvote": 39
  }
}