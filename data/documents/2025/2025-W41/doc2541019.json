{
  "context": "Fast-dLLM v2, a block diffusion language model, efficiently converts pretrained autoregressive models for parallel text generation, achieving significant speedup without compromising accuracy. Autoregressive (AR)large language models(LLMs) have achieved remarkable\nperformance across a wide range of natural language tasks, yet their inherent\nsequential decoding limits inference efficiency. In this work, we propose\nFast-dLLM v2, a carefully designed block diffusion language model (dLLM) that\nefficiently adapts pretrained AR models into dLLMs for parallel text\ngeneration, requiring only approximately 1B tokens of fine-tuning. This\nrepresents a 500x reduction in training data compared to full-attention\ndiffusion LLMs such as Dream (580B tokens), while preserving the original\nmodel's performance. Our approach introduces a novel training recipe that\ncombines ablock diffusion mechanismwith a complementaryattention mask,\nenablingblockwise bidirectional context modelingwithout sacrificing AR\ntraining objectives. To further accelerate decoding, we design a hierarchical\ncaching mechanism: ablock-level cachethat stores historical context\nrepresentations across blocks, and asub-block cachethat enables efficient\nparallel generation within partially decoded blocks. Coupled with our parallel\ndecoding pipeline, Fast-dLLM v2 achieves up to 2.5x speedup over standard AR\ndecoding without compromising generation quality. Extensive experiments across\ndiverse benchmarks demonstrate that Fast-dLLM v2 matches or surpasses AR\nbaselines in accuracy, while delivering state-of-the-art efficiency among dLLMs\n- marking a significant step toward the practical deployment of fast and\naccurate LLMs. Code and model will be publicly released. How this paper is not paper of the day ? Thanks for your interst! We have submitted to daily paper.  What inference engine was used for this demo? Here is the visualization of generation process. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2541019",
    "title": "Fast-dLLM v2: Efficient Block-Diffusion LLM",
    "authors": [
      "Zhijian Liu"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/NVlabs/Fast-dLLM",
    "huggingface_url": "https://huggingface.co/papers/2509.26328",
    "upvote": 55
  }
}