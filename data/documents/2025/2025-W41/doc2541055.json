{
  "context": "Introducing reasoning data during pretraining significantly enhances LLM performance compared to post-training, with pretraining benefiting more from diverse data patterns while SFT benefits more from high-quality data. The prevailing paradigm for enhancing thereasoningabilities ofLLMsrevolves aroundpost-trainingon high-quality,reasoning-intensive data. While\nemerging literature suggests thatreasoning datais increasingly incorporated\nalso during themid-trainingstage-a practice that is relatively more\nproprietary and less openly characterized-the role of such data inpretrainingremains unclear. In particular, due to the opaqueness ofpretrainingcorpora in\nmost frontier models, the effect ofreasoning dataintroduced at different\nphases of pre- and/orpost-trainingis relatively less reported in the\nscientific literature. This raises several important questions: Is addingreasoning dataearlier duringpretrainingany better than introducing it duringpost-training? Could earlier inclusion riskoverfittingand harmgeneralization, or instead establish durable foundations that later fine-tuning\ncannot recover? We conduct the firstsystematic studyof howreasoningdata-varying in scale, diversity, and quality-affects LLM performance when\nintroduced at different stages of training. We find that front-loadingreasoning dataintopretrainingis critical (19% avg gain), establishing\nfoundational capabilities that cannot be fully replicated by later-stageSFT,\neven with more data. We uncover an asymmetric principle for optimal data\nallocation:pretrainingbenefits most from broad diversity inreasoningpatterns (11% avg gain), whileSFTis more sensitive to data quality (15% avg\ngain). We show that high-qualitypretrainingdata has latent effects, activated\nonly afterSFT, and that naively scalingSFTdata can be detrimental, washing\naway the benefits of earlyreasoninginjection. Our results challenge the\nconventional separation oflanguage modelingandreasoning, providing a\nprincipled guide for strategically allocating data across the entire training\npipeline to build more capable models. This work investigates the underexplored role of reasoning data in the pretraining phase of LLM development. Through controlled studies varying data scale, diversity, and quality, we find that front-loading reasoning data during pretraining yields lasting improvements‚Äîup to 19% average gains‚Äîthat cannot be recovered through later-stage SFT. We uncover a clear asymmetry: diverse reasoning patterns most benefit pretraining, while high-quality data drives post-training success. Moreover, high-quality pretraining data exhibits latent effects, activated only during fine-tuning, whereas na√Øvely scaling SFT data can erode prior gains. These findings challenge the conventional separation of language modeling and reasoning, providing a principled framework for allocating reasoning data across training stages. When should an LLM learn to reason? ü§î Early in pretraining or late in fine-tuning? Our new work, \"Front-Loading Reasoning,\" challenges the \"save it for later\" approach. We show that injecting reasoning data into pretraining is critical for building models that reach the frontier. The key? An asymmetric data strategy.üìù Blog:https://research.nvidia.com/labs/adlr/Synergy/üîóPaper:https://tinyurl.com/3tzkemtp  We find that \"front-loading\" reasoning data into pretraining creates a durable, compounding advantage.üìà Stage 1 (Pretraining): +16% avg. gain out of the gate.üìà Stage 2 (SFT): Advantage grows to +9.3% after fine-tuning.üìà Stage 3 (RL): Finishes with a massive +19% lead on expert benchmarks.SFT & RL amplify a strong foundation; they can't create one.   The optimal data strategy is phase-dependent:üß† Pretraining thrives on DIVERSITY & SCALE. A broad mix of reasoning patterns builds a robust foundation, giving an +11% boost over using only narrow, high-quality data at this stage.üéØ SFT demands QUALITY. Fine-tuning on a small, high-quality dataset is far more effective, boosting performance by +15% over a large, mixed-quality one. High-quality data has a surprising latent effect.Adding a small, high-quality dataset to a diverse pretraining mix showed minimal immediate gains. But after SFT, its value was \"unlocked,\" providing an additional +4% boost.A deep synergy exists: pretraining can instill the potential that alignment activates.  Can a model with no reasoning in its pretraining \"catch up\" by getting more SFT data?No.We doubled the SFT data for our baseline model. While it improved, it still couldn't match the performance of even the weakest reasoning-pretrained model.A strong start is irreplaceable.  Is more data always better in SFT? No.Our ablations show that blindly scaling SFT with mixed-quality data is actively HARMFUL.‚ùå Doubling the SFT data dropped math reasoning scores by -5%.‚úÖ Scaling with small high quality data provides consistent gains.SFT is for targeted refinement, not brute-force scaling. Our work provides a principled guide for training reasoning-centric LLMs:Don't wait:Inject reasoning data into pretraining.Be strategic:Use DIVERSE data for pretraining, emphasize HIGH-QUALITY data for SFT.Be careful:Avoid polluting your SFT with low-quality data.This moves us from \"more data\" to a smarter, phase-aware approach. Hi@SieraL! Thank you for publishing these insights and learnings in such a digestible manner!I would like to ask about the formatting of the added high-quality data added to the pretraining mixture.For the experiments done, were there any special care taken for the tokenization and packing of the high quality datasets?For example, do they need to be untruncated and tokenized with special tokens like what is commonly done for post-training, or were they simply truncated randomly and tokenized without any special tokens other than bos and eos? This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend ¬∑Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2541055",
    "title": "Front-Loading Reasoning: The Synergy between Pretraining and\n  Post-Training Data",
    "authors": [
      "Syeda Nahida Akter"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2510.03264",
    "upvote": 23
  }
}