{
  "context": "A 15-billion parameter multimodal reasoning model achieves competitive performance through a progressive training methodology without reinforcement learning, demonstrating efficient use of computational resources. We present Apriel-1.5-15B-Thinker, a 15-billion parameter open-weightsmultimodal reasoning modelthat achieves frontier-level performance through\ntraining design rather than sheer scale. Starting from Pixtral-12B, we apply a\nprogressive three-stage methodology: (1)depth upscalingto expand reasoning\ncapacity without pretraining from scratch, (2)staged continual pre-trainingthat first develops foundational text and vision understanding, then enhances\nvisual reasoning through targetedsynthetic data generationaddressing spatial\nstructure,compositional understanding, andfine-grained perception, and (3)\nhigh-qualitytext-only supervised fine-tuningon curated instruction-response\npairs with explicitreasoning tracesspanning mathematics, coding, science, and\ntool use. Notably, our model achieves competitive results without reinforcement\nlearning or preference optimization, isolating the contribution of our\ndata-centric continual pre-training approach. On the Artificial Analysis\nIntelligence Index, Apriel-1.5-15B-Thinker attains a score of 52, matching\nDeepSeek-R1-0528 despite requiring significantly fewer computational resources.\nAcross ten image benchmarks, its performance is on average within five points\nof Gemini-2.5-Flash and Claude Sonnet-3.7, a key achievement for a model\noperating withinsingle-GPU deploymentconstraints. Our results demonstrate\nthat thoughtful mid-training 2 design can close substantial capability gaps\nwithout massive scale, making frontier-level multimodal reasoning accessible to\norganizations with limited infrastructure. We release the model checkpoint, all\ntraining recipes, and evaluation protocols under the MIT license to to advance\nopen-source research. Introducing ServiceNowâ€™s 15B-parameter model that matches ğ——ğ—²ğ—²ğ—½ğ—¦ğ—²ğ—²ğ—¸â€“ğ—¥ğŸ­â€“ğŸ¬ğŸ±ğŸ®ğŸ´, ğ— ğ—¶ğ˜€ğ˜ğ—¿ğ—®ğ—¹â€“ğ—ºğ—²ğ—±ğ—¶ğ˜‚ğ—ºâ€“ğŸ­.ğŸ® and ğ—šğ—²ğ—ºğ—¶ğ—»ğ—¶ ğ—™ğ—¹ğ—®ğ˜€ğ—µ ğŸ®.ğŸ± on the Artificial Analysis Index (ğ—”ğ—”ğ—œ ğŸ±ğŸ®) â€” delivering comparable results at a ğ—³ğ—¿ğ—®ğ—°ğ˜ğ—¶ğ—¼ğ—» ğ—¼ğ—³ ğ˜ğ—µğ—² ğ˜€ğ—¶ğ˜‡ğ—² (at least 8-10 times smaller) ğ—™ğ—¿ğ—¼ğ—»ğ˜ğ—¶ğ—²ğ—¿-ğ—¹ğ—²ğ˜ƒğ—²ğ—¹ ğ—¿ğ—²ğ—®ğ˜€ğ—¼ğ—»ğ—¶ğ—»ğ—´ on a single GPUğ—¡ğ—¼ ğ—¥ğ—Ÿ ğ—½ğ—µğ—®ğ˜€ğ—² â€” the step-change comes from mid-trainingğ—¥ğ—²ğ—®ğ˜€ğ—¼ğ—»ğ˜€ ğ—¼ğ˜ƒğ—²ğ—¿ ğ—¶ğ—ºğ—®ğ—´ğ—²ğ˜€ -Â Image + Text mid training enables model to reason over images without additional trainingğ—šğ—¿ğ—²ğ—®ğ˜ ğ—®ğ˜ ğ—¿ğ—²ğ—®ğ˜€ğ—¼ğ—»ğ—¶ğ—»ğ—´ â€” AIME2025: 88, GPQA: 71, LCB: 73ğ—™ğ—¼ğ—¹ğ—¹ğ—¼ğ˜„ğ˜€ ğ—¶ğ—»ğ˜€ğ˜ğ—¿ğ˜‚ğ—°ğ˜ğ—¶ğ—¼ğ—»ğ˜€ reliably â€” IFBench: 62Tğ—®ğ˜‚ğŸ® ğ—•ğ—²ğ—»ğ—°ğ—µ (Telecom): 68 â†’ ready for real-world workflowsğ—¢ğ—½ğ—²ğ—»  ğ˜„ğ—²ğ—¶ğ—´ğ—µğ˜ğ˜€ model to further research and reproducibility (MIT license) will the data be released? This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Thanks for your work! I have a quick question: how do you organize the data formats for tasks like Image Reconstruction and Visual Matching in CPT Stage 2? I think this synthetic augmentation approach is particularly interesting. Thank you! arXiv explained breakdown of this paper ğŸ‘‰https://arxivexplained.com/papers/apriel-15-15b-thinker Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2541004",
    "title": "Apriel-1.5-15b-Thinker",
    "authors": [
      "Aman Tiwari",
      "Akintunde Oladipo",
      "Oluwanifemi Bamgbose",
      "Sai Rajeswar Mudumba",
      "Torsten Scholak"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2510.01141",
    "upvote": 119
  }
}