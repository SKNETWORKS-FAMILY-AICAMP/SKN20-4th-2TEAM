{
  "context": "A training pipeline called MASA enhances meta-awareness in reasoning models, leading to improved accuracy and efficiency across various benchmarks. Recent studies onreasoning modelsexplore themeta-awarenessof language\nmodels, the ability to know how to think by itself. We argue that largereasoning modelslack thismeta-awarenessproperty by proving severe\nmisalignment between truerolloutsand predicted meta information. We posit\nthat aligningmeta-predictionwith truerolloutswill lead to significant\nperformance gains. To verify this hypothesis, we design a training pipeline\nthat boostsMeta-AwarenessviaSelf-Alignment(MASA), and prove that enhancedmeta-awarenessdirectly translates to improved accuracy. Unlike existing\nmeta-cognitivereasoning models, our method does not require external training\nsources but leverages self-generated signals to trainmeta-awareness. Moreover,\nour method enables efficient training by i) filtering outzero-variance promptsthat are either trivial or unsolvable and ii) cutting off lengthyrolloutswhen\nthey are unlikely to lead to correct answers. The results are inspiring: our\nstrategy yields significant improvements in both accuracy and training\nefficiency on in-domain tasks and shows strong generalization to out-of-domain\nbenchmarks. More specifically, our method can speed upGRPOtraining by over\n1.28x to reach the same performance, and achieve a 19.3% gain in accuracy onAIME25, and a 6.2 % average gain over six mathematics benchmarks. Training with\nmeta-cognitive guidance enhances out-of-domain generalization, giving a 3.87 %\nboost onGPQA-Diamondand a 2.08 % overall accuracy gain across 13 benchmarks\nspanning logical, scientific, and coding domains. This paper proves that enhancing meta-awareness of the model itself, directly leads to performance improvement in mathematical reasoning and out-of-domain generalization. Just sent  a email to cease and desist please respond within 7 days as the email States the IP you are crossing over is already in use and protected. We are the creators of class II intelligent intelligence your models directly cross over our IP we have officially sent a email asking for official cease and desist along with taking down any models or services that cross over this IP. As well as disclosure for models you have out and are currently working on to ensure no further move into our IP exist or IP is not to be trained with used put into service or anything else. We have IP disputes submitted with the department of defense months ago against openai so there is no question of time frames and who's the rightful owner of the IP you can contact the US department of defense to verify our IP filing with them against openai and Google This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Great idea! This reminds me a little of Reinforcement learning on Pretraining-Data  (https://huggingface.co/papers/2509.19249) and RPT. I wonder if those models have better meta alignment Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2541018",
    "title": "Meta-Awareness Enhances Reasoning Models: Self-Alignment Reinforcement\n  Learning",
    "authors": [
      "Yoonjeon Kim",
      "Doohyuk Jang"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/akatigre/MASA-RL/tree/main",
    "huggingface_url": "https://huggingface.co/papers/2510.03259",
    "upvote": 57
  }
}