{
  "context": "A benchmark and training strategy for reward models to improve long-context consistency and performance in large language models. Reward model(RM) plays a pivotal role in aligninglarge language model(LLM)\nwith human preferences. As real-world applications increasingly involve long\nhistory trajectories, e.g., LLM agent, it becomes indispensable to evaluate\nwhether a model's responses are not only high-quality but also grounded in and\nconsistent with the provided context. Yet, current RMs remain confined toshort-contextsettings and primarily focus on response-level attributes (e.g.,\nsafety or helpfulness), while largely neglecting the critical dimension of longcontext-response consistency. In this work, we introduce Long-RewardBench, a\nbenchmark specifically designed forlong-contextRM evaluation, featuring bothPairwise ComparisonandBest-of-Ntasks. Our preliminary study reveals that\neven state-of-the-art generative RMs exhibit significant fragility inlong-contextscenarios, failing to maintain context-aware preference judgments.\nMotivated by the analysis of failure patterns observed in model outputs, we\npropose a generalmulti-stage trainingstrategy that effectively scales\narbitrary models into robustLong-context RMs(LongRMs). Experiments show that\nour approach not only substantially improves performance onlong-contextevaluation but also preserves strongshort-contextcapability. Notably, our 8B\nLongRM outperforms much larger 70B-scale baselines and matches the performance\nof the proprietary Gemini 2.5 Pro model. Reward model (RM) plays a pivotal role in aligning large language model (LLM) with human preferences. As real-world applications increasingly involve long history trajectories, e.g., LLM agent, it becomes indispensable to evaluate whether a model's responses are not only high-quality but also grounded in and consistent with the provided context. Yet, current RMs remain confined to short-context settings and primarily focus on response-level attributes (e.g., safety or helpfulness), while largely neglecting the critical dimension of long context-response consistency. In this work, we introduce Long-RewardBench, a benchmark specifically designed for long-context RM evaluation, featuring both Pairwise Comparison and Best-of-N tasks. Our preliminary study reveals that even state-of-the-art generative RMs exhibit significant fragility in long-context scenarios, failing to maintain context-aware preference judgments. Motivated by the analysis of failure patterns observed in model outputs, we propose a general multi-stage training strategy that effectively scales arbitrary models into robust Long-context RMs (LongRMs). Experiments show that our approach not only substantially improves performance on long-context evaluation but also preserves strong short-context capability. Notably, our 8B LongRM outperforms much larger 70B-scale baselines and matches the performance of the proprietary Gemini 2.5 Pro model. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2541081",
    "title": "LongRM: Revealing and Unlocking the Context Boundary of Reward Modeling",
    "authors": [
      "Zecheng Tang",
      "Baibei Ji",
      "Quantong Qiu"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/LCM-Lab/LongRM",
    "huggingface_url": "https://huggingface.co/papers/2510.06915",
    "upvote": 14
  }
}