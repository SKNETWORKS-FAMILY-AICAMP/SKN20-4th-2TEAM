{
  "context": "HERO, a reinforcement learning framework, combines verifier signals with reward-model scores to enhance reasoning in large language models, outperforming both RM-only and verifier-only methods. Post-training for reasoning of large language models (LLMs) increasingly\nrelies onverifiable rewards:deterministic checkersthat provide 0-1\ncorrectness signals. While reliable, suchbinary feedbackis brittle--many\ntasks admit partially correct or alternative answers that verifiers\nunder-credit, and the resulting all-or-nothing supervision limits learning.Reward modelsoffer richer,continuous feedback, which can serve as a\ncomplementary supervisory signal to verifiers. We introduceHERO(Hybrid\nEnsemble Reward Optimization), areinforcement learningframework that\nintegrates verifier signals with reward-model scores in a structured way.HEROemploysstratified normalizationto bound reward-model scores within\nverifier-defined groups, preserving correctness while refining quality\ndistinctions, andvariance-aware weightingto emphasize challenging prompts\nwhere dense signals matter most. Across diverse mathematical reasoning\nbenchmarks,HEROconsistently outperforms RM-only and verifier-only baselines,\nwith strong gains on both verifiable and hard-to-verify tasks. Our results show\nthat hybrid reward design retains the stability of verifiers while leveraging\nthe nuance ofreward modelsto advance reasoning. HERO (Hybrid Ensemble Reward Optimization), a reinforcement learning framework that integrates verifier signals with reward-model scores in a structured way. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Thank you for the fantastic work on the HERO paper! The reward design is very insightful. I have a quick question about the variance-aware reweighting. My understanding is that GRPO normalize advantageswithineach prompt's response group, e.g.,Adv = (r - mean(r)) / std(r). If this is the case, the reweighting factorWwould apply to both the numerator and the denominator (std(W*r) = W*std(r)), causing it to be canceled out in Adv. So I suspect that the GRPO advantage normalization here is instead performed at the batch level rather than at the group level? This would preserveWin Adv. Could you please confirm if this understanding is correct? Thanks This is the author. Thanks for the reminder! We remove the advantage normalization during training. Sorry for forgetting mentioning it in the paper. Thanks for the reminder, will add it later. Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2541038",
    "title": "Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense",
    "authors": [
      "Leitian Tao"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2510.07242",
    "upvote": 30
  }
}