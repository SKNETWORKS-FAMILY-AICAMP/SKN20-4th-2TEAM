{
  "context": "Reinforce-Ada is an adaptive sampling framework for online reinforcement learning post-training of large language models, which accelerates convergence and improves performance by dynamically reallocating sampling effort based on prompt uncertainty. Reinforcement learningapplied tolarge language models(LLMs) for reasoning\ntasks is often bottlenecked by unstablegradient estimatesdue to fixed and\nuniform sampling of responses across prompts. Prior work such as GVM-RAFT\naddresses this by dynamically allocatinginference budgetper prompt to\nminimizestochastic gradient varianceunder a budget constraint. Inspired by\nthis insight, we propose Reinforce-Ada, anadaptive samplingframework for\nonline RL post-training of LLMs that continuously reallocates sampling effort\nto the prompts with the greatest uncertainty or learning potential. Unlike\nconventional two-stage allocation methods, Reinforce-Ada interleaves estimation\nand sampling in anonline successive eliminationprocess, and automatically\nstops sampling for a prompt once sufficient signal is collected. To stabilize\nupdates, we form fixed-size groups with enforcedreward diversityand computeadvantage baselinesusing global statistics aggregated over the adaptive\nsampling phase. Empirical results across multiple model architectures and\nreasoning benchmarks show that Reinforce-Ada accelerates convergence and\nimproves final performance compared to GRPO, especially when using the balanced\nsampling variant. Our work highlights the central role ofvariance-aware,adaptive data curationin enabling efficient and reliable reinforcement\nlearning for reasoning-capable LLMs. Code is available at\nhttps://github.com/RLHFlow/Reinforce-Ada. ü§î The Problem: Where are my training signals going?You might be wasting up to 60% of your compute. In RL methods like GRPO/RFT, prompts often generate \"zero-signal\" sample groups (all-pass or all-fail). No reward variance means no gradient, which equals wasted GPU cycles. Common fixes fall short:‚ùå Just drop them (DAPO)? A temporary fix that leaves many prompts untrained long-term.‚ùå More sampling (n=256) is expensive with diminishing returns. üí° Our First Step: GVM (NIPS‚Äô25) ‚ú® The new Answer: Online Adaptive Sampling (Reinforce-ada) The core of this new report is a simpler, more elegant \"online\" strategy that perfectly solves GVM's bottlenecks. We've merged estimation and allocation into a single, unified process: üöÄ Results & Implementation: This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend ¬∑Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2541078",
    "title": "Reinforce-Ada: An Adaptive Sampling Framework for Reinforce-Style LLM\n  Training",
    "authors": [],
    "publication_year": 2025,
    "github_url": "https://github.com/RLHFlow/Reinforce-Ada",
    "huggingface_url": "https://huggingface.co/papers/2510.04996",
    "upvote": 15
  }
}