{
  "context": "M2PO, a reinforcement learning algorithm, enables stable off-policy training with stale data by constraining the second moment of importance weights, achieving performance comparable to on-policy methods. Reinforcement learninghas been central to recent advances in large language\nmodel reasoning, but most algorithms rely onon-policy trainingthat demands\nfresh rollouts at every update, limiting efficiency and scalability.Asynchronous RLsystems alleviate this by decouplingrollout generationfrom\ntraining, yet their effectiveness hinges on tolerating largestalenessin\nrollout data, a setting where existing methods either degrade in performance or\ncollapse. We revisit this challenge and uncover a prosperity-before-collapse\nphenomenon: stale data can be as informative as on-policy data if exploited\nproperly. Building on this insight, we introduceM2PO(Second-Moment Trust\nPolicy Optimization), which constrains the second moment ofimportance weightsto suppress only extreme outliers while preserving informative updates.\nNotably,M2POsharply reduces the fraction ofclipped tokensunder highstaleness(from 1.22% to 0.06% over training), precisely masking high-variance\ntokens while maintaining stable optimization. Extensive evaluation across six\nmodels (from 1.7B to 32B) and eight benchmarks shows thatM2POdelivers stableoff-policy trainingeven with data stale by at least 256 model updates and\nmatches on-policy performance. Reinforcement learning has been central to recent advances in large language model reasoning, but most algorithms rely on on-policy training that demands fresh rollouts at every update, limiting efficiency and scalability. Asynchronous RL systems alleviate this by decoupling rollout generation from training, yet their effectiveness hinges on tolerating large staleness in rollout data, a setting where existing methods either degrade in performance or collapse. We revisit this challenge and uncover a prosperity-before-collapse phenomenon: stale data can be as informative as on-policy data if exploited properly. Building on this insight, we introduce M2PO (Second-Moment Trust Policy Optimization), which constrains the second moment of importance weights to suppress only extreme outliers while preserving informative updates. Notably, M2PO sharply reduces the fraction of clipped tokens under high staleness (from 1.22% to 0.06% over training), precisely masking high-variance tokens while maintaining stable optimization. Extensive evaluation across six models (from 1.7B to 32B) and eight benchmarks shows that M2PO delivers stable off-policy training even with data stale by at least 256 model updates and matches on-policy performance. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2541086",
    "title": "Prosperity before Collapse: How Far Can Off-Policy RL Reach with Stale\n  Data on LLMs?",
    "authors": [],
    "publication_year": 2025,
    "github_url": "https://github.com/Infini-AI-Lab/M2PO",
    "huggingface_url": "https://huggingface.co/papers/2510.01161",
    "upvote": 13
  }
}