{
  "context": "UniDoc-Bench is a large-scale benchmark for multimodal retrieval-augmented generation, evaluating systems across text, images, and their fusion in real-world document-centric scenarios. Multimodal retrieval-augmented generation(MM-RAG) is a key approach for\napplyinglarge language models(LLMs) and agents to real-worldknowledge bases,\nyet current evaluations are fragmented, focusing on either text or images in\nisolation or on simplified multimodal setups that fail to capture\ndocument-centric multimodal use cases. In this paper, we introduceUniDoc-Bench, the first large-scale, realistic benchmark for MM-RAG built from\n70k real-worldPDF pagesacross eight domains. Our pipeline extracts and links\nevidence from text, tables, and figures, then generates 1,600 multimodal QA\npairs spanningfactual retrieval,comparison,summarization, and logical\nreasoning queries. To ensure reliability, 20% of QA pairs are validated by\nmultiple annotators and expert adjudication.UniDoc-Benchsupports\napples-to-applescomparisonacross four paradigms: (1)text-only, (2)image-only, (3)multimodal text-image fusion, and (4) multimodal joint\nretrieval -- under a unified protocol with standardizedcandidate pools,prompts, andevaluation metrics. Our experiments show that multimodal\ntext-image fusion RAG systems consistently outperform both unimodal and jointly\nmultimodal embedding-based retrieval, indicating that neither text nor images\nalone are sufficient and that current multimodal embeddings remain inadequate.\nBeyond benchmarking, our analysis reveals when and howvisual contextcomplementstextual evidence, uncovers systematicfailure modes, and offers\nactionable guidance for developing morerobust MM-RAG pipelines. code:https://github.com/SalesforceAIResearch/UniDoc-Benchdata:https://huggingface.co/datasets/Salesforce/UniDoc-Bench UniDoc-Bench overview This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2541079",
    "title": "UNIDOC-BENCH: A Unified Benchmark for Document-Centric Multimodal RAG",
    "authors": [
      "Cab Qin"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/SalesforceAIResearch/UniDoc-Bench",
    "huggingface_url": "https://huggingface.co/papers/2510.03663",
    "upvote": 15
  }
}