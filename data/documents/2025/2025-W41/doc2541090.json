{
  "context": "PaDT, a unified paradigm for multimodal large language models, directly generates both textual and visual outputs, achieving state-of-the-art performance in visual perception tasks. Multimodal large language models(MLLMs) have advanced rapidly in recent\nyears. However, existing approaches for vision tasks often rely on indirect\nrepresentations, such as generating coordinates as text fordetection, which\nlimits performance and prevents dense prediction tasks likesegmentation. To\novercome these challenges, we introducePatch-as-Decodable Token(PaDT), a\nunified paradigm that enables MLLMs to directly generate both textual and\ndiverse visual outputs. Central toPaDTareVisual Reference Tokens(VRTs),\nderived fromvisual patch embeddingsof query images and interleaved seamlessly\nwith LLM's output textual tokens. Alightweight decoderthen transforms LLM's\noutputs intodetection,segmentation, andgrounding predictions. Unlike prior\nmethods,PaDTprocessesVRTsindependently at each forward pass and dynamically\nexpands the embedding table, thus improving localization and differentiation\namong similar objects. We further tailor a training strategy forPaDTby\nrandomly selectingVRTsforsupervised fine-tuningand introducing a robustper-token cross-entropy loss. Our empirical studies across four visual\nperception and understanding tasks suggestPaDTconsistently achieving\nstate-of-the-art performance, even compared with significantly larger MLLM\nmodels. The code is available at https://github.com/Gorilla-Lab-SCUT/PaDT. Multimodal large language models (MLLMs) have advanced rapidly in recent years. However, existing approaches for vision tasks often rely on indirect representations, such as generating coordinates as text for detection, which limits performance and prevents dense prediction tasks like segmentation. To overcome these challenges, we introduce Patch-as-Decodable Token (PaDT), a unified paradigm that enables MLLMs to directly generate both textual and diverse visual outputs. Central to PaDT are Visual Reference Tokens (VRTs), derived from visual patch embeddings of query images and interleaved seamlessly with LLM's output textual tokens. A lightweight decoder then transforms LLM's outputs into detection, segmentation, and grounding predictions. Unlike prior methods, PaDT processes VRTs independently at each forward pass and dynamically expands the embedding table, thus improving localization and differentiation among similar objects. We further tailor a training strategy for PaDT by randomly selecting VRTs for supervised fine-tuning and introducing a robust per-token cross-entropy loss. Our empirical studies across four visual perception and understanding tasks suggest PaDT consistently achieving state-of-the-art performance, even compared with significantly larger MLLM models. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2541090",
    "title": "Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in\n  MLLMs",
    "authors": [
      "Haojie Zhang"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/Gorilla-Lab-SCUT/PaDT",
    "huggingface_url": "https://huggingface.co/papers/2510.01954",
    "upvote": 12
  }
}