{
  "context": "DeepPrune, a novel framework using dynamic pruning and a specialized judge model, significantly reduces computational inefficiency in parallel scaling of large language models by pruning redundant reasoning traces. Parallel scaling has emerged as a powerful paradigm to enhance reasoning\ncapabilities inlarge language models(LLMs) by generating multipleChain-of-Thought(CoT) traces simultaneously. However, this approach introduces\nsignificant computational inefficiency due to inter-trace redundancy -- our\nanalysis reveals that over 80% ofparallel reasoningtraces yield identical\nfinal answers, representing substantial wasted computation. To address this\ncritical efficiency bottleneck, we proposeDeepPrune, a novel framework that\nenables efficient parallel scaling throughdynamic pruning. Our method features\na specializedjudge modeltrained withfocal lossandoversampling techniquesto accurately predict answer equivalence from partial reasoning traces which\nrealizes 0.87AUROCon equivalence prediction, combined with an online greedy\nclustering algorithm that dynamically prunes redundant paths while preserving\nanswer diversity. Comprehensive evaluations across three challenging benchmarks\n(AIME 2024, AIME 2025, and GPQA) and multiple reasoning models demonstrate thatDeepPruneachieves remarkabletoken reductionby over 80% compared to\nconventional consensus sampling on most cases, while maintaining competitive\naccuracy within 3 percentage points. Our work establishes a new standard for\nefficientparallel reasoning, making high-performance reasoning more efficient.\nOur code and data are here: https://deepprune.github.io/ Large language models (LLMs) often generate multiple reasoning traces in parallel to improve answer reliability (e.g. majority voting). However, these traces frequently exhibitsevere inter-trace redundancy, leading to wasted computation and inflated inference costs. DeepPruneaddresses this by learning to identify and prune semantically redundant tracesbeforefull execution—enablingcost-effective parallel reasoningwhile preserving performance. More details can be found in ourwebsite   This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend ·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2541048",
    "title": "DeepPrune: Parallel Scaling without Inter-trace Redundancy",
    "authors": [],
    "publication_year": 2025,
    "github_url": "https://github.com/THU-KEG/DeepPrune",
    "huggingface_url": "https://huggingface.co/papers/2510.08483",
    "upvote": 24
  }
}