{
  "context": "WaltzRL, a multi-agent reinforcement learning framework, improves LLM safety and helpfulness by collaboratively training a conversation agent and a feedback agent, reducing unsafe responses and overrefusals. Harnessing the power ofLLMsrequires a delicate dance between being helpful\nand harmless. This creates a fundamental tension between two competing\nchallenges: vulnerability toadversarial attacksthat elicit unsafe content,\nand a tendency for overrefusal on benign but sensitive prompts. Current\napproaches often navigate this dance withsafeguard modelsthat completely\nreject any content that contains unsafe portions. This approach cuts the music\nentirely-it may exacerbate overrefusals and fails to provide nuanced guidance\nfor queries it refuses. To teach models a more coordinated choreography, we\npropose WaltzRL, a novelmulti-agent reinforcement learningframework that\nformulates safety alignment as a collaborative, positive-sum game. WaltzRL\njointly trains aconversation agentand afeedback agent, where the latter is\nincentivized to provide useful suggestions that improve the safety and\nhelpfulness of theconversation agent's responses. At the core of WaltzRL is aDynamic Improvement Reward(DIR) that evolves over time based on how well theconversation agentincorporates the feedback. At inference time, unsafe or\noverrefusing responses from theconversation agentare improved rather than\ndiscarded. Thefeedback agentis deployed together with theconversation agentand only engages adaptively when needed, preserving helpfulness and low latency\non safe queries. Our experiments, conducted across five diverse datasets,\ndemonstrate that WaltzRL significantly reduces both unsafe responses (e.g.,\nfrom 39.0% to 4.6% onWildJailbreak) and overrefusals (from 45.3% to 9.9% onOR-Bench) compared to various baselines. By enabling the conversation andfeedback agents to co-evolve and adaptively apply feedback, WaltzRL enhances\nLLM safety without degrading general capabilities, thereby advancing the Pareto\nfront between helpfulness and harmlessness. Harnessing the power of LLMs requires a delicate dance between being helpful and harmless. This creates a fundamental tension between two competing challenges: vulnerability to adversarial attacks that elicit unsafe content, and a tendency for overrefusal on benign but sensitive prompts. Current approaches often navigate this dance with safeguard models that completely reject any content that contains unsafe portions. This approach cuts the music entirely-it may exacerbate overrefusals and fails to provide nuanced guidance for queries it refuses. To teach models a more coordinated choreography, we propose WaltzRL, a novel multi-agent reinforcement learning framework that formulates safety alignment as a collaborative, positive-sum game. WaltzRL jointly trains a conversation agent and a feedback agent, where the latter is incentivized to provide useful suggestions that improve the safety and helpfulness of the conversation agent's responses. At the core of WaltzRL is a Dynamic Improvement Reward (DIR) that evolves over time based on how well the conversation agent incorporates the feedback. At inference time, unsafe or overrefusing responses from the conversation agent are improved rather than discarded. The feedback agent is deployed together with the conversation agent and only engages adaptively when needed, preserving helpfulness and low latency on safe queries. Our experiments, conducted across five diverse datasets, demonstrate that WaltzRL significantly reduces both unsafe responses (e.g., from 39.0% to 4.6% on WildJailbreak) and overrefusals (from 45.3% to 9.9% on OR-Bench) compared to various baselines. By enabling the conversation and feedback agents to co-evolve and adaptively apply feedback, WaltzRL enhances LLM safety without degrading general capabilities, thereby advancing the Pareto front between helpfulness and harmlessness. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2541026",
    "title": "The Alignment Waltz: Jointly Training Agents to Collaborate for Safety",
    "authors": [],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2510.08240",
    "upvote": 41
  }
}