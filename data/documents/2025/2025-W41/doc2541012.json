{
  "context": "MingTok, a continuous latent space visual tokenizer, unifies vision-language understanding and generation within an autoregressive framework, achieving state-of-the-art performance across both domains. Visual tokenizationremains a core challenge in unifying visual understanding\nand generation within theautoregressive paradigm. Existing methods typically\nemploy tokenizers indiscrete latent spacesto align with the tokens from large\nlanguage models, where thequantization errorscan limit semantic\nexpressiveness and degrade the capability of vision-language understanding. To\naddress this, we introduce MingTok, a new family of visual tokenizers with acontinuous latent space, for unified autoregressive generation and\nunderstanding. While understanding tasks favor discriminative high-dimensional\nfeatures, generation tasks prefer compact low-level codes. Thus, to reconcile\nthese competing demands, MingTok adopts athree-stage sequential architectureinvolvinglow-level encoding,semantic expansion, andvisual reconstruction.\nBuilt on top of it,Ming-UniVisioneliminates the need for task-specific visual\nrepresentations, and unifies diverse vision-language tasks under a single\nautoregrsssive prediction paradigm. By formulating both understanding and\ngeneration asnext-token predictionin a shared continuous space, it seamlessly\nsupports multi-round, in-context tasks such asiterative understanding,generation and editing. Empirically, we find that using a unified continuous\nvisual representation reconciles the competing requirements on the tokenizers\nby the understanding and generation tasks, thereby leading to state-of-the-art\nlevel performance across both domains. We hope our findings will facilitate\nunifiedvisual tokenizationin the continuous domain. Inference code and model\nweights are released to benefit community. Introducing Ming‑UniVision & MingTok — the first autoregressive model to natively unify vision understanding & generation in a continuous unified representation space. Code:https://github.com/inclusionAI/Ming-UniVisionBlog:https://inclusionai.github.io/blog/mingtok/Modelscope:https://www.modelscope.cn/models/inclusionAI/Ming-UniVision-16B-A3BHuggingface:https://huggingface.co/inclusionAI/Ming-UniVision-16B-A3B This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Multimodal Latent Language Modeling with Next-Token Diffusion(2024) ·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2541012",
    "title": "Ming-UniVision: Joint Image Understanding and Generation with a Unified\n  Continuous Tokenizer",
    "authors": [
      "Ziyuan Huang",
      "DanDan Zheng",
      "Cheng Zou",
      "Xiaolong Wang",
      "Kaixiang Ji",
      "Weilong Chai",
      "Libin Wang",
      "Yongjie Lv",
      "Qingpei Guo"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/inclusionAI/Ming-UniVision",
    "huggingface_url": "https://huggingface.co/papers/2510.06590",
    "upvote": 73
  }
}