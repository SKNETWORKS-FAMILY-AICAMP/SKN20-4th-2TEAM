{
  "context": "NewtonBench is a benchmark for scientific law discovery that addresses scalability, scientific relevance, and memorization resistance by using metaphysical shifts and interactive model discovery. Large language modelsare emerging as powerful tools for scientific law\ndiscovery, a foundational challenge in AI-driven science. However, existing\nbenchmarks for this task suffer from a fundamental methodological trilemma,\nforcing a trade-off between scientific relevance, scalability, and resistance\nto memorization. Furthermore, they oversimplify discovery as static function\nfitting, failing to capture the authentic scientific process of uncovering\nembedded laws through the interactiveexplorationof complex model systems. To\naddress these critical gaps, we introduceNewtonBench, a benchmark comprising\n324scientific law discoverytasks across 12 physics domains. Our design\nmitigates the evaluation trilemma by usingmetaphysical shifts- systematic\nalterations of canonical laws - to generate a vast suite of problems that are\nscalable, scientifically relevant, and memorization-resistant. Moreover, we\nelevate the evaluation from static function fitting to interactive model\ndiscovery, requiring agents to experimentally probe simulated complex systems\nto uncover hidden principles. Our extensive experiment reveals a clear but\nfragile capability for discovery in frontier LLMs: this ability degrades\nprecipitously with increasing system complexity and exhibits extreme\nsensitivity toobservational noise. Notably, we uncover a paradoxical effect of\ntool assistance: providing acode interpretercan hinder more capable models by\ninducing a premature shift fromexplorationtoexploitation, causing them to\nsatisfice on suboptimal solutions. These results demonstrate that robust,\ngeneralizable discovery in complex, interactive environments remains the core\nchallenge. By providing a scalable, robust, and scientifically authentic\ntestbed,NewtonBenchoffers a crucial tool for measuring true progress and\nguiding the development of next-generation AI agents capable of genuine\nscientific discovery. Code & Data:https://github.com/HKUST-KnowComp/NewtonBenchNewtonBench constructed a counterfactual physical law discovery task, requiring LLM (agents) to actively experiment and explore simulation system parameters to discover hidden physical laws. This task is more challenging and novel than fitting given data in Scientific Discovery. NewtonBench also integrates code agents, allowing LLM agents to use code interfaces to fit and explore. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2541042",
    "title": "NewtonBench: Benchmarking Generalizable Scientific Law Discovery in LLM\n  Agents",
    "authors": [
      "Tianshi Zheng",
      "Newt Hue-Nam K. Nguyen",
      "Zhaowei Wang",
      "Tianqing Fang"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/HKUST-KnowComp/NewtonBench",
    "huggingface_url": "https://huggingface.co/papers/2510.07172",
    "upvote": 28
  }
}