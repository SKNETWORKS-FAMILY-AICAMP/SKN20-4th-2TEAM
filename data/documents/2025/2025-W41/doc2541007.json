{
  "context": "AgentFlow, a trainable agentic framework with in-the-flow optimization, enhances reasoning in large language models by coordinating specialized modules and outperforms top baselines across various tasks. Outcome-drivenreinforcement learninghas advanced reasoning in large\nlanguage models (LLMs), but prevailingtool-augmented approachestrain a\nsingle,monolithic policythat interleaves thoughts and tool calls under full\ncontext; this scales poorly with long horizons and diverse tools and\ngeneralizes weakly to new scenarios.Agentic systemsoffer a promising\nalternative by decomposing work acrossspecialized modules, yet most remain\ntraining-free or rely on offline training decoupled from the live dynamics ofmulti-turn interaction. We introduce AgentFlow, a trainable, in-the-flow\nagentic framework that coordinates four modules (planner,executor,verifier,generator) through anevolving memoryand directly optimizes itsplannerinside\nthe multi-turn loop. To train on-policy in live environments, we proposeFlow-based Group Refined Policy Optimization(Flow-GRPO), which tackleslong-horizon,sparse-reward credit assignmentby converting multi-turn\noptimization into a sequence of tractablesingle-turn policy updates. It\nbroadcasts a single, verifiabletrajectory-level outcometo every turn to align\nlocalplannerdecisions with global success and stabilizes learning withgroup-normalized advantages. Across ten benchmarks, AgentFlow with a 7B-scale\nbackbone outperforms top-performing baselines with average accuracy gains of\n14.9% on search, 14.0% on agentic, 14.5% on mathematical, and 4.1% onscientific tasks, even surpassing larger proprietary models likeGPT-4o.\nFurther analyses confirm the benefits of in-the-flow optimization, showing\nimproved planning, enhanced tool-calling reliability, and positive scaling with\nmodel size and reasoning turns. üß©A team of four specialized agents coordinates via shared memory and toolkit: üí°The magic: üåÄüí´  AgentFlow directly optimizes its Planner agentlive, inside the system, using our new method, Flow-GRPO (Flow-based Group Refined Policy Optimization). This is \"in-the-flow\" reinforcement learning. üìäThe result:AgentFlow (Qwen-2.5-7B-Instruct Backbone) outperforms top baselines on 10 benchmarks: üèÜEven surpasses larger-scale models like GPT-4o (~200B).  üåê¬†Website:https://agentflow.stanford.edu/üõ†Ô∏è Code:https://github.com/lupantech/ineqmathüöÄ Demo:https://huggingface.co/spaces/AgentFlow/agentflow This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Thanks for the great interest in our work! For those curious about the technical \"how\" behind #AgentFlow, here‚Äôs a look under the hood at the core methods. Instead of one giant model trying to do everything, AgentFlow uses a team of four specialized agents that collaborate via a shared memory: This modular design allows each agent to excel at its specific task.  How do you teach thePlannerto make good decisions at thebeginningof a 10-step task when the reward (a correct final answer) only comes at the very end? This is the classic credit assignment problem in reinforcement learning. Our solution is a new RL algorithm we callFlow-GRPO(Flow-based Group Refined Policy Optimization). üí°The Big Idea:We make learning direct and simple. Once the entire task is finished, we \"broadcast\" the final outcome (pass/fail) back toevery single decisionthe Planner made along the way. This \"in-the-flow\" optimization directly connects early actions to the final goal, making the training stable and highly effective.  So, what does this training actually teach the Planner? Let's see it in action. This is the key result: AgentFlow learns toself-correctand find creative solutions when its initial plan fails, a critical skill for robust reasoning.  We're excited about this direction for building more capable and reliable agents and are looking forward to working with the community to push these ideas further! Connect us: ¬∑Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2541007",
    "title": "In-the-Flow Agentic System Optimization for Effective Planning and Tool\n  Use",
    "authors": [
      "Zhuofeng Li",
      "Yu Zhang",
      "Pan Lu"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/lupantech/AgentFlow",
    "huggingface_url": "https://huggingface.co/papers/2510.05592",
    "upvote": 106
  }
}