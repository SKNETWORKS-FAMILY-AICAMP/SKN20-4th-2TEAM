{
  "context": "Introducing perturbations in instruction-tuning data can enhance large language models' resistance to noisy instructions and improve performance on benchmarks. Instruction-tuningplays a vital role in enhancing the task-solving abilities\noflarge language models(LLMs), improving their usability in generating\nhelpful responses on various tasks. However, previous work has demonstrated\nthat they are sensitive to minor variations in instruction phrasing. In this\npaper, we explore whether introducingperturbationsininstruction-tuningdata\ncan enhanceLLMs' resistance against noisy instructions. We focus on howinstruction-tuningwithperturbations, such as removingstop wordsor shuffling\nwords, affectsLLMs' performance on the original and perturbed versions of\nwidely-used benchmarks (MMLU,BBH,GSM8K). We further assesslearning dynamicsand potential shifts inmodel behavior. Surprisingly, our results suggest thatinstruction-tuningon perturbed instructions can, in some cases, improve\ndownstream performance. These findings highlight the importance of including\nperturbed instructions ininstruction-tuning, which can makeLLMsmore\nresilient tonoisy user inputs. Instruction-tuning is crucial for enhancing large language models’ (LLMs) ability to follow tasks and generate useful responses. Yet, prior work shows that LLMs remain sensitive to small variations in instruction phrasing. This paper investigates whether introducing perturbations during instruction-tuning can improve robustness to noisy inputs. We apply perturbations such as stop-word removal and word shuffling and evaluate performance on original and perturbed versions of MMLU, BBH, and GSM8K. Our results show that tuning on perturbed instructions can, in some cases, enhance downstream performance and stability. These findings suggest that incorporating controlled noise in instruction-tuning may yield more resilient and adaptable LLMs. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend ·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2541075",
    "title": "Fine-Tuning on Noisy Instructions: Effects on Generalization and\n  Performance",
    "authors": [
      "Ahmed Alajrami",
      "Xingwei Tan"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/aajrami/finetuning-on-noisy-instructions",
    "huggingface_url": "https://huggingface.co/papers/2510.03528",
    "upvote": 17
  }
}