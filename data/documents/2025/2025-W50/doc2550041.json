{
  "context": "Derf, a novel point-wise normalization function, outperforms existing alternatives across various domains, enhancing generalization without increased fitting capacity. Although normalization layers have long been viewed as indispensable components of deep learning architectures, the recent introduction ofDynamic Tanh(DyT) has demonstrated that alternatives are possible. The point-wise function DyT constrains extreme values for stable convergence and reaches normalization-level performance; this work seeks further for function designs that can surpass it. We first study how theintrinsic propertiesofpoint-wise functionsinfluence training and performance. Building on these findings, we conduct alarge-scale searchfor a more effective function design. Through this exploration, we introduceDerf(x) = erf(αx + s), where erf(x) is therescaled Gaussian cumulative distribution function, and identify it as the most performant design.DerfoutperformsLayerNorm,RMSNorm, and DyT across a wide range of domains, includingvision(image recognitionand generation),speech representation, andDNA sequence modeling. Our findings suggest that the performance gains ofDerflargely stem from its improved generalization rather than stronger fitting capacity. Its simplicity and stronger performance makeDerfa practical choice fornormalization-free Transformer architectures. Although normalization layers have long been viewed as indispensable components of deep learning architectures, the recent introduction of Dynamic Tanh (DyT) has demonstrated that alternatives are possible. The point-wise function DyT constrains extreme values for stable convergence and reaches normalization-level performance; this work seeks further for function designs that can surpass it. We first study how the intrinsic properties of point-wise functions influence training and performance. Building on these findings, we conduct a large-scale search for a more effective function design. Through this exploration, we introduce Derf(x)=erf(αx+s), where erf(x) is the rescaled Gaussian cumulative distribution function, and identify it as the most performant design. Derf outperforms LayerNorm, RMSNorm, and DyT across a wide range of domains, including vision (image recognition and generation), speech representation, and DNA sequence modeling. Our findings suggest that the performance gains of Derf largely stem from its improved generalization rather than stronger fitting capacity. Its simplicity and stronger performance make Derf a practical choice for normalization-free Transformer architectures. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend How does performance (as in speed, wall-clock) compare to DyT / RMSNorm? ·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2550041",
    "title": "Stronger Normalization-Free Transformers",
    "authors": [
      "Mingzhi Chen",
      "Taiming Lu"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/zlab-princeton/Derf",
    "huggingface_url": "https://huggingface.co/papers/2512.10938",
    "upvote": 19
  }
}