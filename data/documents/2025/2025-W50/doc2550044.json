{
  "context": "InfiniteVL, a linear-complexity VLM architecture combining sliding window attention and Gated DeltaNet, achieves competitive performance with less data and faster inference than leading Transformer-based models. Window attentionandlinear attentionrepresent two principal strategies for mitigating the quadratic complexity and ever-growing KV cache inVision-Language Models(VLMs). However, we observe that window-basedVLMssuffer performance degradation whensequence lengthexceeds thewindow size, whilelinear attentionunderperforms on information-intensive tasks such as OCR and document understanding. To overcome these limitations, we propose InfiniteVL, a linear-complexity VLM architecture that synergizessliding window attention(SWA) withGated DeltaNet. For achieving competitivemultimodal performanceunder constrained resources, we design a three-stage training strategy comprisingdistillation pretraining,instruction tuning, andlong-sequence SFT. Remarkably, using less than 2\\% of the training data required by leadingVLMs, InfiniteVL not only substantially outperforms previous linear-complexityVLMsbut also matches the performance of leading Transformer-basedVLMs, while demonstrating effectivelong-term memory retention. Compared to similar-sized Transformer-basedVLMsaccelerated byFlashAttention-2, InfiniteVL achieves over 3.6\\timesinference speedupwhile maintaining constant latency and memory footprint. In streaming video understanding scenarios, it sustains a stable 24 FPSreal-time prefill speedwhile preserving long-term memory cache. Code and models are available at https://github.com/hustvl/InfiniteVL. Window attention and linear attention represent two principal strategies for mitigating the quadratic complexity and ever-growing KV cache in Vision-Language Models (VLMs). However, we observe that window-based VLMs suffer performance degradation when sequence length exceeds the window size, while linear attention underperforms on information-intensive tasks such as OCR and document understanding. To overcome these limitations, we propose InfiniteVL, a linear-complexity VLM architecture that synergizes sliding window attention (SWA) with Gated DeltaNet. For achieving competitive multimodal performance under constrained resources, we design a three-stage training strategy comprising distillation pretraining, instruction tuning, and long-sequence SFT. Remarkably, using less than 2% of the training data required by leading VLMs, InfiniteVL not only substantially outperforms previous linear-complexity VLMs but also matches the performance of leading Transformer-based VLMs, while demonstrating effective long-term memory retention. Compared to similar-sized Transformer-based VLMs accelerated by FlashAttention-2, InfiniteVL achieves over 3.6\\times inference speedup while maintaining constant latency and memory footprint. In streaming video understanding scenarios, it sustains a stable 24 FPS real-time prefill speed while preserving long-term memory cache. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Nice work! Send me a lightweight model of that (8gb vram) STAT! =] <3 Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2550044",
    "title": "InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models",
    "authors": [
      "Hongyuan Tao",
      "Bencheng Liao",
      "Wenyu Liu",
      "Xinggang Wang"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/hustvl/InfiniteVL",
    "huggingface_url": "https://huggingface.co/papers/2512.08829",
    "upvote": 18
  }
}