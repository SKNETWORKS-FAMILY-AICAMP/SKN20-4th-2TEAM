{
  "context": "A three-stage framework, SPARK, uses a generator and verifier to create synthetic training data for process reward models, enabling reference-free reinforcement learning that surpasses ground-truth methods in mathematical reasoning tasks. Process reward models(PRMs) that provide dense, step-level feedback have shown promise forreinforcement learning, yet their adoption remains limited by the need for expensive step-level annotations or ground truth references. We propose SPARK: a three-stage framework where in the first stage agenerator modelproduces diverse solutions and averifier modelevaluates them usingparallel scaling(self-consistency) andsequential scaling(meta-critique). In the second stage, we use these verification outputs as synthetic training data to fine-tunegenerative process reward models, which subsequently serve as reward signals during training. We show that aggregating multiple independent verifications at the step level produces training data forprocess reward modelsthat surpass ground-truth outcome supervision, achieving 67.5 F1 onProcessBench(a benchmark for identifying erroneous steps in mathematical reasoning) compared to 66.4 for reference-guided training and 61.9 for GPT-4o. In the final stage, we apply our generative PRM withchain-of-thought verification(PRM-CoT) as the reward model inRL experimentson mathematical reasoning, and introduce format constraints to preventreward hacking. UsingQwen2.5-Math-7B, we achieve 47.4% average accuracy across six mathematical reasoning benchmarks, outperforming ground-truth-basedRLVR(43.9%). Our work enablesreference-free RL trainingthat exceeds ground-truth methods, opening new possibilities for domains lacking verifiable answers or accessible ground truth. Please find our paper on training process reward models without ground truth by leveraging inference-time scaling methods, enabling reinforcement learning in domains where verifiable answers are unavailable. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2550053",
    "title": "SPARK: Stepwise Process-Aware Rewards for Reference-Free Reinforcement Learning",
    "authors": [],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2512.03244",
    "upvote": 16
  }
}