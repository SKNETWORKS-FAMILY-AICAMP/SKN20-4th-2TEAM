{
  "context": "sCoT, a language-only CoT paradigm with self-calling subagents, enhances visual reasoning performance and efficiency through group-relative policy optimization. Thinking-with-images paradigms have showcased remarkablevisual reasoningcapability by integrating visual information as dynamic elements into theChain-of-Thought(CoT). However, optimizinginterleaved multimodal CoT(iMCoT) throughreinforcement learningremains challenging, as it relies on scarce high-quality reasoning data. In this study, we propose Self-CallingChain-of-Thought(sCoT), a novelvisual reasoningparadigm that reformulatesiMCoTas a language-onlyCoTwith self-calling. Specifically, a main agent decomposes the complexvisual reasoningtask to atomic subtasks and invokes its virtual replicas, i.e.parameter-sharing subagents, to solve them in isolated context. sCoTenjoys substantial training effectiveness and efficiency, as it requires no explicit interleaving between modalities. sCoTemploysgroup-relative policy optimizationto reinforce effective reasoning behavior to enhance optimization. Experiments onHR-Bench 4Kshow that sCoTimproves the overall reasoning performance by up to 1.9% with sim 75% fewer GPU hours compared to strong baseline approaches. Code is available at https://github.com/YWenxi/think-with-images-through-self-calling. üß†üñºÔ∏èVision-language models are getting smarter‚Äîbut also harder to train.Many recent systems ‚Äúthink with images,‚Äù weaving visual information directly into their reasoning. While powerful, this approach can be hard to incentivize, as it usually requires LLMs to reason across modalites. ‚ú® This paper introducesthinking-with-images-through-self-calling (sCoT)-- a simpler idea:let the model think in language, break problems into atomic steps, and call itself to solve them. Instead of mixing text and images throughout its reasoning, a main agent splits a visual problem into small pieces‚Äîlike reading text or spotting an object‚Äîand delegates them to lightweightsubagentsü§ñ. These subagents are virtual copies of the same model that answer one focused visual question and return a short text response. The main agent then combines everything through pure language reasoning. üöÄThe result? Easier training and stronger performance.The sCoT-based model trained with end-to-end RL, named asSubagentVL, outperforms previous state-of-the-art methods on challenging high-resolution benchmarks (V* and HR-Bench) with less GPU hours.  üëâBottom line:smarter visual reasoning doesn‚Äôt require more complex multimodal thinking‚Äîletting models reason in language and ask for help from its virtual replicas. Code is available atgithub repo.Paper is available atarxiv This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend ¬∑Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2550036",
    "title": "Thinking with Images via Self-Calling Agent",
    "authors": [
      "Wenxi Yang",
      "Yuzhong Zhao"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/YWenxi/think-with-images-through-self-calling",
    "huggingface_url": "https://huggingface.co/papers/2512.08511",
    "upvote": 21
  }
}