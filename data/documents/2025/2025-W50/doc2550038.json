{
  "context": "FAE, a framework using a feature auto-encoder and dual decoders, adapts pre-trained visual representations for generative models, achieving high performance in image generation tasks. Visual generative models (e.g.,diffusion models) typically operate in compressedlatent spacesto balance training efficiency and sample quality. In parallel, there has been growing interest in leveraging high-qualitypre-trained visual representations, either by aligning them insideVAEsor directly within the generative model. However, adapting such representations remains challenging due to fundamental mismatches between understanding-oriented features and generation-friendlylatent spaces.Representation encodersbenefit fromhigh-dimensional latentsthat capture diverse hypotheses for masked regions, whereas generative models favorlow-dimensional latentsthat must faithfully preserve injected noise. This discrepancy has led prior work to rely on complex objectives and architectures. In this work, we propose FAE (Feature Auto-Encoder), a simple yet effective framework that adaptspre-trained visual representationsintolow-dimensional latentssuitable for generation using as little as a single attention layer, while retaining sufficient information for both reconstruction and understanding. The key is to couple two separatedeep decoders: one trained to reconstruct the original feature space, and a second that takes the reconstructed features as input forimage generation. FAE is generic; it can be instantiated with a variety ofself-supervised encoders(e.g.,DINO,SigLIP) and plugged into two distinct generative families:diffusion modelsandnormalizing flows. Across class-conditional andtext-to-image benchmarks, FAE achieves strong performance. For example, onImageNet256x256, our diffusion model withCFGattains a near state-of-the-artFIDof 1.29 (800 epochs) and 1.70 (80 epochs). WithoutCFG, FAE reaches the state-of-the-artFIDof 1.48 (800 epochs) and 2.08 (80 epochs), demonstrating both high quality and fast learning. We proposed FAE which adapts pretrained ViT as the latent space for visual generative models This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2550038",
    "title": "One Layer Is Enough: Adapting Pretrained Visual Encoders for Image Generation",
    "authors": [
      "Yuan Gao",
      "Jiatao Gu"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2512.07829",
    "upvote": 21
  }
}