{
  "context": "Vision-Language models fine-tuned on anonymized image captions can capture relational similarity between images, a capability lacking in current visual similarity metrics. Humans do not just see attribute similarity -- we also seerelational similarity. An apple is like a peach because both are reddish fruit, but the Earth is also like a peach: its crust, mantle, and core correspond to the peach's skin, flesh, and pit. This ability to perceive and recognizerelational similarity, is arguable by cognitive scientist to be what distinguishes humans from other species. Yet, all widely used visual similarity metrics today (e.g.,LPIPS,CLIP,DINO) focus solely onperceptual attribute similarityand fail to capture the rich, often surprising relational similarities that humans perceive. How can we go beyond the visible content of an image to capture its relational properties? How can we bring images with the samerelational logiccloser together inrepresentation space? To answer these questions, we first formulate relational image similarity as a measurable problem: two images are relationally similar when their internal relations or functions among visual elements correspond, even if their visual attributes differ. We then curate 114kimage-caption datasetin which the captions are anonymized -- describing the underlyingrelational logicof the scene rather than its surface content. Using this dataset, we finetune aVision-Language modelto measure therelational similaritybetween images. This model serves as the first step toward connecting images by their underlying relational structure rather than their visible appearance. Our study shows that whilerelational similarityhas a lot of real-world applications, existing image similarity models fail to capture it -- revealing a critical gap in visual computing. Humans do not just see attribute similarity -- we also see relational similarity. An apple is like a peach because both are reddish fruit, but the Earth is also like a peach: its crust, mantle, and core correspond to the peach's skin, flesh, and pit. This ability to perceive and recognize relational similarity, is arguable by cognitive scientist to be what distinguishes humans from other species. Yet, all widely used visual similarity metrics today (e.g., LPIPS, CLIP, DINO) focus solely on perceptual attribute similarity and fail to capture the rich, often surprising relational similarities that humans perceive. How can we go beyond the visible content of an image to capture its relational properties? How can we bring images with the same relational logic closer together in representation space? To answer these questions, we first formulate relational image similarity as a measurable problem: two images are relationally similar when their internal relations or functions among visual elements correspond, even if their visual attributes differ. We then curate 114k image-caption dataset in which the captions are anonymized -- describing the underlying relational logic of the scene rather than its surface content. Using this dataset, we finetune a Vision-Language model to measure the relational similarity between images. This model serves as the first step toward connecting images by their underlying relational structure rather than their visible appearance. Our study shows that while relational similarity has a lot of real-world applications, existing image similarity models fail to capture it -- revealing a critical gap in visual computing. Live image retrieval results:https://thaoshibe.github.io/relsim/retrieve/index.html(uncurated)Or try the metric ;) pip install relsim  This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2550033",
    "title": "Relational Visual Similarity",
    "authors": [
      "Thao Nguyen"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/thaoshibe/relsim",
    "huggingface_url": "https://huggingface.co/papers/2512.07833",
    "upvote": 24
  }
}