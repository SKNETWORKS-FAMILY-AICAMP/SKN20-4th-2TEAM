{
  "context": "The Outcome-based Process Verifier (OPV) improves the verification of complex reasoning chains in large language models by combining outcome-based and process-based verification with iterative active learning and Rejection Fine-Tuning, achieving state-of-the-art performance on various benchmarks. Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks byReinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliableverifiers. However, currentoutcome-based verifiers (OVs)are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, currentprocess-based verifiers (PVs)have difficulties in reliably detecting errors in the complex longCoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from longCoTsto achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt aniterative active learningframework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV throughRejection Fine-Tuning (RFT)and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-outOPV-Bench, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy ofDeepSeek-R1-Distill-Qwen-32Bfrom 55.2% to 73.3% onAIME2025as the compute budget scales. We propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2550019",
    "title": "OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification",
    "authors": [
      "Yuzhe Gu"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2512.10756",
    "upvote": 34
  }
}