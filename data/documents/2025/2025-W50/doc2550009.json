{
  "context": "OPV, an iterative active learning framework with Rejection Fine-Tuning, enhances verification of long reasoning chains in large language models, achieving state-of-the-art results and improving accuracy in collaborative tasks. Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks byReinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, currentoutcome-based verifiers (OVs)are unable to inspect the unreliable intermediate steps in thelong reasoning chains of thought (CoTs). Meanwhile, currentprocess-based verifiers (PVs)have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt aniterative active learningframework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV throughRejection Fine-Tuning (RFT)and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out \\thisbench, outperforming much larger open-source models such as Qwen3-Max-Preview with anF1 scoreof 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising theaccuracyofDeepSeek-R1-Distill-Qwen-32Bfrom 55.2\\% to 73.3\\% onAIME2025as the compute budget scales. Due to a user error, the abstract displayed in this paper contains some errors ðŸ˜­ (the abstract in the PDF is correct).The correct and complete abstract is as follows: Large Reasoning Models (LRMs) have expanded the mathematical reasoning frontier through Chain-of-Thought (CoT) techniques and Reinforcement Learning with Verifiable Rewards (RLVR), capable of solving AIME-level problems. However, the performance of LRMs is heavily dependent on the extended reasoning context length. For solving ultra-hard problems like those in the International Mathematical Olympiad (IMO), the required reasoning complexity surpasses the space that an LRM can explore in a single round. Previous works attempt to extend the reasoning context of LRMs but remain prompt-based and built upon proprietary models, lacking systematic structures and training pipelines. Therefore, this paper introduces Intern-S1-MO, a long-horizon math agent that conducts multi-round hierarchical reasoning, composed of an LRM-based multi-agent system including reasoning, summary, and verification. By maintaining a compact memory in the form of lemmas, Intern-S1-MO can more freely explore the lemma-rich reasoning spaces in multiple reasoning stages, thereby breaking through the context constraints for IMO-level math problems. Furthermore, we propose OREAL-H, an RL framework for training the LRM using the online explored trajectories to simultaneously bootstrap the reasoning ability of LRM and elevate the overall performance of Intern-S1-MO.  Experiments show that Intern-S1-MO can obtain 26 out of 35 points on the non-geometry problems of IMO2025, matching the performance of silver medalists. It also surpasses the current advanced LRMs on inference benchmarks such as HMMT2025, AIME2025, and CNMO2025. In addition, our agent officially participates in CMO2025 and achieves a score of 102/126 under the judgment of human experts, reaching the gold medal level. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend ðŸŽ‰ Congratulations on this great work!I noticed that you used an example fromAMO-Benchto illustrate the recursive subproblem-solving process, but the citation was accidentally omitted. Could you please add it? Additionally, would you be interested in evaluating your model's performance on the entire AMO-Bench dataset? Thank you for your attention. We will add references and discussions to AMO-bench in the next version. Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2550009",
    "title": "Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving",
    "authors": [
      "Yuzhe Gu",
      "Wenwei Zhang",
      "Tianyou Ma",
      "Junhao Shen",
      "Duanyang Zhang",
      "Kuikun Liu",
      "Dahua Lin"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2512.10739",
    "upvote": 46
  }
}