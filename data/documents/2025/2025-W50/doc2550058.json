{
  "context": "A unified multimodal large language model (MLLM) that integrates depth and segmentation modalities enhances spatial reasoning and perception through adaptive interleaved reasoning, improving spatial intelligence and general performance. Visual Spatial Reasoningis crucial for enablingMultimodal Large Language Models (MLLMs)to understand object properties and spatial relationships, yet current models still struggle with3D-aware reasoning. Existing approaches typically enhance either perception, by augmenting RGB inputs withauxiliary modalitiessuch asdepthandsegmentation, or reasoning, by training onspatial VQA datasetsand applyingreinforcement learning, and thus treat these two aspects in isolation. In this work, we investigate whether a unified MLLM can develop an intrinsic ability to enhancespatial perceptionand, throughadaptive interleaved reasoning, achieve strongerspatial intelligence. We propose COOPER, a unified MLLM that leveragesdepthandsegmentationasauxiliary modalitiesand is trained in two stages to acquire auxiliary modality generation and adaptive, interleaved reasoning capabilities. COOPER achieves an average 6.91\\% improvement in spatial reasoning while maintaining general performance. Moreover, even a variant trained only for auxiliary modality generation attains a 7.92\\% gain on distance andsize estimation, suggesting that learning to generateauxiliary modalitieshelps internalize spatial knowledge and strengthen spatial understanding. Visual Spatial Reasoning is crucial for enabling Multimodal Large Language Models (MLLMs) to understand object properties and spatial relationships, yet current models still struggle with 3D-aware reasoning. Existing approaches typically enhance either perception, by augmenting RGB inputs with auxiliary modalities such as depth and segmentation, or reasoning, by training on spatial VQA datasets and applying reinforcement learning, and thus treat these two aspects in isolation. In this work, we investigate whether a unified MLLM can develop an intrinsic ability to enhance spatial perception and, through adaptive interleaved reasoning, achieve stronger spatial intelligence.We propose \\textbf{COOPER}, a unified MLLM that leverages depth and segmentation as auxiliary modalities and is trained in two stages to acquire auxiliary modality generation and adaptive, interleaved reasoning capabilities. COOPER achieves an average \\textbf{6.91%} improvement in spatial reasoning while maintaining general performance. Moreover, even a variant trained only for auxiliary modality generation attains a \\textbf{7.92%} gain on distance and size estimation, suggesting that learning to generate auxiliary modalities helps internalize spatial knowledge and strengthen spatial understanding. ðŸ§  GRPO Training for BAGEL via TRL:Fine-tune BAGEL-style multimodal models with RL-style objectives.Optimize perceptionâ€“reasoning behavior directly from feedback signals.Seamlessly extend from supervised multimodal CoT training to RL-based refinement. ðŸ“Š VLMEvalKit Integration for BAGEL:One-line evaluation on a wide range of multimodal benchmarks.Unified interfaces for dataset loading, inference, and result aggregation.Direct comparison with other VLMs under consistent evaluation protocols. ðŸ§© SIBench (Single-Image Part) + GPT/Deepseek Answer Extraction:Fully integrated into VLMEvalKit as a first-class evaluation task.Equipped with GPT/Deepseek-based answer extractors to: Robustly parse free-form model outputs. Reduce evaluation noise from formatting and phrasing. Provide more accurate and reliable spatial reasoning scores. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2550058",
    "title": "COOPER: A Unified Model for Cooperative Perception and Reasoning in Spatial Intelligence",
    "authors": [
      "Zefeng Zhang",
      "Xiangzhao Hao"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/zhangzef/COOPER",
    "huggingface_url": "https://huggingface.co/papers/2512.04563",
    "upvote": 14
  }
}