{
  "context": "CAPO, a curriculum advantage policy optimization, enhances reinforcement learning for large language models by strategically introducing positive and negative advantage signals, improving reasoning capabilities and generalization. Reinforcement learninghas emerged as a paradigm forpost-traininglarge language models, boosting theirreasoning capabilities. Such approaches compute anadvantage valuefor each sample, reflecting better or worse performance than expected, thereby yielding both positive andnegative signalsfor training. However, the indiscriminate mixing of the two signals in existing methods, especially from the early stages, may lead to ambiguous guidance and limited gains. To address this issue, we propose **CAPO** (**C**urriculum **A**dvantage **P**olicy **O**ptimization), an adaptivecurriculum mechanismbased on advantage signals. The proposed mechanism bootstrapsimitation learningwith positive-only advantage samples to establish robust foundations, and subsequently introducesnegative signalsto cultivatediscriminative capabilities, thereby improvinggeneralizationacross complex scenarios. Compatible with diverse optimization methods includingGRPO,PPO,RLOO, andReinforce++, our method consistently achieves stable and significant improvements inmathematical reasoning tasks, and further generalizes effectively tomultimodal Graphical User Interface (GUI) reasoning scenarios, establishing itself as a versatile and robustoptimization framework. üöÄ[New Paper] CAPO: From Imitation to Discrimination ‚Äì Rethinking Advantage in RL Early RL training often suffers from instability due to \"mixed signals\" (simultaneous positive & negative feedback). Inspired by child cognitive development, we proposeCAPO (Curriculum Advantage Policy Optimization). ‚ú®The Core Intuition:Instead of a static curriculum, we leverageAdvantagevalues to create a dynamic, two-phase process:1Ô∏è‚É£Imitation Phase:Train onPositive Advantageonly. This reduces variance and establishes a stable behavioral foundation (Imitate to learn).2Ô∏è‚É£Discrimination Phase:IntroduceNegative Signalslater. [cite_start]This restores unbiased estimation and refines decision boundaries (Discriminate to generalize). üìàHighlights: This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend ¬∑Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2550028",
    "title": "From Imitation to Discrimination: Toward A Generalized Curriculum Advantage Mechanism Enhancing Cross-Domain Reasoning Tasks",
    "authors": [],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2512.02580",
    "upvote": 27
  }
}