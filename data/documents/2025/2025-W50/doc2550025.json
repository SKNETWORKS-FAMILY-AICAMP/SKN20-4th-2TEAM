{
  "context": "EgoEdit is a real-time, instruction-following egocentric video editor that addresses challenges in handling egomotion and hand-object interactions, outperforming existing methods on egocentric editing tasks. We study instruction-guided editing of egocentric videos for interactive AR applications. While recent AI video editors perform well on third-person footage, egocentric views present unique challenges - including rapidegomotionand frequenthand-object interactions- that create a significant domain gap. Moreover, existing offline editing pipelines suffer from high latency, limiting real-time interaction. To address these issues, we present a complete ecosystem foregocentric video editing. First, we constructEgoEditData, a carefully designed and manually curated dataset specifically designed for egocentric editing scenarios, featuring richhand-object interactions, while explicitly preserving hands. Second, we developEgoEdit, aninstruction-followingegocentric video editor that supportsreal-time streaming inferenceon a single GPU. Finally, we introduceEgoEditBench, an evaluation suite targetinginstruction faithfulness, hand andinteraction preservation, andtemporal stabilityunderegomotion. Across both egocentric and general editing tasks,EgoEditproduces temporally stable, instruction-faithful results with interactive latency. It achieves clear gains on egocentric editing benchmarks-where existing methods struggle-while maintaining performance comparable to the strongest baselines on general editing tasks.EgoEditDataandEgoEditBenchwill be made public for the research community. See our website at https://snap-research.github.io/EgoEdit We propose a framework for real-time egocentric video editing. Our system is composed of: EgoEditData, a manually curated dataset of 100k video editing pairs focusing on the egocentric case and featuring object substitution and removal under challenging hand occlusions, interactions, and large egomotion; EgoEdit the first real-time autoregressive model for egocentric video editing running in real time on a single H100 with 855ms first-frame latency and enabling live augmented reality (AR) interactions; EgoEditBench, a comprehensive benchmark for evaluation of egocentric video editing systems. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2550025",
    "title": "EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing",
    "authors": [
      "Runjia Li"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/snap-research/EgoEdit",
    "huggingface_url": "https://huggingface.co/papers/2512.06065",
    "upvote": 28
  }
}