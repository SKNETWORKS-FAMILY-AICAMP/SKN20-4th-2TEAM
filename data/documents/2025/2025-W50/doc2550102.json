{
  "context": "A large-scale dataset and multimodal model improve embodied interaction comprehension in robots by addressing perspective bias and enhancing multimodal signal integration. As robots enter human workspaces, there is a crucial need for them to comprehend embodied human instructions, enabling intuitive and fluent human-robot interaction (HRI). However, accurate comprehension is challenging due to a lack of large-scale datasets that capture natural embodied interactions in diverse HRI settings. Existing datasets suffer from perspective bias, single-view collection, inadequate coverage of nonverbal gestures, and a predominant focus on indoor environments. To address these issues, we present theRefer360dataset, a large-scale dataset of embodied verbal and nonverbal interactions collected across diverse viewpoints in both indoor and outdoor settings. Additionally, we introduceMuRes, amultimodal guided residual moduledesigned to improveembodied referring expression comprehension.MuResacts as aninformation bottleneck, extractingsalient modality-specific signalsand reinforcing them intopre-trained representationsto form complementary features for downstream tasks. We conduct extensive experiments on fourHRI datasets, including theRefer360dataset, and demonstrate that current multimodal models fail to capture embodied interactions comprehensively; however, augmenting them withMuResconsistently improves performance. These findings establishRefer360as a valuable benchmark and exhibit the potential of guided residual learning to advanceembodied referring expression comprehensionin robots operating within human environments. The paper introducesRefer360, a comprehensive multimodal dataset for embodied referring expression comprehension in human-robot interaction (HRI), and proposesMuRes, a lightweight guided residual module that selectively reinforces modality-specific features to improve multimodal grounding performance in real-world scenarios. â¡ï¸ğŠğğ² ğ‡ğ¢ğ ğ¡ğ¥ğ¢ğ ğ¡ğ­ğ¬ ğ¨ğŸ ğ­ğ¡ğ ğ‘ğğŸğğ«ğŸ‘ğŸ”ğŸ ğğğ§ğœğ¡ğ¦ğšğ«ğ¤ + ğŒğ®ğ‘ğğ¬ ğŒğ¨ğğ®ğ¥ğ:ğŸ§ ğ‘¹ğ’†ğ’‡ğ’†ğ’“ğŸ‘ğŸ”ğŸ: ğ‘­ğ’Šğ’“ğ’”ğ’• ğ‘¬ğ’ğ’ƒğ’ğ’…ğ’Šğ’†ğ’… ğ‘¹ğ‘¬ ğ‘«ğ’‚ğ’•ğ’‚ğ’”ğ’†ğ’• ğ’˜ğ’Šğ’•ğ’‰ ğ‘´ğ’–ğ’ğ’•ğ’Š-ğ‘½ğ’Šğ’†ğ’˜, ğ‘´ğ’–ğ’ğ’•ğ’Š-ğ‘ºğ’†ğ’ğ’”ğ’ğ’“ ğ‘´ğ’ğ’…ğ’‚ğ’ğ’Šğ’•ğ’Šğ’†ğ’”: Introduces a dataset with synchronizedegocentric and exocentric views,RGB, depth, infrared, 3D skeleton,eye gaze, andaudio, acrossindoor and outdoorenvironments. With 13,990 annotated interactions (3.2M frames), it overcomes biases in existing datasets (e.g., single view, indoor-only, no gesture/gaze integration).ğŸ”ğ‘´ğ’–ğ‘¹ğ’†ğ’”: ğ‘®ğ’–ğ’Šğ’…ğ’†ğ’… ğ‘¹ğ’†ğ’”ğ’Šğ’…ğ’–ğ’‚ğ’ ğ‘©ğ’ğ’•ğ’•ğ’ğ’†ğ’ğ’†ğ’„ğ’Œ ğ’‡ğ’ğ’“ ğ‘´ğ’–ğ’ğ’•ğ’Šğ’ğ’ğ’…ğ’‚ğ’ ğ‘­ğ’–ğ’”ğ’Šğ’ğ’: Proposes a novel residual architecture that usescross-attentionto guide modality-specific signals (visual/language) through aninformation bottleneck, preventing feature dilution during fusion and outperforming both vanilla residuals and attention-only fusion across 4 datasets.ğŸ“ˆğ‘ºğ’Šğ’ˆğ’ğ’Šğ’‡ğ’Šğ’„ğ’‚ğ’ğ’• ğ‘®ğ’‚ğ’Šğ’ğ’” ğ’‚ğ’„ğ’“ğ’ğ’”ğ’” ğ‘¯ğ‘¹ğ‘° ğ’‚ğ’ğ’… ğ‘½ğ‘¸ğ‘¨ ğ‘»ğ’‚ğ’”ğ’Œğ’”: On Refer360, integrating MuRes into CLIP improved IOU-25 by+3.4%, and on CAESAR-PRO by+4.99%. For broader VQA tasks like ScienceQA and A-OKVQA, MuRes boosted model accuracy by up to+30%, highlighting its generalization ability across task domains. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2550102",
    "title": "Embodied Referring Expression Comprehension in Human-Robot Interaction",
    "authors": [
      "Md Mofijul Islam",
      "Alexi Gladstone",
      "Sujan Sarker",
      "Aman Chadha"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2512.06558",
    "upvote": 3
  }
}