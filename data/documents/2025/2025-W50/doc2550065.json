{
  "context": "C3 is an uncertainty quantification method for training controllable video models that provides dense confidence estimation and out-of-distribution detection, addressing hallucination issues. Recent advances ingenerative video modelshave led to significant breakthroughs in high-fidelity video synthesis, specifically in controllable video generation where the generated video is conditioned on text and action inputs, e.g., ininstruction-guided video editingand world modeling in robotics. Despite these exceptional capabilities,controllable video modelsoften hallucinate - generating future video frames that are misaligned with physical reality - which raises serious concerns in many tasks such asrobot policy evaluationandplanning. However, state-of-the-art video models lack the ability to assess and express their confidence, impeding hallucination mitigation. To rigorously address this challenge, we propose C3, anuncertainty quantification(UQ) method for trainingcontinuous-scale calibratedcontrollable video modelsfordense confidence estimationat thesubpatch level, precisely localizing the uncertainty in each generated video frame. OurUQmethod introduces three core innovations to empower video models to estimate their uncertainty. First, our method develops a novel framework that trains video models for correctness and calibration via strictly proper scoring rules. Second, we estimate the video model's uncertainty inlatent space, avoiding training instability and prohibitive training costs associated withpixel-spaceapproaches. Third, we map the dense latent-space uncertainty to interpretable pixel-level uncertainty in the RGB space for intuitive visualization, providing high-resolution uncertainty heatmaps that identify untrustworthy regions. Through extensive experiments on large-scale robot learning datasets (Bridge and DROID) and real-world evaluations, we demonstrate that our method not only providescalibrated uncertainty estimateswithin thetraining distribution, but also enables effectiveout-of-distribution detection. Recent advances in generative video models have led to significant breakthroughs in high-fidelity video synthesis, specifically in controllable video generation where the generated video is conditioned on text and action inputs, e.g., in instruction-guided video editing and world modeling in robotics. Despite these exceptional capabilities, controllable video models often hallucinate - generating future video frames that are misaligned with physical reality - which raises serious concerns in many tasks such as robot policy evaluation and planning. However, state-of-the-art video models lack the ability to assess and express their confidence, impeding hallucination mitigation. To rigorously address this challenge, we propose C3, an uncertainty quantification (UQ) method for training continuous-scale calibrated controllable video models for dense confidence estimation at the subpatch level, precisely localizing the uncertainty in each generated video frame. Our UQ method introduces three core innovations to empower video models to estimate their uncertainty. First, our method develops a novel framework that trains video models for correctness and calibration via strictly proper scoring rules. Second, we estimate the video model's uncertainty in latent space, avoiding training instability and prohibitive training costs associated with pixel-space approaches. Third, we map the dense latent-space uncertainty to interpretable pixel-level uncertainty in the RGB space for intuitive visualization, providing high-resolution uncertainty heatmaps that identify untrustworthy regions. Through extensive experiments on large-scale robot learning datasets (Bridge and DROID) and real-world evaluations, we demonstrate that our method not only provides calibrated uncertainty estimates within the training distribution, but also enables effective out-of-distribution detection. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2550065",
    "title": "World Models That Know When They Don't Know: Controllable Video Generation with Calibrated Uncertainty",
    "authors": [],
    "publication_year": 2025,
    "github_url": "https://github.com/irom-princeton/c-cubed",
    "huggingface_url": "https://huggingface.co/papers/2512.05927",
    "upvote": 11
  }
}