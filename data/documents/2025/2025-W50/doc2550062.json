{
  "context": "MIND-V generates long-horizon robotic manipulation videos by integrating semantic reasoning, domain-invariant representations, and physical plausibility through a hierarchical framework. Embodied imitation learning is constrained by the scarcity of diverse, long-horizon robotic manipulation data. Existing video generation models for this domain are limited to synthesizing short clips of simple actions and often rely on manually defined trajectories. To this end, we introduce MIND-V, a hierarchical framework designed to synthesize physically plausible and logically coherent videos of long-horizon robotic manipulation. Inspired by cognitive science, MIND-V bridges high-level reasoning with pixel-level synthesis through three core components: aSemantic Reasoning Hub(SRH) that leverages a pre-trained vision-language model for task planning; aBehavioral Semantic Bridge(BSB) that translates abstract instructions into domain-invariant representations; and aMotor Video Generator(MVG) for conditional video rendering. MIND-V employsStaged Visual Future Rollouts, a test-time optimization strategy to enhance long-horizon robustness. To align the generated videos with physical laws, we introduce aGRPO reinforcement learningpost-training phase guided by a novelPhysical Foresight Coherence(PFC) reward. PFC leverages theV-JEPA world modelto enforce physical plausibility by aligning the predicted and actual dynamic evolutions in the feature space. MIND-V demonstrates state-of-the-art performance in long-horizon robotic manipulation video generation, establishing a scalable and controllable paradigm for embodied data synthesis. We propose MIND-V, a hierarchical framework designed to synthesize physically plausible and logically coherent videos of long-horizon robotic manipulation.  This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2550062",
    "title": "MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment",
    "authors": [],
    "publication_year": 2025,
    "github_url": "https://github.com/Richard-Zhang-AI/MIND-V",
    "huggingface_url": "https://huggingface.co/papers/2512.06628",
    "upvote": 12
  }
}