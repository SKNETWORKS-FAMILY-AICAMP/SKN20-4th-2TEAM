{
  "context": "The Alignment Trilemma in RLHF shows that achieving representativeness, tractability, and robustness is computationally infeasible, leading to trade-offs in current implementations. Reinforcement Learning from Human Feedback (RLHF)is widely used for aligning large language models, yet practitioners face a persistent puzzle: improving safety often reduces fairness, scaling to diverse populations becomes computationally intractable, and making systems robust often amplifies majority biases. We formalize this tension as theAlignment Trilemma: no RLHF system can simultaneously achieve (i)epsilon-representativenessacross diverse human values, (ii)polynomial tractabilityin sample and compute complexity, and (iii)delta-robustnessagainst adversarial perturbations and distribution shift. Through acomplexity-theoretic analysisintegratingstatistical learning theoryandrobust optimization, we prove that achieving both representativeness (epsilon <= 0.01) and robustness (delta <= 0.001) for global-scale populations requires Omega(2^{d_context}) operations, which is super-polynomial in the context dimensionality. We show that current RLHF implementations resolve this trilemma by sacrificing representativeness: they collect only 10^3--10^4 samples from homogeneous annotator pools while 10^7--10^8 samples are needed for true global representation. Our framework provides a unified explanation for documented RLHF pathologies includingpreference collapse,sycophancy, andsystematic bias amplification. We conclude with concrete directions for navigating these fundamental trade-offs through strategic relaxations of alignment requirements. This paper formalizes an Alignment Trilemma, proving that no RLHF-based alignment strategy can simultaneously achieveÎµ\\varepsilonÎµ-representativeness, polynomial-time tractability, andÎ´\\deltaÎ´-robustness, with any two of these goals implying exponential cost in the third.  âž¡ï¸Key Highlights of the Alignment Trilemma Framework:ðŸ§ Formalization of Alignment Constraints: The paper rigorously definesÎµ\\varepsilonÎµ-representativeness (alignment fidelity across diverse human values), polynomial tractability (sample and compute complexity), andÎ´\\deltaÎ´-robustness (resilience to adversarial perturbations). It proves that satisfying all three simultaneously is impossible for large populations and high-dimensional context spaces, i.e., achieving both smallÎµ\\varepsilonÎµandÎ´\\deltaÎ´requiresÎ©(2dcontext)\\Omega(2^{d_{\\text{context}}})Î©(2dcontextâ€‹)operations. ðŸ“ˆComplexity-Theoretic Lower Bounds on Scalability: The authors show that alignment requires operations scaling asÎ©(Îºâ‹…2dcontext/(Îµ2nÎ´))\\Omega(\\kappa \\cdot 2^{d_{\\text{context}}} / (\\varepsilon^2 n \\delta))Î©(Îºâ‹…2dcontextâ€‹/(Îµ2nÎ´)), wheredcontextâ‰«logâ¡nd_{\\text{context}} \\gg \\log ndcontextâ€‹â‰«logn. This implies that as model context spaces or population diversity grow, alignment becomes super-polynomial in cost, rendering naive scaling approaches ineffective for global representational alignment. âš–ï¸Practical Trade-off Analysis in Current RLHF Pipelines: The study maps how existing RLHF systems navigate the trilemma: choosing small, homogeneous annotator pools (typically10310^3103â€“\\(10^4\\) samples) and strong KL penalties to maintain tractability and partial robustness, at the cost of representativeness. This design leads directly to known pathologies such as sycophancy, reward hacking, and collapse of minority valuesâ€”shown here to be inevitable outcomes of trilemma constraints. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2548097",
    "title": "Position: The Complexity of Perfect AI Alignment -- Formalizing the RLHF Trilemma",
    "authors": [
      "Aman Chadha"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2511.19504",
    "upvote": 2
  }
}