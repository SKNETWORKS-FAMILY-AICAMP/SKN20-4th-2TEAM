{
  "context": "LLMs exhibit reasoning gaps compared to humans, underutilizing cognitive elements and failing to deploy meta-cognitive controls, but test-time guidance can improve their performance on complex problems. Large language models (LLMs) solve complex problems yet fail on simpler variants, suggesting they achieve correct outputs through mechanisms fundamentally different from human reasoning. To understand this gap, we synthesize cognitive science research into a taxonomy of 28cognitive elementsspanningreasoning invariants,meta-cognitive controls,representationsfor organizing reasoning & knowledge, andtransformation operations. We introduce afine-grained evaluation frameworkand conduct the first large-scale empirical analysis of 192K traces from 18 models across text, vision, and audio, complemented by 54 humanthink-aloud traces, which we make publicly available. We find that models under-utilizecognitive elementscorrelated with success, narrowing to rigidsequential processingon ill-structured problems where diverserepresentationsand meta-cognitive monitoring are critical. Human traces show moreabstractionandconceptual processing, while models default tosurface-level enumeration. Meta-analysis of 1.6K LLM reasoning papers reveals the research community concentrates on easily quantifiable elements (sequential organization: 55%, decomposition: 60%) but neglectingmeta-cognitive controls(self-awareness: 16%) that correlate with success. Models possess behavioral repertoires associated with success but fail to deploy them spontaneously. Leveraging these patterns, we develop test-timereasoning guidancethat automatically scaffold successful structures, improving performance by up to 66.7% on complex problems. By establishing a shared vocabulary between cognitive science and LLM research, our framework enables systematic diagnosis of reasoning failures and principled development of models that reason throughrobust cognitive mechanismsrather thanspurious shortcuts, while providing tools to test theories of human cognition at scale. ðŸ“ŒWe built a comprehensive framework grounded in cognitive science to understand how LLMs actually reason. By analyzing192K+ reasoning tracesfrom 18 models alongside human think-aloud traces, we found that: Check out our paper, code, dataset, and blog: Excellent work! The 28-element cognitive framework and 192K annotated traces provide exactly the principled diagnostic lens our field needs. Clarifying question on scope: The empirical analysis focuses on open models (1.5B-671B params) where traces are accessible, but the title suggests broader applicability. Do you expect similar cognitive element underutilization patterns in frontier foundation models (GPT-4o, Claude 3.5/4, Gemini)? I ask because: (1) frontier models demonstrably benefit from scaffolding (CoT, self-reflection), suggesting your framework remains relevant, but (2) they often exhibit more spontaneous meta-cognition at scale. Without trace access to proprietary models, it's unclear if the cognitive gaps persist or are more pronounced in smaller models. Suggestion: Would explicitly scoping findings to \"open reasoning SLMs\" (while noting frontier models as future work) strengthen the claims? Or perhaps outcome-only proxy experiments on frontier models could test generalizability? Regardless, this taxonomy is exactly what we need to move beyond ad-hoc prompt engineering. Excited about applications to inference-time scaffolding and domain-specific reasoning frameworks! P.S. Anthropic's Extended Thinking traces ARE available This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2548068",
    "title": "Cognitive Foundations for Reasoning and Their Manifestation in LLMs",
    "authors": [],
    "publication_year": 2025,
    "github_url": "https://github.com/pkargupta/cognitive_foundations/",
    "huggingface_url": "https://huggingface.co/papers/2511.16660",
    "upvote": 9
  }
}