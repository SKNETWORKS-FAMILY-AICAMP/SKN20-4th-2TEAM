{
  "context": "Canvas-to-Image is a unified framework that encodes diverse control signals into a composite canvas image for high-fidelity multimodal image generation, outperforming existing methods in various benchmarks. While moderndiffusion modelsexcel at generating high-quality and diverse images, they still struggle withhigh-fidelity compositionalandmultimodal control, particularly when users simultaneously specifytext prompts,subject references,spatial arrangements,pose constraints, andlayout annotations. We introduce Canvas-to-Image, aunified frameworkthat consolidates these heterogeneous controls into a single canvas interface, enabling users to generate images that faithfully reflect their intent. Our key idea is to encode diverse control signals into a singlecomposite canvas imagethat the model can directly interpret for integratedvisual-spatial reasoning. We further curate a suite ofmulti-task datasetsand propose aMulti-Task Canvas Trainingstrategy that optimizes the diffusion model to jointly understand and integrate heterogeneous controls intotext-to-image generationwithin a unified learning paradigm. This joint training enables Canvas-to-Image to reason across multiple control modalities rather than relying on task-specific heuristics, and it generalizes well to multi-control scenarios during inference. Extensive experiments show that Canvas-to-Image significantly outperforms state-of-the-art methods inidentity preservationandcontrol adherenceacross challenging benchmarks, includingmulti-person composition,pose-controlled composition,layout-constrained generation, andmulti-control generation. While modern diffusion models excel at generating high-quality and diverse images, they still struggle with high-fidelity compositional and multimodal control. We introduce Canvas-to-Image, a unified framework that consolidates these heterogeneous controls into a single canvas interface. Our key idea is to encode diverse control signals, including subject references, bounding boxes, and pose skeletons, into a single composite canvas image that the model can directly interpret for integrated visual-spatial reasoning. where can we try the demo?? How is the image consistency, e.g. with successive images (video)? This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend This needs added to ComfyUI asap! Where is download code for Comfy node? Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2548021",
    "title": "Canvas-to-Image: Compositional Image Generation with Multimodal Controls",
    "authors": [
      "Yusuf Dalva",
      "Tsai-Shien Chen"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2511.21691",
    "upvote": 35
  }
}