{
  "context": "Monet, a training framework, enables MLLMs to reason in latent visual space using continuous embeddings, addressing challenges like computational cost and supervision, and outperforms on visual reasoning tasks. \"Thinking with images\" has emerged as an effective paradigm for advancing visual reasoning, extending beyond text-only chains of thought by injecting visual evidence into intermediate reasoning steps. However, existing methods fall short of human-like abstract visual thinking, as their flexibility is fundamentally limited by external tools. In this work, we introduce Monet, a training framework that enablesmultimodal large language models(MLLMs) to reason directly within thelatent visual spaceby generatingcontinuous embeddingsthat function asintermediate visual thoughts. We identify two core challenges in trainingMLLMsfor latent visual reasoning: high computational cost in latent-vision alignment and insufficient supervision over latent embeddings, and address them with athree-stage distillation-based supervised fine-tuning(SFT) pipeline. We further reveal a limitation of applyingGRPOto latent reasoning: it primarily enhances text-based reasoning rather than latent reasoning. To overcome this, we proposeVLPO(Visual-latent Policy Optimization), areinforcement learningmethod that explicitly incorporates latent embeddings intopolicy gradient updates. To supportSFT, we construct Monet-SFT-125K, a high-quality text-image interleavedCoT datasetcontaining 125K real-world, chart, OCR, and geometry CoTs. Our model, Monet-7B, shows consistent gains acrossreal-world perceptionandreasoning benchmarksand exhibits strongout-of-distribution generalizationon challenging abstract visual reasoning tasks. We also empirically analyze the role of each training component and discuss our early unsuccessful attempts, providing insights for future developments in visual latent reasoning. Our model, data, and code are available at https://github.com/NOVAglow646/Monet. \"Thinking with images\" has emerged as an effective paradigm for advancing visual reasoning, extending beyond text-only chains of thought by injecting visual evidence into intermediate reasoning steps. However, existing methods fall short of human-like abstract visual thinking, as their flexibility is fundamentally limited by external tools. In this work, we introduce Monet, a training framework that enables multimodal large language models (MLLMs) to reason directly within the latent visual space by generating continuous embeddings that function as intermediate visual thoughts. We identify two core challenges in training MLLMs for latent visual reasoning: high computational cost in latent-vision alignment and insufficient supervision over latent embeddings, and address them with a three-stage distillation-based supervised fine-tuning (SFT) pipeline. We further reveal a limitation of applying GRPO to latent reasoning: it primarily enhances text-based reasoning rather than latent reasoning. To overcome this, we propose VLPO (Visual-latent Policy Optimization), a reinforcement learning method that explicitly incorporates latent embeddings into policy gradient updates. To support SFT, we construct Monet-SFT-125K, a high-quality text-image interleaved CoT dataset containing 125K real-world, chart, OCR, and geometry CoTs. Our model, Monet-7B, shows consistent gains across real-world perception and reasoning benchmarks and exhibits strong out-of-distribution generalization on challenging abstract visual reasoning tasks. We also empirically analyze the role of each training component and discuss our early unsuccessful attempts, providing insights for future developments in visual latent reasoning. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2548046",
    "title": "Monet: Reasoning in Latent Visual Space Beyond Images and Language",
    "authors": [
      "Yang Shi"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/NOVAglow646/Monet",
    "huggingface_url": "https://huggingface.co/papers/2511.21395",
    "upvote": 16
  }
}