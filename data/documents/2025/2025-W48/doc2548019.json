{
  "context": "Soft Adaptive Policy Optimization (SAPO) enhances the stability and performance of reinforcement learning in large language models by adaptively attenuating off-policy updates with a smooth, temperature-controlled gate, leading to improved training stability and performance. Reinforcement learning(RL) plays an increasingly important role in enhancing the reasoning capabilities oflarge language models(LLMs), yet stable and performantpolicy optimizationremains challenging.Token-level importance ratiosoften exhibit high variance-a phenomenon exacerbated inMixture-of-Experts models-leading to unstable updates. Existing group-basedpolicy optimizationmethods, such asGSPOandGRPO, alleviate this problem via hard clipping, making it difficult to maintain both stability and effective learning. We proposeSoft Adaptive Policy Optimization(SAPO), which replaces hard clipping with a smooth, temperature-controlled gate that adaptively attenuatesoff-policy updateswhile preserving useful learning signals. Compared withGSPOandGRPO, SAPO is bothsequence-coherentandtoken-adaptive. LikeGSPO, SAPO maintainssequence-level coherence, but its soft gating forms a continuous trust region that avoids the brittle hard clipping band used inGSPO. When a sequence contains a few highly off-policy tokens,GSPOsuppresses all gradients for that sequence, whereas SAPO selectively down-weights only the offending tokens and preserves the learning signal from the near-on-policy ones, improvingsample efficiency. Relative toGRPO, SAPO replaces hard token-level clipping with smooth, temperature-controlled scaling, enabling more informative and stable updates. Empirical results on mathematical reasoning benchmarks indicate that SAPO exhibits improved training stability and higherPass@1 performanceunder comparable training budgets. Moreover, we employ SAPO to train theQwen3-VL model series, demonstrating that SAPO yields consistent performance gains across diverse tasks and different model sizes. Overall, SAPO provides a more reliable, scalable, and effective optimization strategy for RL training of LLMs. Soft Adaptive Policy Optimization ðŸš€ SAPO is now live on thems-swiftmain branch! Support includes hundreds of  state-of-the-art LLMs and MLLMs â€” try it out with your favorite model! Documentation is availablehere This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Why are your motivations aligned with CISPO and GPPO, yet you do not cite or compare them? Supported in TRL:https://huggingface.co/docs/trl/en/paper_index#soft-adaptive-policy-optimization   Hi! Thanks for the paper - we implemented this in SkyRL (https://github.com/NovaSky-AI/SkyRL/pull/762), but in my experiments with a small dense model (Qwen3-4B), I'm seeing that SAPO underperforms a DAPO baseline, although training reward is similar. Is this work primarily focused on results for MoE models, and is the focus on pass@1 assuming some temperature/sampling settings for the inference engine (currently using temperature 1, top_p=0.7 for eval)? Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2548019",
    "title": "Soft Adaptive Policy Optimization",
    "authors": [
      "Chujie Zheng",
      "Shixuan Liu",
      "An Yang"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/modelscope/ms-swift",
    "huggingface_url": "https://huggingface.co/papers/2511.20347",
    "upvote": 40
  }
}