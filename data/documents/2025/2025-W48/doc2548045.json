{
  "context": "M-GRPO, an extension of Group Relative Policy Optimization for hierarchical multi-agent systems, improves stability and efficiency in tool-augmented reasoning tasks by aligning heterogeneous trajectories and decoupling agent training. Multi-agent systemsperform well on general reasoning tasks. However, the lack of training in specialized areas hinders their accuracy. Current training methods train a unifiedlarge language model (LLM)for all agents in the system. This may limit the performances due to different distributions underlying for different agents. Therefore, trainingmulti-agent systemswith distinct LLMs should be the next step to solve. However, this approach introduces optimization challenges. For example, agents operate at different frequencies, rollouts involve varying sub-agent invocations, and agents are often deployed across separate servers, disrupting end-to-end gradient flow. To address these issues, we propose M-GRPO, a hierarchical extension ofGroup Relative Policy Optimizationdesigned for verticalMulti-agent systemswith a main agent (planner) and multiple sub-agents (multi-turn tool executors). M-GRPO computes group-relative advantages for both main and sub-agents, maintaininghierarchical credit assignment. It also introduces atrajectory-alignment schemethat generates fixed-size batches despite variable sub-agent invocations. We deploy adecoupled training pipelinein which agents run on separate servers and exchange minimal statistics via ashared store. This enables scalable training withoutcross-server backpropagation. In experiments on real-world benchmarks (e.g., GAIA, XBench-DeepSearch, and WebWalkerQA), M-GRPO consistently outperforms both single-agent GRPO and multi-agent GRPO with frozen sub-agents, demonstrating improved stability and sample efficiency. These results show that aligning heterogeneous trajectories and decoupling optimization across specialized agents enhancestool-augmented reasoning tasks. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2548045",
    "title": "Multi-Agent Deep Research: Training Multi-Agent Systems with M-GRPO",
    "authors": [],
    "publication_year": 2025,
    "github_url": "https://github.com/AQ-MedAI/MrlX",
    "huggingface_url": "https://huggingface.co/papers/2511.13288",
    "upvote": 17
  }
}