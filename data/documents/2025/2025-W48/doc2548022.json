{
  "context": "iMontage repurposes pre-trained video models to generate high-quality, diverse image sets with natural transitions and enhanced dynamics through a unified framework and tailored adaptation strategy. Pre-trained video modelslearn powerful priors for generating high-quality, temporally coherent content. While these models excel attemporal coherence, their dynamics are often constrained by thecontinuous natureof their training data. We hypothesize that by injecting the rich and unconstrainedcontent diversityfromimage datainto this coherent temporal framework, we can generate image sets that feature both natural transitions and a far more expansive dynamic range. To this end, we introduceiMontage, aunified frameworkdesigned to repurpose a powerful video model into an all-in-oneimage generator. The framework consumes and producesvariable-length image sets, unifying a wide array ofimage generationand editing tasks. To achieve this, we propose an elegant and minimally invasiveadaptation strategy, complemented by a tailoreddata curationprocess andtraining paradigm. This approach allows the model to acquire broadimage manipulationcapabilities without corrupting its invaluable originalmotion priors.iMontageexcels across several mainstream many-in-many-out tasks, not only maintaining strongcross-image contextual consistencybut also generating scenes with extraordinary dynamics that surpass conventional scopes. Find our homepage at: https://kr1sjfu.github.io/iMontage-web/. Pre-trained video models learn powerful priors for generating high-quality, temporally coherent content. While these models excel at temporal coherence, their dynamics are often constrained by the continuous nature of their training data. We hypothesize that by injecting the rich and unconstrained content diversity from image data into this coherent temporal framework, we can generate image sets that feature both natural transitions and a far more expansive dynamic range. To this end, we introduce iMontage, a unified framework designed to repurpose a powerful video model into an all-in-one image generator. The framework consumes and produces variable-length image sets, unifying a wide array of image generation and editing tasks. To achieve this, we propose an elegant and minimally invasive adaptation strategy, complemented by a tailored data curation process and training paradigm. This approach allows the model to acquire broad image manipulation capabilities without corrupting its invaluable original motion priors. iMontage excels across several mainstream many-in-many-out tasks, not only maintaining strong cross-image contextual consistency but also generating scenes with extraordinary dynamics that surpass conventional scopes. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2548022",
    "title": "iMontage: Unified, Versatile, Highly Dynamic Many-to-many Image Generation",
    "authors": [
      "Wei Cheng"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/Kr1sJFU/iMontage",
    "huggingface_url": "https://huggingface.co/papers/2511.20635",
    "upvote": 32
  }
}