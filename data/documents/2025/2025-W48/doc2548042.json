{
  "context": "CLaRa enhances retrieval-augmented generation by introducing unified embedding-based compression and joint optimization, achieving state-of-the-art performance in QA benchmarks. Retrieval-augmented generation (RAG) enhanceslarge language models(LLMs) withexternal knowledgebut still suffers fromlong contextsanddisjoint retrieval-generation optimization. In this work, we proposeCLaRa(Continuous Latent Reasoning), a unified framework that performsembedding-based compressionandjoint optimizationin ashared continuous space. To obtain semantically rich and retrievable compressed vectors, we introduceSCP, akey-preserving data synthesisframework usingQAandparaphrase supervision.CLaRathen trains thererankerandgeneratorend-to-end via a single language modeling loss, with gradients flowing through both modules using adifferentiable top-k estimator. Theoretically, this unified optimization aligns retrieval relevance with answer quality. Experiments across multipleQAbenchmarks show thatCLaRaachieves state-of-the-art compression and reranking performance, often surpassing text-based fine-tuned baselines. We introduce CLaRa: Continuous Latent Reasoning, the first end-to-end framework that unifies retrieval and generation within a continuous representation space. It employs Salient Compressor Pretraining (SCP) to learn semantically dense compressed vectors from QA and paraphrase signals, and uses differentiable top-k retrieval via Straight-Through estimation to make the retrieval process fully differentiable and trainable with weak supervision. By operating in a shared continuous latent space, CLaRa ensures that retrieval and generation rely on the same semantic representations, resulting in natural consistency. Empirically, it outperforms strong baselines across multiple QA benchmarksâ€”even exceeding traditional RAG systems built on full text plus BGE embeddingsâ€”and achieves up to 16Ã— context length reduction, greatly improving efficiency. Overall, CLaRa suggests a new direction for RAG: effective reasoning may rely not on long contexts, but on a unified latent reasoning space. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend arXiv explained breakdown of this paper ðŸ‘‰https://arxivexplained.com/papers/clara-bridging-retrieval-and-generation-with-continuous-latent-reasoning Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2548042",
    "title": "CLaRa: Bridging Retrieval and Generation with Continuous Latent Reasoning",
    "authors": [
      "Richard He Bai"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/apple/ml-clara",
    "huggingface_url": "https://huggingface.co/papers/2511.18659",
    "upvote": 19
  }
}