{
  "context": "LLMs exhibit varying levels of stability in encoding truth representations, influenced more by epistemic familiarity than linguistic form, as assessed through perturbation analysis of their activations. Large language models(LLMs) are widely used for factual tasks such as \"What treats asthma?\" or \"What is the capital of Latvia?\". However, it remains unclear how stably LLMs encode distinctions between true, false, and neither-true-nor-false content in their internal probabilistic representations. We introducerepresentational stabilityas the robustness of an LLM'sveracity representationsto perturbations in the operational definition of truth. We assessrepresentational stabilityby (i) training alinear probeon an LLM'sactivationsto separate true from not-true statements and (ii) measuring how its learned decision boundary shifts under controlled label changes. Usingactivationsfrom sixteen open-source models and three factual domains, we compare two types of neither statements. The first are fact-like assertions about entities we believe to be absent from any training data. We call theseunfamiliar neither statements. The second are nonfactual claims drawn from well-known fictional contexts. We call thesefamiliar neither statements. The unfamiliar statements induce the largest boundary shifts, producing up to 40% flippedtruth judgementsin fragile domains (such as word definitions), while familiar fictional statements remain more coherently clustered and yield smaller changes (leq 8.2%). These results suggest thatrepresentational stabilitystems more fromepistemic familiaritythan from linguistic form. More broadly, our approach provides a diagnostic for auditing and training LLMs to preserve coherent truth assignments undersemantic uncertainty, rather than optimizing for output accuracy alone. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2548105",
    "title": "Representational Stability of Truth in Large Language Models",
    "authors": [
      "Germans Savcisens"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2511.19166",
    "upvote": 1
  }
}