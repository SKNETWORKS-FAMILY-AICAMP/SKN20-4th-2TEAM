{
  "context": "Mantis, a VLA framework with Disentangled Visual Foresight and a diffusion Transformer, improves action prediction, comprehension, and reasoning while reducing training complexity. Recent advances in Vision-Language-Action (VLA) models demonstrate that visual signals can effectively complement sparse action supervisions. However, letting VLA directly predict high-dimensional visual states can distribute model capacity and incur prohibitive training cost, while compressing visual states into more compact supervisory signals inevitably incurs information bottlenecks. Moreover, existing methods often suffer from poor comprehension and reasoning capabilities due to the neglect of language supervision. This paper introduces Mantis, a novel framework featuring aDisentangled Visual Foresight(DVF) to tackle these issues. Specifically, Mantis decouples visual foresight prediction from the backbone with the combination ofmeta queriesand adiffusion Transformer(DiT) head. With the current visual state provided to the DiT via aresidual connection, a simple next-state prediction objective enables themeta queriesto automatically capture thelatent actionsthat delineate the visual trajectory, and hence boost the learning of explicit actions. The disentanglement reduces the burden of the VLA backbone, enabling it to maintain comprehension and reasoning capabilities through language supervision. Empirically, pretrained on human manipulation videos, robot demonstrations, and image-text pairs, Mantis achieves a 96.7% success rate onLIBERO benchmarkafter fine-tuning, surpassing powerful baselines while exhibiting high convergence speed. Real-world evaluations show that Mantis outperforms π_{0.5}, a leading open-source VLA model, particularly ininstruction-following capability, generalization to unseen instructions, andreasoning ability. Code and weights are released to support the open-source community.  This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend ·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2548057",
    "title": "Mantis: A Versatile Vision-Language-Action Model with Disentangled Visual Foresight",
    "authors": [
      "Yi Yang"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/zhijie-group/Mantis",
    "huggingface_url": "https://huggingface.co/papers/2511.16175",
    "upvote": 12
  }
}