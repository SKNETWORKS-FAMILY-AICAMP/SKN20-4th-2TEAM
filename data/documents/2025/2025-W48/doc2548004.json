{
  "context": "LatentMAS enables efficient, lossless collaboration among LLM agents in latent space, improving performance and reducing computational costs compared to text-based methods. Multi-agent systems(MAS) extendlarge language models(LLMs) from independent single-model reasoning to coordinative system-level intelligence. While existing LLM agents depend on text-based mediation for reasoning and communication, we take a step forward by enabling models to collaborate directly within the continuouslatent space. We introduceLatentMAS, an end-to-end training-free framework that enables pure latent collaboration among LLM agents. InLatentMAS, each agent first performsauto-regressive latent thoughts generationthrough last-layer hidden embeddings. A sharedlatent working memorythen preserves and transfers each agent's internal representations, ensuring losslessinformation exchange. We provide theoretical analyses establishing thatLatentMASattains higherexpressivenessand lossless information preservation with substantially lower complexity than vanilla text-based MAS. In addition, empirical evaluations across 9 comprehensive benchmarks spanningmath and science reasoning,commonsense understanding, andcode generationshow thatLatentMASconsistently outperforms strong single-model and text-based MAS baselines, achieving up to 14.6% higher accuracy, reducing output token usage by 70.8%-83.7%, and providing 4x-4.3x fasterend-to-end inference. These results demonstrate that our new latent collaboration framework enhances system-level reasoning quality while offering substantial efficiency gains without any additional training. Code and data are fully open-sourced at https://github.com/Gen-Verse/LatentMAS. Code and Data are released here:https://github.com/Gen-Verse/LatentMASX/Twitter Cover:https://x.com/LingYang_PU/status/1993510834245714001LinkedIn Cover:https://www.linkedin.com/feed/update/urn:li:activity:7399636490164559872 Very exciting work!How does this effect bandwidth? Does it trade token efficiency for bandwidth inefficiency? Hi Michael, Thanks for your excellent question on our LatentMAS work. I will provide a detailed response below. Let me know if you want to discuss more! How does this effect bandwidth? Does it trade token efficiency for bandwidth inefficiency? Short answer: No. LatentMAS does not trade token efficiency for bandwidth inefficiency. Its ‚Äúbandwidth‚Äù is in latent working memory, and since each latent step is far more expressive than a token, LatentMAS requires many fewer steps, making it both token-efficient and bandwidth-efficient, with faster inference. In normal TextMAS, bandwidth = #tokens √ó |V| (vocabulary-level information throughput) In LatentMAS, bandwidth = #latent steps √ó d‚Çï √ó L (hidden-state KV transfer).Note: Here, ‚Äúbandwidth‚Äù refers to internal GPU memory movement of latent working memory (stored in KV caches) between agents. As we know from Theorem 3.1:Latent¬†expressiveness=Œ©‚Å£(dhlog‚Å°‚à£V‚à£)√óText\\text{Latent expressiveness} = \\Omega\\!\\left(\\frac{d_h}{\\log |V|}\\right) \\times \\text{Text}Latent¬†expressiveness=Œ©(log‚à£V‚à£dh‚Äã‚Äã)√óText This means: Thus, while each latent step transmits dense vectors, each step carries far more information than a token, drastically reducing the number of required communication steps. According to the paper, both theoretical complexity analysis and empirical results (70‚Äì80% fewer tokens, 4√ó speedup) later demonstrate that LatentMAS is strictly more bandwidth-efficient at the system level than text-based multi-agent communication. Thank you. So we're looking at perhaps 50 latent steps, which ought to take about the same time as 50 tokens, perhaps ~10% less. Then it will output perhaps 1,000 tokens which would be the answer minus the verbose reasoning, reaching a solution that would ordinarily take 8,000+ tokens. So without a multi-agent setup it's effectively a Latent Recurrent Transformer. Assuming my understanding is correct I have a few more questions if you don't mind answering. Can we mix and match LLM's?Qwen 3 4b with Qwen 3 14b? How about mixing with other families? Does it support quantization? Does it work with Lora? Is the hierarchical agent setup concurrent? I.e can two small models work together in realtime, with instant access to each other's steps, or does it need to fully complete all the steps before it is shared? This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend arXiv explained breakdown of this paper üëâhttps://arxivexplained.com/papers/latent-collaboration-in-multi-agent-systems Wow, that's fantastic, thanks for the support! Hi, thanks for the great work!I have a question regarding the latent reasoning mechanism. Since the backbone model was not explicitly trained to refine its latent representations recurrently step-by-step, I am curious about the effectiveness of the feedback loop.I understand that the $W_a$ matrix helps align the hidden state distribution with the input embeddings. However, what specifically ensures that these aligned features remain semantically meaningful for the next step? In other words, what drives the model to actually 'refine' the thought rather than just drifting, given it wasn't trained for this specific type of recurrence? My (limited) understanding is that the model was trained for precisely this. With standard inference it's doing almost exactly the same. The only thing that could interfere is a) the lack of grammar steering at the tokenizer level, and b) possible mismatch in the new projection layer. If we assume that the projection is perfect then the results must be identical to standard inference (minus grammar steering). I might be wrong, but that's my assumption. I would say that the transformer layers are already basically a recurrent network split over several layers, I.e if you merged all the layers together then you'd be forced to use recursion, and the result would be identical arXiv lens breakdown of this paper üëâhttps://arxivlens.com/PaperView/Details/latent-collaboration-in-multi-agent-systems-9045-32453d85 ¬∑Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2548004",
    "title": "Latent Collaboration in Multi-Agent Systems",
    "authors": [
      "Jiaru Zou",
      "Pan Lu",
      "Ling Yang"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/Gen-Verse/LatentMAS",
    "huggingface_url": "https://huggingface.co/papers/2511.20639",
    "upvote": 117
  }
}