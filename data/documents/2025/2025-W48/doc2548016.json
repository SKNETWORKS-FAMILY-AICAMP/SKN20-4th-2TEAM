{
  "context": "PRFL optimizes video generation preferences in latent space, improving alignment with human preferences while reducing memory consumption and training time. Reward feedback learning(ReFL) has proven effective for aligning image generation with human preferences. However, its extension tovideo generationfaces significant challenges. Existing video reward models rely onvision-language modelsdesigned for pixel-space inputs, confiningReFLoptimization to near-complete denoising steps after computationally expensiveVAE decoding. This pixel-space approach incurs substantial memory overhead and increased training time, and its late-stage optimization lacks early-stage supervision, refining only visual quality rather than fundamental motion dynamics and structural coherence. In this work, we show that pre-trainedvideo generationmodels are naturally suited for reward modeling in the noisylatent space, as they are explicitly designed to processnoisy latent representationsat arbitrary timesteps and inherently preserve temporal information through theirsequential modelingcapabilities. Accordingly, we propose ProcessReward Feedback Learning~(PRFL), a framework that conducts preference optimization entirely inlatent space, enabling efficientgradient backpropagationthroughout the full denoising chain withoutVAE decoding. Extensive experiments demonstrate thatPRFLsignificantly improves alignment with human preferences, while achieving substantial reductions in memory consumption and training time compared to RGBReFL. üé¨ PRFL: Efficient Video Generation Alignment in Latent Space We introduce Process Reward Feedback Learning (PRFL), a novel framework that enables efficient human preference alignment for video generation models‚Äîentirely in latent space! Key Innovation: Instead of relying on expensive pixel-space reward models, we demonstrate that pre-trained video generation models themselves are excellent reward models. They naturally understand noisy latent representations at any timestep and preserve temporal information. Why it matters:‚ú® Full denoising chain optimization without VAE decoding‚ö° Significantly reduced memory & training time vs RGB-based ReFLüéØ Better alignment with human preferences This opens up new possibilities for scaling video generation alignment! Check out our paper and project page for demos. üìÑ Paper:https://arxiv.org/abs/2511.21541üåê Project:https://kululumi.github.io/PRFL/ cool! üëçüëçüëç This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend I am very interested in your work. I was wondering if you have any plans to open-source the code in the near future. Pending approval. Please wait. ¬∑Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2548016",
    "title": "Video Generation Models Are Good Latent Reward Models",
    "authors": [
      "Xiaoyue Mi",
      "Zijun Liu",
      "Guozhen Zhang"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2511.21541",
    "upvote": 45
  }
}