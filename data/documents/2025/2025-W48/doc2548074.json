{
  "context": "Combining visual and linguistic reasoning strategies improves performance on abstract reasoning tasks in the ARC-AGI dataset by leveraging the strengths of each modality. Abstract reasoningfrom minimal examples remains a core unsolved problem for frontier foundation models such asGPT-5andGrok 4. These models still fail to infer structured transformation rules from a handful of examples, which is a key hallmark of human intelligence. TheAbstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI)provides a rigorous testbed for this capability, demandingconceptual rule inductionand transfer to novel tasks. Most existing methods treat ARC-AGI as a purely textual reasoning task, overlooking the fact that humans rely heavily onvisual abstractionwhen solving such puzzles. However, our pilot experiments reveal a paradox: naively rendering ARC-AGI grids as images degrades performance due to imprecise rule execution. This leads to our central hypothesis that vision and language possess complementary strengths across distinct reasoning stages: vision supportsglobal pattern abstractionand verification, whereas language specializes insymbolic rule formulationand precise execution. Building on this insight, we introduce two synergistic strategies: (1)Vision-Language Synergy Reasoning (VLSR), which decomposes ARC-AGI into modality-aligned subtasks; and (2)Modality-Switch Self-Correction (MSSC), which leverages vision to verify text-based reasoning forintrinsic error correction. Extensive experiments demonstrate that our approach yields up to a 4.33% improvement over text-only baselines across diverse flagship models and multiple ARC-AGI tasks. Our findings suggest that unifyingvisual abstractionwith linguistic reasoning is a crucial step toward achieving generalizable, human-like intelligence in future foundation models. Source code will be released soon. Abstract reasoning from minimal examples remains a core unsolved problem for frontier foundation models such as GPT-5 and Grok 4. These models still fail to infer structured transformation rules from a handful of examples, which is a key hallmark of human intelligence. The Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) provides a rigorous testbed for this capability, demanding conceptual rule induction and transfer to novel tasks. Most existing methods treat ARC-AGI as a purely textual reasoning task, overlooking the fact that humans rely heavily on visual abstraction when solving such puzzles. However, our pilot experiments reveal a paradox: naively rendering ARC-AGI grids as images degrades performance due to imprecise rule execution. This leads to our central hypothesis that vision and language possess complementary strengths across distinct reasoning stages: vision supports global pattern abstraction and verification, whereas language specializes in symbolic rule formulation and precise execution. Building on this insight, we introduce two synergistic strategies: (1) Vision-Language Synergy Reasoning (VLSR), which decomposes ARC-AGI into modality-aligned subtasks; and (2) Modality-Switch Self-Correction (MSSC), which leverages vision to verify text-based reasoning for intrinsic error correction. Extensive experiments demonstrate that our approach yields up to a 4.33% improvement over text-only baselines across diverse flagship models and multiple ARC-AGI tasks. Our findings suggest that unifying visual abstraction with linguistic reasoning is a crucial step toward achieving generalizable, human-like intelligence in future foundation models. Source code is released at \\url{https://github.com/InternLM/ARC-VL}. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2548074",
    "title": "Think Visually, Reason Textually: Vision-Language Synergy in ARC",
    "authors": [
      "Yuhang Zang"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/InternLM/ARC-VL",
    "huggingface_url": "https://huggingface.co/papers/2511.15703",
    "upvote": 8
  }
}