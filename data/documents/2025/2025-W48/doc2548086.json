{
  "context": "A unified vision-language-action framework, MobileVLA-R1, enhances reasoning and control for quadruped robots through supervised chain-of-thought alignment and GRPO reinforcement learning, achieving superior performance in complex environments. Grounding natural-language instructions into continuous control for quadruped robots remains a fundamental challenge in vision language action. Existing methods struggle to bridge high-level semantic reasoning and low-level actuation, leading to unstable grounding and weak generalization in the real world. To address these issues, we presentMobileVLA-R1, a unifiedvision-language-action frameworkthat enables explicit reasoning and continuous control for quadruped robots. We construct MobileVLA-CoT, a large-scale dataset of multi-granularitychain-of-thought (CoT)for embodied trajectories, providing structured reasoning supervision for alignment. Built upon this foundation, we introduce a two-stage training paradigm that combines supervised CoT alignment withGRPO reinforcement learningto enhance reasoning consistency, control stability, and long-horizon execution. Extensive evaluations on VLN andVLA tasksdemonstrate superior performance over strong baselines, with approximately a 5% improvement. Real-world deployment on a quadruped robot validates robust performance in complex environments. Code: https://github.com/AIGeeksGroup/MobileVLA-R1. Website: https://aigeeksgroup.github.io/MobileVLA-R1. MobileVLA-R1 introduces a unified vision-language-action framework for quadruped robots that combines multi-granularity chain-of-thought (MobileVLA-CoT) with GRPO reinforcement learning. This two-stage training improves reasoning consistency and long-horizon control, and we validate it both in simulation and on a real quadruped platform. Feedback on the CoT design and RL reward formulation is very welcome! This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2548086",
    "title": "MobileVLA-R1: Reinforcing Vision-Language-Action for Mobile Robots",
    "authors": [
      "Ting Huang",
      "Zeyu Zhang",
      "Zida Yang"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/AIGeeksGroup/MobileVLA-R1",
    "huggingface_url": "https://huggingface.co/papers/2511.17889",
    "upvote": 5
  }
}