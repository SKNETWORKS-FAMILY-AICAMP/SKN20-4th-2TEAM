{
  "context": "ENACT is a benchmark that evaluates embodied cognition in vision-language models through world modeling from egocentric interaction in a VQA format, revealing performance gaps and anthropocentric biases. Embodied cognitionargues that intelligence arises from sensorimotor interaction rather than passive observation. It raises an intriguing question: do modernvision-language models(VLMs), trained largely in a disembodied manner, exhibit signs ofembodied cognition? We introduce ENACT, abenchmarkthat casts evaluation ofembodied cognitionasworld modelingfrom egocentric interaction in avisual question answering(VQA) format. Framed as apartially observable Markov decision process(POMDP) whose actions arescene graphchanges, ENACT comprises two complementary sequence reordering tasks:forward world modeling(reorder shuffled observations given actions) andinverse world modeling(reorder shuffled actions given observations). While conceptually simple, solving these tasks implicitly demands capabilities central toembodied cognition-affordance recognition,action-effect reasoning,embodied awareness, and interactive,long-horizon memoryfrom partially observable egocentric input, while avoiding low-level image synthesis that could confound the evaluation. We provide a scalable pipeline that synthesizes QA pairs fromrobotics simulation(BEHAVIOR) and evaluates models on 8,972 QA pairs spanning long-horizon home-scale activities. Experiments reveal a performance gap between frontierVLMsand humans that widens with interaction horizon. Models consistently perform better on the inverse task than the forward one and exhibitanthropocentric biases, including a preference for right-handed actions and degradation when camera intrinsics or viewpoints deviate from human vision. Website at https://enact-embodied-cognition.github.io/. Embodied cognition argues that intelligence arises from sensorimotor interaction rather than passive observation. It raises an intriguing question: do modern vision-language models (VLMs), trained largely in a disembodied manner, exhibit signs of embodied cognition? We introduce ENACT, a benchmark that casts evaluation of embodied cognition as world modeling from egocentric interaction in a visual question answering (VQA) format. Framed as a partially observable Markov decision process (POMDP) whose actions are scene graph changes, ENACT comprises two complementary sequence reordering tasks: forward world modeling (reorder shuffled observations given actions) and inverse world modeling (reorder shuffled actions given observations). While conceptually simple, solving these tasks implicitly demands capabilities central to embodied cognition-affordance recognition, action-effect reasoning, embodied awareness, and interactive, long-horizon memory from partially observable egocentric input, while avoiding low-level image synthesis that could confound the evaluation. We provide a scalable pipeline that synthesizes QA pairs from robotics simulation (BEHAVIOR) and evaluates models on 8,972 QA pairs spanning long-horizon home-scale activities.Experiments reveal a performance gap between frontier VLMs and humans that widens with interaction horizon. Models consistently perform better on the inverse task than the forward one and exhibitanthropocentric biases, including a preference for right-handed actions and degradation when camera intrinsics or viewpoints deviate from human vision.Website atthis https URL. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2548050",
    "title": "ENACT: Evaluating Embodied Cognition with World Modeling of Egocentric Interaction",
    "authors": [
      "Qineng Wang"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/mll-lab-nu/ENACT",
    "huggingface_url": "https://huggingface.co/papers/2511.20937",
    "upvote": 15
  }
}