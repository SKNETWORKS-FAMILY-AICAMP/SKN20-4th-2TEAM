{
  "context": "A data-driven neural network approach estimates mutual information using a meta-dataset of synthetic distributions, offering flexibility, efficiency, and uncertainty quantification. We propose a fully data-driven approach to designingmutual information(MI) estimators. Since anyMI estimatoris a function of the observed sample from two random variables, we parameterize this function with aneural network(MIST) and train it end-to-end to predict MI values. Training is performed on a largemeta-datasetof 625,000 synthetic joint distributions with known ground-truth MI. To handle variable sample sizes and dimensions, we employ atwo-dimensional attentionscheme ensuringpermutation invarianceacross input samples. To quantify uncertainty, we optimize aquantile regressionloss, enabling the estimator to approximate the sampling distribution of MI rather than return a single point estimate. This research program departs from prior work by taking a fully empirical route, trading universal theoretical guarantees for flexibility and efficiency. Empirically, the learned estimators largely outperform classical baselines across sample sizes and dimensions, including on joint distributions unseen during training. The resultingquantile-based intervalsare well-calibrated and more reliable thanbootstrap-based confidence intervals, while inference is orders of magnitude faster than existing neural baselines. Beyond immediate empirical gains, this framework yields trainable, fully differentiable estimators that can be embedded into larger learning pipelines. Moreover, exploiting MI's invariance to invertible transformations,meta-datasets can be adapted to arbitrary data modalities vianormalizing flows, enabling flexible training for diverse target meta-distributions. TL;DR: MIST is a fast, differentiable, neural mutual information estimator trained on synthetic data that outperforms baselines and provides well-calibrated uncertainty intervals. paper:https://arxiv.org/abs/2511.18945github:https://github.com/grgera/mistdataset: zenodo.org/records/17599669 This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend ·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2548071",
    "title": "MIST: Mutual Information Via Supervised Training",
    "authors": [
      "German Gritsai",
      "Maxime Méloux",
      "Maxime Peyrard"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/grgera/mist",
    "huggingface_url": "https://huggingface.co/papers/2511.18945",
    "upvote": 8
  }
}