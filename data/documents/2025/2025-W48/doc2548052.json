{
  "context": "VisMem enhances Vision-Language Models by incorporating dynamic latent vision memories, improving performance on complex visual tasks through perceptual fidelity and semantic consistency. Despite the remarkable success of Vision-Language Models (VLMs), their performance on a range of complex visual tasks is often hindered by a \"visual processing bottleneck\": a propensity to lose grounding in visual evidence and exhibit a deficit in contextualized visual experience during prolonged generation. Drawing inspiration from human cognitive memory theory, which distinguishesshort-term visually-dominant memoryandlong-term semantically-dominant memory, we propose VisMem, a cognitively-aligned framework that equips VLMs withdynamic latent vision memories, a short-term module for fine-grainedperceptual retentionand a long-term module for abstractsemantic consolidation. These memories are seamlessly invoked during inference, allowing VLMs to maintain both perceptual fidelity and semantic consistency across thinking and generation. Extensive experiments across diverse visual benchmarks for understanding, reasoning, and generation reveal that VisMem delivers a significant average performance boost of 11.8% relative to the vanilla model and outperforms all counterparts, establishing a new paradigm forlatent-space memory enhancement. The code will be available: https://github.com/YU-deep/VisMem.git. VisMem: Latent Vision Memory Unlocks Potential of Vision-Language Models This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2548052",
    "title": "VisMem: Latent Vision Memory Unlocks Potential of Vision-Language Models",
    "authors": [
      "Peng-Tao Jiang"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/YU-deep/VisMem.git",
    "huggingface_url": "https://huggingface.co/papers/2511.11007",
    "upvote": 15
  }
}