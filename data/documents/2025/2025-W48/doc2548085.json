{
  "context": "A method that enhances vision language models with spatial-temporal signals and motion tracking improves their performance on physics-driven video reasoning tasks. Vision Language Models(VLMs) perform well on standard video tasks but struggle withphysics-driven reasoninginvolvingmotion dynamicsandspatial interactions. This limitation reduces their ability to interpret real or AI-generated content (AIGC) videos and to generate physically consistent content. We present an approach that addresses this gap by translating physical-world context cues into interpretable representations aligned withVLMs' perception,comprehension, and reasoning. We introduceMASS-Bench, a comprehensive benchmark consisting of 4,350 real-world andAIGC videosand 8,361 free-formvideo question-answeringpairs focused on physics-relatedcomprehensiontasks, with detailed annotations includingvisual detections,sub-segment grounding, and full-sequence3D motion trackingof entities. We further presentMASS, amodel-agnostic methodthat injects spatial-temporal signals into the VLM language space viadepth-based 3D encodingandvisual grounding, coupled with amotion trackerforobject dynamics. To strengthencross-modal alignmentand reasoning, we applyreinforcement fine-tuning. Experiments and ablations show that our refinedVLMsoutperform comparable and larger baselines, as well as prior state-of-the-art models, by 8.7% and 6.0%, achieving performance comparable to close-source SoTAVLMssuch asGemini-2.5-Flashonphysics reasoningandcomprehension. These results validate the effectiveness of our approach. Vision Language Models (VLMs) perform well on standard video tasks but struggle with physics-driven reasoning involving motion dynamics and spatial interactions. This limitation reduces their ability to interpret real or AI-generated content (AIGC) videos and to generate physically consistent content. We present an approach that addresses this gap by translating physical-world context cues into interpretable representations aligned with VLMs' perception, comprehension, and reasoning. We introduce MASS-Bench, a comprehensive benchmark consisting of 4,350 real-world and AIGC videos and 8,361 free-form video question-answering pairs focused on physics-related comprehension tasks, with detailed annotations including visual detections, sub-segment grounding, and full-sequence 3D motion tracking of entities. We further present MASS, a model-agnostic method that injects spatial-temporal signals into the VLM language space via depth-based 3D encoding and visual grounding, coupled with a motion tracker for object dynamics. To strengthen cross-modal alignment and reasoning, we apply reinforcement fine-tuning. Experiments and ablations show that our refined VLMs outperform comparable and larger baselines, as well as prior state-of-the-art models, by 8.7% and 6.0%, achieving performance comparable to close-source SoTA VLMs such as Gemini-2.5-Flash on physics reasoning and comprehension. These results validate the effectiveness of our approach. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2548085",
    "title": "MASS: Motion-Aware Spatial-Temporal Grounding for Physics Reasoning and Comprehension in Vision-Language Models",
    "authors": [],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2511.18373",
    "upvote": 5
  }
}