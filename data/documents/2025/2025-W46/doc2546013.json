{
  "context": "The study presents a two-stage reinforcement learning approach for competitive-programming code generation, achieving state-of-the-art performance using Group Relative Policy Optimization and a hard-focus curriculum. Recent reasoning-first models (e.g., OpenAI o1, DeepSeek R1) have spurred a\nresurgence of interest inRLVR. Nevertheless, advances are dominated by\nmathematics (e.g., AIME), withcompetitive-programming code generationunderexplored and data curation receiving less attention than RL algorithm\ndesign. We investigate how to constructRLVRdatasets (i.e., RL prompts) and\npresent practical training techniques that yield strong performance oncompetitive-programming code generation. Our pipeline begins with supervised\nfine-tuning (SFT) distilled from strong open-source models, augmented with\ngeneral-purpose and reasoning-intensive data. RL then follows a two-stage\nprocess with executable, testcase-driven rewards: first, training on a large,\nuniformly distributed set of competitive-programming problems using Group\nRelative Policy Optimization (GRPO) with 8 rollouts per prompt and a relatively\nshort response-generation window (e.g., 32k during SFT and 24k in this stage)\nto expand entropy and mitigate repetition and truncation; second, we performPre-GRPO: updating on a small, high-quality set of challenging\nproblems with a large rollout budget (64 rollouts per prompt) under a\nhard-focus curriculum that continuously retains the most difficult instances\nthroughout training. We implement our method onQwen2.5-32Band evaluate onLeetCodeandCodeforcesweekly contests to avoid data leakage. The resulting\nmodel achieves state-of-the-art performance among models of similar scale and\nis comparable to leading systems such as DeepSeek v3.1 and Doubao-1.5-Thinking.\nWe also examine scaling trends and observe strong RL scaling on an internal\nlarge-scaleMoE model. Our study distills concise best practices for data\ncuration,entropy expansion, andcurriculum designinRLVRforcompetitive-programming code generation. Recent reasoning-first models (e.g., OpenAI o1, DeepSeek R1) have spurred a resurgence of interest in RLVR. Nevertheless, advances are dominated by mathematics (e.g., AIME), with competitive-programming code generation underexplored and data curation receiving less attention than RL algorithm design. We investigate how to construct RLVR datasets (i.e., RL prompts) and present practical training techniques that yield strong performance on competitive-programming code generation. Our pipeline begins with supervised fine-tuning (SFT) distilled from strong open-source models, augmented with general-purpose and reasoning-intensive data. RL then follows a two-stage process with executable, testcase-driven rewards: first, training on a large, uniformly distributed set of competitive-programming problems using Group Relative Policy Optimization (GRPO) with 8 rollouts per prompt and a relatively short response-generation window (e.g., 32k during SFT and 24k in this stage) to expand entropy and mitigate repetition and truncation; second, we perform \\textbf{Pre-GRPO}: updating on a small, high-quality set of challenging problems with a large rollout budget (64 rollouts per prompt) under a hard-focus curriculum that continuously retains the most difficult instances throughout training. We implement our method on Qwen2.5-32B and evaluate on LeetCode and Codeforces weekly contests to avoid data leakage. The resulting model achieves state-of-the-art performance among models of similar scale and is comparable to leading systems such as DeepSeek v3.1 and Doubao-1.5-Thinking. We also examine scaling trends and observe strong RL scaling on an internal large-scale MoE model. Our study distills concise best practices for data curation, entropy expansion, and curriculum design in RLVR for competitive-programming code generation. nice work where is the dataset link This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend The github of our paper: The ckpts of our paper: Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2546013",
    "title": "DRIVE: Data Curation Best Practices for Reinforcement Learning with\n  Verifiable Reward in Competitive Code Generation",
    "authors": [
      "Guang Chen"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/Tencent-Hunyuan/DRIVE-RLVR",
    "huggingface_url": "https://huggingface.co/papers/2511.06307",
    "upvote": 51
  }
}