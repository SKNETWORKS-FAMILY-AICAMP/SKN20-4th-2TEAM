{
  "context": "A novel policy optimization algorithm, SofT-GRPO, enhances soft-thinking in Large Language Models by integrating Gumbel noise and the Gumbel-Softmax technique, leading to improved performance over discrete-token methods. Thesoft-thinkingparadigm forLarge Language Model (LLM)reasoning can\noutperform the conventional discrete-tokenChain-of-Thought (CoT)reasoning in\nsome scenarios, underscoring its research and application value. However, while\nthe discrete-token CoT reasoning pattern can be reinforced through policy\noptimization algorithms such asgroup relative policy optimization (GRPO),\nextending thesoft-thinkingpattern withReinforcement Learning (RL)remains\nchallenging. This difficulty stems from the complexities of injecting\nstochasticity intosoft-thinkingtokens and updatingsoft-thinkingpolicies\naccordingly. As a result, previous attempts to combinesoft-thinkingwith GRPO\ntypically underperform their discrete-token GRPO counterparts. To fully unlock\nthe potential ofsoft-thinking, this paper presents a novelpolicy optimizationalgorithm, SofT-GRPO, to reinforce LLMs under thesoft-thinkingreasoning\npattern. SofT-GRPO injects theGumbel noiseinto logits, employs theGumbel-Softmaxtechnique to avoidsoft-thinkingtokens outside the pre-trained\nembedding space, and leverages thereparameterization trickinpolicy gradient.\nWe conduct experiments across base LLMs ranging from 1.5B to 7B parameters, and\nresults demonstrate that SofT-GRPO enablessoft-thinkingLLMs to slightly\noutperform discrete-token GRPO on Pass@1 (+0.13% on average accuracy), while\nexhibiting a substantial uplift on Pass@32 (+2.19% on average accuracy). Codes\nand weights are available on https://github.com/zz1358m/SofT-GRPO-master This paper develops the first powerful RLVR algorithm, SofT-GRPO, for soft-thinking. It integrates the Gumbel-Softmax technique into the group rollout process, actively obtaining diverse but valid soft-thinking reasoning paths. We also propose an innovative gradient estimation approach via Gumbel reparameterization, enabling precise attribution of improvements to the LLM’s output probability distributions in policy optimization. We conduct comprehensive experiments across LLMs of 1.5B–7B parameters on five benchmarks, demonstrating that SofT-GRPO consistently outperforms the discrete-token GRPO baselines, especially at higher sample rates (Pass@16 and Pass@32). SofT-GRPO can also improve the out-of-Domain generalization ability of LLMs. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend ·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2546037",
    "title": "SofT-GRPO: Surpassing Discrete-Token LLM Reinforcement Learning via\n  Gumbel-Reparameterized Soft-Thinking Policy Optimization",
    "authors": [
      "Zhi Zheng"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/zz1358m/SofT-GRPO-master/tree/main",
    "huggingface_url": "https://huggingface.co/papers/2511.06411",
    "upvote": 17
  }
}