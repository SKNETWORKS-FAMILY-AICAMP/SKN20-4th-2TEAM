{
  "context": "A novel video self-supervised reinforcement learning framework, VideoSSR, enhances MLLM performance across various video understanding tasks by leveraging intrinsic video information. Reinforcement Learning with Verifiable Rewards(RLVR) has substantially\nadvanced the video understanding capabilities of Multimodal Large Language\nModels (MLLMs). However, the rapid progress of MLLMs is outpacing the\ncomplexity of existing video datasets, while the manual annotation of new,\nhigh-quality data remains prohibitively expensive. This work investigates a\npivotal question: Can the rich, intrinsic information within videos be\nharnessed to self-generate high-quality, verifiable training data? To\ninvestigate this, we introduce threeself-supervised pretext tasks: Anomaly\nGrounding,Object Counting, andTemporal Jigsaw. We construct the Video\nIntrinsic Understanding Benchmark (VIUBench) to validate their difficulty,\nrevealing that current state-of-the-art MLLMs struggle significantly on these\ntasks. Building upon these pretext tasks, we develop theVideoSSR-30K datasetand proposeVideoSSR, a novel video self-supervised reinforcement learning\nframework for RLVR. Extensive experiments across 17 benchmarks, spanning four\nmajor video domains (General Video QA,Long Video QA,Temporal Grounding, andComplex Reasoning), demonstrate thatVideoSSRconsistently enhances model\nperformance, yielding an average improvement of over 5\\%. These results\nestablishVideoSSRas a potent foundational framework for developing more\nadvanced video understanding in MLLMs. The code is available at\nhttps://github.com/lcqysl/VideoSSR. VideoSSR: Video Self-Supervised Reinforcement Learning VideoSSR yields an average improvement of over 5% on 17 benchmarks compared to Qwen-VL3. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2546030",
    "title": "VideoSSR: Video Self-Supervised Reinforcement Learning",
    "authors": [],
    "publication_year": 2025,
    "github_url": "https://github.com/lcqysl/VideoSSR",
    "huggingface_url": "https://huggingface.co/papers/2511.06281",
    "upvote": 24
  }
}