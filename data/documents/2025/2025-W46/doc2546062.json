{
  "context": "Diffusion-SDPO improves text-to-image generation quality by adaptively scaling the loser gradient in preference optimization, ensuring the preferred output's error does not increase. Text-to-imagediffusion modelsdeliver high-quality images, yet aligning them\nwith human preferences remains challenging. We revisit diffusion-based Direct\nPreference Optimization (DPO) for these models and identify a critical\npathology: enlarging thepreference margindoes not necessarily improve\ngeneration quality. In particular, the standard Diffusion-DPO objective can\nincrease thereconstruction errorof both winner andloser branches.\nConsequently, degradation of the less-preferred outputs can become sufficiently\nsevere that the preferred branch is also adversely affected even as the margin\ngrows. To address this, we introduceDiffusion-SDPO, asafeguarded update rulethat preserves the winner by adaptively scaling theloser gradientaccording to\nitsalignmentwith thewinner gradient. A first-order analysis yields aclosed-form scaling coefficientthat guarantees the error of the preferred\noutput is non-increasing at each optimization step. Our method is simple,\nmodel-agnostic, broadly compatible with existing DPO-stylealignmentframeworks\nand adds only marginal computational overhead. Across standard text-to-image\nbenchmarks,Diffusion-SDPOdelivers consistent gains over preference-learning\nbaselines onautomated preference,aesthetic, andprompt alignment metrics.\nCode is publicly available at https://github.com/AIDC-AI/Diffusion-SDPO. GitHub:https://github.com/AIDC-AI/Diffusion-SDPO This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2546062",
    "title": "Diffusion-SDPO: Safeguarded Direct Preference Optimization for Diffusion\n  Models",
    "authors": [
      "Minghao Fu",
      "Guo-Hua Wang",
      "Tianyu Cui"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/AIDC-AI/Diffusion-SDPO",
    "huggingface_url": "https://huggingface.co/papers/2511.03317",
    "upvote": 6
  }
}