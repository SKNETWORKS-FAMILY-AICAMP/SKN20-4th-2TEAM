{
  "context": "Aligning routing weights with task embeddings in Sparse Mixture-of-Experts (MoE) models improves generalization and reduces performance gaps in large language models. Sparse Mixture-of-Experts(MoE) have been widely adopted in recent large\nlanguage models since it can efficiently scale up the model capability without\nincreasing the inference cost. However, evaluations on broad downstream tasks\nreveal a consistent suboptimality of the routers in existingMoELLMs, which\nresults in a severe performance gap (e.g., 10-20% in accuracy) to the optimal\nrouting. In this paper, we show that aligning the manifold ofrouting weightswith that oftask embeddingcan effectively reduce the gap and improveMoELLMs' generalization performance. Our method, \"Routing Manifold Alignment\n(RoMA)\", introduces an additionalmanifold regularizationterm in the\npost-training objective and only requires lightweight finetuning of routers\n(with other parameters frozen). Specifically, the regularization encourages therouting weightsof each sample to be close to those of its successful neighbors\n(whoserouting weightslead to correct answers) in atask embeddingspace.\nConsequently, samples targeting similar tasks will share similar expert choices\nacross layers. Building such bindings between tasks and experts over different\nsamples is essential to achieve better generalization. Moreover,RoMAdemonstrates the advantage of unifying the task understanding (by embedding\nmodels) with solution generation (byMoELLMs). In experiments, we finetune\nrouters inOLMoE,DeepSeekMoE, andQwen3-MoEusingRoMA. Evaluations on diverse\nbenchmarks and extensive comparisons with baselines show the substantial\nimprovement brought byRoMA. TL;DR: RoMA uses pretrained embedding to post-train MoE routers. It aligns routing weights' manifold with the manifold of task embedding, and significantly improves MoE LLMs’ performance on downstream tasks by 5-10% on average.  This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend ·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2546029",
    "title": "Routing Manifold Alignment Improves Generalization of Mixture-of-Experts\n  LLMs",
    "authors": [
      "Tianyi Zhou"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/tianyi-lab/RoMA",
    "huggingface_url": "https://huggingface.co/papers/2511.07419",
    "upvote": 26
  }
}