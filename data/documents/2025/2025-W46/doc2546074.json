{
  "context": "AI agents frequently perform refactoring in open-source Java projects, focusing on low-level consistency edits and improving code quality metrics. Agentic coding tools, such as OpenAI Codex, Claude Code, and Cursor, are transforming the software engineering landscape. These AI-powered systems function as autonomous teammates capable of planning and executing complex development tasks. Agents have become active participants in refactoring, a cornerstone of sustainable software development aimed at improving internal code quality without altering observable behavior. Despite their increasing adoption, there is a critical lack of empirical understanding regarding how agentic refactoring is utilized in practice, how it compares to human-driven refactoring, and what impact it has on code quality. To address this empirical gap, we present a large-scale study of AI agent-generated refactorings in real-world open-source Java projects, analyzing 15,451 refactoring instances across 12,256 pull requests and 14,988 commits derived from the AIDev dataset. Our empirical analysis shows that refactoring is a common and intentional activity in this development paradigm, with agents explicitly targeting refactoring in 26.1% of commits. Analysis of refactoring types reveals that agentic efforts are dominated by low-level, consistency-oriented edits, such as Change Variable Type (11.8%), Rename Parameter (10.4%), and Rename Variable (8.5%), reflecting a preference for localized improvements over the high-level design changes common in human refactoring. Additionally, the motivations behind agentic refactoring focus overwhelmingly on internal quality concerns, with maintainability (52.5%) and readability (28.1%). Furthermore, quantitative evaluation of code quality metrics shows that agentic refactoring yields small but statistically significant improvements in structural metrics, particularly for medium-level changes, reducing class size and complexity (e.g., Class LOC median Δ = -15.25). Are AI agents the new software architects, or just efficient janitors? This large-scale empirical study analyzes over 15,000 refactoring instances from agents like Codex and Cursor, revealing a stark reality behind the \"autonomous engineer\" hype. While agents are surprisingly active refactorers (explicitly targeting refactoring in 26.1% of commits), the data shows they act conservatively: dominating in low-level consistency edits (like renaming and type changes) while shying away from the complex, high-level structural changes that humans prioritize. Crucially, while they improve basic code metrics, they fail to meaningfully reduce actual design smells. If agents are only polishing the surface without tackling architectural debt, are we ready to trust them as autonomous teammates, or do they need better tools to see the \"big picture\"? This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend ·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2546074",
    "title": "Agentic Refactoring: An Empirical Study of AI Coding Agents",
    "authors": [
      "Hao Li"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2511.04824",
    "upvote": 4
  }
}