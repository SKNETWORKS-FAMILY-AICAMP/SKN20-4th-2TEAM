{
  "context": "PhysWorld integrates video generation and physical world modeling to enable accurate robotic manipulation from visual demonstrations without real robot data. We introduce PhysWorld, a framework that enables robot learning from video\ngeneration throughphysical world modeling. Recentvideo generationmodels can\nsynthesize photorealistic visual demonstrations from language commands and\nimages, offering a powerful yet underexplored source of training signals for\nrobotics. However, directly retargeting pixel motions from generated videos to\nrobots neglects physics, often resulting in inaccurate manipulations. PhysWorld\naddresses this limitation by couplingvideo generationwith physical world\nreconstruction. Given a single image and a task command, our method generatestask-conditioned videosand reconstructs the underlying physical world from the\nvideos, and the generated video motions are grounded into physically accurate\nactions throughobject-centric residual reinforcement learningwith the\nphysical world model. This synergy transforms implicit visual guidance intophysically executable robotic trajectories, eliminating the need for real robot\ndata collection and enablingzero-shot generalizable robotic manipulation.\nExperiments on diverse real-world tasks demonstrate that PhysWorld\nsubstantially improves manipulation accuracy compared to previous approaches.\nVisit https://pointscoder.github.io/PhysWorld_Web/{the project webpage}\nfor details. We introduce PhysWorld, a framework that enables robot learning from video generation through physical world modeling. Recent video generation models can synthesize photorealistic visual demonstrations from language commands and images, offering a powerful yet underexplored source of training signals for robotics. However, directly retargeting pixel motions from generated videos to robots neglects physics, often resulting in inaccurate manipulations. PhysWorld addresses this limitation by coupling video generation with physical world reconstruction. Given a single image and a task command, our method generates task-conditioned videos and reconstructs the underlying physical world from the videos, and the generated video motions are grounded into physically accurate actions through object-centric residual reinforcement learning with the physical world model. This synergy transforms implicit visual guidance into physically executable robotic trajectories, eliminating the need for real robot data collection and enabling zero-shot generalizable robotic manipulation. Experiments on diverse real-world tasks demonstrate that PhysWorld substantially improves manipulation accuracy compared to previous approaches. Visit \\href{this https URL}{the project webpage} for details. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2546026",
    "title": "Robot Learning from a Physical World Model",
    "authors": [],
    "publication_year": 2025,
    "github_url": "https://github.com/PointsCoder/OpenReal2Sim",
    "huggingface_url": "https://huggingface.co/papers/2511.07416",
    "upvote": 30
  }
}