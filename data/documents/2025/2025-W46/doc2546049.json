{
  "context": "ResearchRubrics is a benchmark for evaluating deep research agents, using expert rubrics to assess their factual grounding, reasoning, and clarity across diverse, complex tasks. Deep Research (DR) is an emerging agent application that leverageslarge language models(LLMs) to address open-ended queries. It requires the integration of several capabilities, includingmulti-step reasoning,cross-document synthesis, and the generation ofevidence-backed,long-form answers. Evaluating DR remains challenging because responses are lengthy and diverse, admit many valid solutions, and often depend on dynamic information sources. We introduceResearchRubrics, a standardized benchmark for DR built with over 2,800+ hours of human labor that pairs realistic, domain-diverse prompts with 2,500+ expert-written, fine-grained rubrics to assessfactual grounding,reasoning soundness, andclarity. We also propose a newcomplexity frameworkfor categorizing DR tasks along three axes:conceptual breadth,logical nesting, andexploration. In addition, we develop human and model-based evaluation protocols that measurerubric adherencefor DR agents. We evaluate several state-of-the-art DR systems and find that even leading agents likeGemini's DRandOpenAI's DRachieve under 68% average compliance with our rubrics, primarily due to missed implicit context and inadequate reasoning about retrieved information. Our results highlight the need for robust, scalable assessment of deep research capabilities, to which end we releaseResearchRubrics(including all prompts, rubrics, and evaluation code) to facilitate progress toward well-justified research assistants. ResearchRubrics introduces a standardized benchmark pairing prompts and rubrics to evaluate deep research agents, with a complexity framework and evaluation protocols uncovering current limitations in rubric compliance. arXiv explained breakdown of this paper ðŸ‘‰https://arxivexplained.com/papers/researchrubrics-a-benchmark-of-prompts-and-rubrics-for-evaluating-deep-research-agents This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend arXiv explained breakdown of this paper ðŸ‘‰https://arxivexplained.com/papers/researchrubrics-a-benchmark-of-prompts-and-rubrics-for-evaluating-deep-research-agents Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2546049",
    "title": "ResearchRubrics: A Benchmark of Prompts and Rubrics For Evaluating Deep Research Agents",
    "authors": [
      "Manasi Sharma"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2511.07685",
    "upvote": 9
  }
}