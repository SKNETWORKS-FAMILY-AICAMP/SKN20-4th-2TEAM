{
  "context": "GroundCUA, a large-scale desktop grounding dataset, enables the development of GroundNext models that achieve state-of-the-art performance in mapping instructions to UI elements with less training data. Building reliable computer-use agents requiresgrounding: accurately\nconnectingnatural language instructionsto the correcton-screen elements.\nWhile large datasets exist for web and mobile interactions, high-quality\nresources fordesktop environmentsare limited. To address this gap, we\nintroduceGroundCUA, a large-scale desktopgroundingdataset built from expert\nhuman demonstrations. It covers 87 applications across 12 categories and\nincludes 56Kscreenshots, with every on-screen element carefully annotated for\na total of over 3.56Mhuman-verified annotations. From these demonstrations, we\ngenerate diverse instructions that capture a wide range of real-world tasks,\nproviding high-quality data for model training. UsingGroundCUA, we develop theGroundNextfamily of models that map instructions to their target UI elements.\nAt both 3B and 7B scales,GroundNextachieves state-of-the-art results across\nfive benchmarks usingsupervised fine-tuning, while requiring less than\none-tenth the training data of prior work.Reinforcement learningpost-training\nfurther improves performance, and when evaluated in an agentic setting on theOSWorld benchmarkusing o3 as planner,GroundNextattains comparable or\nsuperior results to models trained with substantially more data,. These results\ndemonstrate the critical role of high-quality, expert-driven datasets in\nadvancing general-purpose computer-use agents. TL;DR:The largest human-annotated desktop-centric GUI grounding dataset. paper:https://arxiv.org/abs/2511.07332github:https://github.com/ServiceNow/GroundCUA/website:https://groundcua.github.io/model:https://huggingface.co/ServiceNow/GroundNext-7B-V0dataset:https://huggingface.co/datasets/ServiceNow/GroundCUA GroundCUAintroduces the first large-scale, human-demonstrated desktop grounding dataset (56K screenshots, 3.56M UI elements, 700 K instruction pairs) for training computer-use agents. Built on it, theGroundNextmodels (3B & 7B) achieve state-of-the-art grounding performance across desktop, web, and mobile benchmarks, all from real human interactions data. Together, they establish a new foundation for multimodal grounding and practical AI-agent research. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend arXiv explained breakdown of this paper ðŸ‘‰https://arxivexplained.com/papers/grounding-computer-use-agents-on-human-demonstrations arXiv lens breakdown of this paper ðŸ‘‰https://arxivlens.com/PaperView/Details/grounding-computer-use-agents-on-human-demonstrations-2113-7f359dd9 Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2546005",
    "title": "Grounding Computer Use Agents on Human Demonstrations",
    "authors": [
      "Aarash Feizi",
      "Shravan Nayak",
      "Xiangru Jian",
      "Kevin Qinghong Lin",
      "Kaixin Li",
      "Rabiul Awal",
      "Xing Han LÃ¹",
      "Johan Obando-Ceron",
      "Juan A. Rodriguez",
      "Spandana Gella",
      "Sai Rajeswar"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/ServiceNow/GroundCUA/",
    "huggingface_url": "https://huggingface.co/papers/2511.07332",
    "upvote": 105
  }
}