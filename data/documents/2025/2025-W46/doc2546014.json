{
  "context": "A framework called Visual Spatial Tuning (VST) enhances the spatial abilities of Vision-Language Models (VLMs) through progressive training with specialized datasets, achieving state-of-the-art results on spatial benchmarks. Capturing spatial relationships from visual inputs is a cornerstone of\nhuman-like general intelligence. Several previous studies have tried to enhance\nthe spatial awareness ofVision-Language Models(VLMs) by adding extra expert\nencoders, which brings extra overhead and usually harms general capabilities.\nTo enhance the spatial ability in general architectures, we introduce Visual\nSpatial Tuning (VST), a comprehensive framework to cultivateVLMswith\nhuman-like visuospatial abilities, fromspatial perceptionto reasoning. We\nfirst attempt to enhancespatial perceptioninVLMsby constructing a\nlarge-scale dataset termedVST-P, which comprises 4.1 million samples spanning\n19 skills across single views, multiple images, and videos. Then, we presentVST-R, a curated dataset with 135K samples that instruct models to reason in\nspace. In particular, we adopt a progressive training pipeline: supervised\nfine-tuning to build foundational spatial knowledge, followed by reinforcement\nlearning to further improvespatial reasoningabilities. Without the\nside-effect to general capabilities, the proposedVSTconsistently achieves\nstate-of-the-art results on several spatial benchmarks, including 34.8% onMMSI-Benchand 61.2% onVSIBench. It turns out that theVision-Language-Action modelscan be significantly enhanced with the proposed\nspatial tuning paradigm, paving the way for more physically grounded AI. Capturing spatial relationships from visual inputs is a cornerstone of human-like general intelligence. Several previous studies have tried to enhance the spatial awareness of Vision-Language Models (VLMs) by adding extra expert encoders, which brings extra overhead and usually harms general capabilities. To enhance the spatial ability in general architectures, we introduce Visual Spatial Tuning (VST), a comprehensive framework to cultivate VLMs with human-like visuospatial abilities, from spatial perception to reasoning. We first attempt to enhance spatial perception in VLMs by constructing a large-scale dataset termed VST-P, which comprises 4.1 million samples spanning 19 skills across single views, multiple images, and videos. Then, we present VST-R, a curated dataset with 135K samples that instruct models to reason in space. In particular, we adopt a progressive training pipeline: supervised fine-tuning to build foundational spatial knowledge, followed by reinforcement learning to further improve spatial reasoning abilities. Without the side-effect to general capabilities, the proposed VST consistently achieves state-of-the-art results on several spatial benchmarks, including 34.8% on MMSI-Bench and 61.2% on VSIBench. It turns out that the Vision-Language-Action models can be significantly enhanced with the proposed spatial tuning paradigm, paving the way for more physically grounded AI. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2546014",
    "title": "Visual Spatial Tuning",
    "authors": [
      "Rui Yang",
      "Yanwei Li",
      "Xiangtai Li"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/Yangr116/VST",
    "huggingface_url": "https://huggingface.co/papers/2511.05491",
    "upvote": 51
  }
}