{
  "context": "HaluMem, a benchmark for evaluating memory hallucinations in AI systems, identifies and analyzes hallucinations across memory extraction, updating, and question answering stages using large-scale human-AI interaction datasets. Memory systemsare key components that enable AI systems such asLLMsand AI\nagents to achieve long-term learning and sustained interaction. However, during\nmemory storage and retrieval, these systems frequently exhibit memoryhallucinations, includingfabrication,errors,conflicts, andomissions.\nExisting evaluations ofmemory hallucinationsare primarily end-to-end question\nanswering, which makes it difficult to localize the operational stage within\nthe memory system wherehallucinationsarise. To address this, we introduce the\nHallucination in Memory Benchmark (HaluMem), the first operation level\nhallucination evaluation benchmark tailored tomemory systems.HaluMemdefines\nthree evaluation tasks (memory extraction,memory updating, and memory question\nanswering) to comprehensively reveal hallucination behaviors across different\noperational stages of interaction. To support evaluation, we constructuser-centric,multi-turn human-AI interaction datasets,HaluMem-MediumandHaluMem-Long. Both include about 15k memory points and 3.5k multi-type\nquestions. The averagedialogue lengthper user reaches 1.5k and 2.6k turns,\nwithcontext lengthsexceeding 1Mtokens, enabling evaluation ofhallucinationsacross different context scales and task complexities. Empirical studies based\nonHaluMemshow that existingmemory systemstend to generate and accumulatehallucinationsduring the extraction and updating stages, which subsequently\npropagateerrorsto the question answering stage. Future research should focus\non developing interpretable and constrained memory operation mechanisms that\nsystematically suppresshallucinationsand improvememory reliability. We introduceHaluMem, the firstoperation-level benchmarkdesigned to systematically evaluate hallucinations in memory systems of AI agents. Unlike conventional black box QA benchmarks, HaluMem opens up the internal mechanisms of memory processing, tracking how hallucinations arise, propagate, and impact final outputs across three key operations:memory extraction, updating, and question answering. HaluMem decomposes memory workflows into granular, traceable stages. It introduces the‚Äúthree-step hallucination tracing mechanism‚Äù, which monitors hallucinations from information extraction to memory revision and final retrieval. Each step is paired with fine-grained gold annotations, enabling precise hallucination localization and quantitative analysis. By exposing hallucinations at their operational roots, HaluMem transforms memory evaluation from opaque outcome testing into interpretable mechanism analysis, thereby laying the empirical foundation fornext-generation reliable memory systemsfor LLMs. HaluMem, built via a six-stage pipeline blending automation, GPT-4o refinement, and human checks, includes: Together, they model realistic long-term user‚ÄìAI interactions to evaluate bothshort-term reliabilityandlong-term robustness. HaluMem categorizes internal memory points into three interpretable types, enabling targeted evaluation of how hallucinations affect distinct aspects of long-term knowledge: HaluMem automatically generatessix categories of evaluation questions, covering the full spectrum from factual recall to contradiction detection:Basic Fact Recall, Multi-hop Inference, Dynamic Update, Memory Boundary, Generalization & Application, and Memory Conflict. We conducted a comprehensive evaluation of several state-of-the-art memory systems, including Mem0, Memobase, SuperMemory, and Zep, under consistent parameter settings to ensure fair comparison. Evaluations of additional memory systems, such as Memos, will be continuously updated. üåêDataset & Resourcesüß© Hugging Face:IAAR-Shanghai/HaluMemüìÑ Paper:arXiv:2511.03506üíª Code & Benchmark Suite:github.com/MemTensor/HaluMem This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend arXiv explained breakdown of this paper üëâhttps://arxivexplained.com/papers/halumem-evaluating-hallucinations-in-memory-systems-of-agents ¬∑Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2546007",
    "title": "HaluMem: Evaluating Hallucinations in Memory Systems of Agents",
    "authors": [
      "Ding Chen"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/MemTensor/HaluMem",
    "huggingface_url": "https://huggingface.co/papers/2511.03506",
    "upvote": 93
  }
}