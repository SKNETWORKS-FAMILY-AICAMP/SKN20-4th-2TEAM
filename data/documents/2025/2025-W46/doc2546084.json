{
  "context": "TimeSearch-R uses interleaved text-video thinking with GRPO-CSV to optimize temporal search in videos, improving performance on long-form video understanding benchmarks. Temporal searchaims to identify a minimal set of relevant frames from tens of thousands based on a given query, serving as a foundation for accurate long-form video understanding. Existing works attempt to progressively narrow the search space. However, these approaches typically rely on a hand-crafted search process, lacking end-to-end optimization for learning optimal search strategies. In this paper, we propose TimeSearch-R, which reformulatestemporal searchas interleaved text-video thinking, seamlessly integrating searching video clips into the reasoning process throughreinforcement learning(RL). However, applying RL training methods, such asGroup Relative Policy Optimization (GRPO), tovideo reasoningcan result in unsupervised intermediate search decisions. This leads to insufficient exploration of the video content and inconsistent logical reasoning. To address these issues, we introduceGRPO with Completeness Self-Verification (GRPO-CSV), which gathers searched video frames from theinterleaved reasoningprocess and utilizes the same policy model to verify the adequacy of searched frames, thereby improving the completeness ofvideo reasoning. Additionally, we construct datasets specifically designed for the SFT cold-start and RL training of GRPO-CSV, filtering out samples with weaktemporal dependenciesto enhance task difficulty and improvetemporal searchcapabilities. Extensive experiments demonstrate that TimeSearch-R achieves significant improvements ontemporal search benchmarkssuch as Haystack-LVBench and Haystack-Ego4D, as well aslong-form video understanding benchmarkslike VideoMME and MLVU. Notably, TimeSearch-R establishes a new state-of-the-art onLongVideoBenchwith 4.1% improvement over the base modelQwen2.5-VLand 2.0% over the advancedvideo reasoningmodelVideo-R1. Our code is available at https://github.com/Time-Search/TimeSearch-R. TimeSearch-R is a novel framework that learns to actively search for relevant temporal clips through end-to-end reinforcement learning. This paradigm reformulates temporal search as interleaved text-video thinking and learns optimal search strategies directly from data. We propose GRPO-CSV, a novel RL algorithm, which ensures sufficient and accurate video exploration by supervising the intermediate steps of temporal search. To support GRPO-CSV training, we also construct a high-quality video reasoning dataset via a two-stage filtering pipeline, enabling the model to learn correct temporal search processes. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2546084",
    "title": "TimeSearch-R: Adaptive Temporal Search for Long-Form Video Understanding via Self-Verification Reinforcement Learning",
    "authors": [
      "Qizhe Zhang",
      "Yuan Zhang"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/Time-Search/TimeSearch-R",
    "huggingface_url": "https://huggingface.co/papers/2511.05489",
    "upvote": 2
  }
}