{
  "context": "Local inference using small language models on accelerators can accurately handle many real-world queries, significantly reducing demand on centralized cloud infrastructure, as measured by intelligence per watt. Large language model(LLM) queries are predominantly processed by frontier models in centralized cloud infrastructure. Rapidly growing demand strains this paradigm, and cloud providers struggle to scale infrastructure at pace. Two advances enable us to rethink this paradigm:small LMs(<=20B active parameters) now achieve competitive performance to frontier models on many tasks, andlocal accelerators(e.g., Apple M4 Max) run these models at interactive latencies. This raises the question: canlocal inferenceviably redistribute demand from centralized infrastructure? Answering this requires measuring whether local LMs can accurately answer real-world queries and whether they can do so efficiently enough to be practical onpower-constrained devices (i.e., laptops). We proposeintelligence per watt(IPW),task accuracydivided by unit ofpower, as a metric for assessing capability and efficiency oflocal inferenceacross model-accelerator pairs. We conduct a large-scale empirical study across 20+ state-of-the-art local LMs, 8 accelerators, and a representative subset ofLLMtraffic: 1M real-worldsingle-turn chatandreasoning queries. For each query, we measure accuracy,energy,latency, andpower. Our analysis reveals 3 findings. First, local LMs can accurately answer 88.7% ofsingle-turn chatandreasoning querieswith accuracy varying by domain. Second, from 2023-2025,IPWimproved 5.3x and localquery coveragerose from 23.2% to 71.3%. Third,local acceleratorsachieve at least 1.4x lowerIPWthancloud acceleratorsrunning identical models, revealing significant headroom for optimization. These findings demonstrate thatlocal inferencecan meaningfully redistribute demand from centralized infrastructure, withIPWserving as the critical metric for tracking this transition. We release ourIPWprofiling harness for systematic intelligence-per-watt benchmarking. We propose intelligence per watt (IPW)—a metric quantifying intelligence delivered per unit of power consumed—to measure the viability of local AI systems within device constraints. We find that Local LMs already handle 88.7% of single-turn chat and reasoning queries, with local IPW improving 5.3× in 2 years—driven by better models (3.2×) and better accelerators (1.7×). As local IPW improves, a meaningful fraction of workloads can shift from centralized infrastructure to local compute, with IPW serving as the critical metric for tracking this transition. I'm having a hard time projecting the conclusion drawn here \"Assuming perfect query-to-model assignment, oracle routing reduces energy consumption by 80.4%, compute by 77.3%, and cost by 73.8% versus cloud-only deployment to the largest model\" to any realistic scenario because the research was designed around \"single-query inference (batch size = 1), to (1) isolate intrinsic model-accelerator efficiency from system-level serving optimizations and (2) follow standard local inference benchmarking practices.\" Cloud inference NEVER runs at batch size 1 because providers want to eke out as much efficiency as they can; further, cloud providers are incentivized to never let their accelerators go idle due to the cost of running them.  It is much more likely, however, that a local endpoint would go idle (if I have a Ryzen Strix Halo machine running a local inference endpoint, it'll go idle any time I'm not using it).  Therefore the \"reduces energy consumption by 80%\" might be accurate for a one-off, but that statistic is effectively meaningless in the real world. I'd love to see a follow-up comparing best-case scenarios for energy optimized inference; I think it would be much more meaningful to industry forecasting for compute and energy consumption. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend ·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2546055",
    "title": "Intelligence per Watt: Measuring Intelligence Efficiency of Local AI",
    "authors": [
      "J. Wes Griffin"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/HazyResearch/intelligence-per-watt",
    "huggingface_url": "https://huggingface.co/papers/2511.07885",
    "upvote": 7
  }
}