{
  "context": "RLoop, a self-improving framework using iterative policy initialization and Rejection-sampling Fine-Tuning, mitigates overfitting and enhances generalization in Reinforcement Learning for Verifiable Rewards. WhileReinforcement Learning for Verifiable Rewards(RLVR) is powerful for\ntraining large reasoning models, its training dynamics harbor a critical\nchallenge:RL overfitting, where models gain training rewards but lose\ngeneralization. Our analysis reveals this is driven by policy\nover-specialization andcatastrophic forgettingof diverse solutions generated\nduring training. Standard optimization discards this valuable inter-step policy\ndiversity. To address this, we introduceRLoop, a self-improving framework\nbuilt oniterative policy initialization.RLooptransforms the standard\ntraining process into a virtuous cycle: it first uses RL to explore the\nsolution space from a given policy, then filters the successful trajectories to\ncreate an expert dataset. This dataset is used via Rejection-sampling\nFine-Tuning (RFT) to refine the initial policy, creating a superior starting\npoint for the next iteration. This loop ofexplorationandexploitationvia\niterative re-initialization effectively convertstransient policy variationsintorobust performance gains. Our experiments showRLoopmitigates forgetting\nand substantially improves generalization, boosting average accuracy by 9% andpass@32by over 15% compared to vanilla RL. Introducing RLoop ðŸ”„, our new framework to fix overfitting in Reinforcement Learning! In RL, models often hit high training rewards but fail to generalize. We found the cause: \"catastrophic forgetting\" discards diverse, valuable policies learned during training. RLoop solves this by turning the entire RL training process into a self-improvement loop: By iteratively exploring and exploiting, RLoop converts fleeting discoveries into robust, generalizable skills. On math reasoning benchmarks, it delivered a +9% accuracy and +15% pass@32 boost. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2546056",
    "title": "RLoop: An Self-Improving Framework for Reinforcement Learning with\n  Iterative Policy Initialization",
    "authors": [
      "Ge Zhang"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2511.04285",
    "upvote": 7
  }
}