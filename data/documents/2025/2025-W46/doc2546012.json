{
  "context": "LLMs struggle to authentically portray morally ambiguous or villainous characters due to safety alignment, as evidenced by the Moral RolePlay benchmark. Large Language Models(LLMs) are increasingly tasked with creative\ngeneration, including the simulation of fictional characters. However, their\nability to portray non-prosocial, antagonistic personas remains largely\nunexamined. We hypothesize that the safety alignment of modern LLMs creates a\nfundamental conflict with the task of authentically role-playing morally\nambiguous or villainous characters. To investigate this, we introduce the Moral\nRolePlay benchmark, a new dataset featuring a four-levelmoral alignment scaleand a balanced test set for rigorous evaluation. We task state-of-the-art LLMs\nwith role-playing characters from moral paragons to pure villains. Our\nlarge-scale evaluation reveals a consistent, monotonic decline in role-playing\nfidelity as character morality decreases. We find that models struggle most\nwith traits directly antithetical tosafety principles, such as ``Deceitful''\nand ``Manipulative'', often substituting nuanced malevolence with superficial\naggression. Furthermore, we demonstrate that generalchatbot proficiencyis a\npoor predictor of villain role-playing ability, with highly safety-aligned\nmodels performing particularly poorly. Our work provides the first systematic\nevidence of this critical limitation, highlighting a key tension between model\nsafety andcreative fidelity. Our benchmark and findings pave the way for\ndeveloping more nuanced, context-aware alignment methods. Are safety-aligned LLMs too good to truly play villains? ğŸ¤–ğŸ­ğŸ˜ˆ Introducing Moral RolePlay, a balanced dataset with 800 characters across 4 moral levels (Paragons â†’ Flawed â†’ Egoists â†’ Villains), featuring 77 personality traits and rigorous scene contexts. This enables the first large-scale, systematic evaluation of moral persona fidelity in LLMs. ğŸ” Key findings:ğŸ“‰ Role-playing fidelity drops as character morality decreases â€” especially for egoists and villains.ğŸš« Models fail most on traits like \"Deceitful\" and \"Manipulative\", due to safety alignment conflicts.âš ï¸ General chatbot skills â‰  good villain acting. Top Arena models fall short on moral ambiguity.ğŸ§  Explicit reasoning doesn't help much â€” models still sanitize complex antagonism. âœ¨ This work reveals a critical limitation in current alignment approaches â€” models trained to be \"too good\" cannot authentically simulate the full spectrum of human psychology, limiting their utility in creative, educational, and social science applications. ğŸ“ Benchmark:https://github.com/Tencent/DigitalHuman/tree/main/RolePlay_VillainğŸ“ƒ Paper:https://arxiv.org/abs/2511.04962ğŸ“Š Dataset:https://huggingface.co/datasets/Zihao1/Moral-RolePlay/tree/main This paper is really interesting and shows how alignment affects creativity. Since models donâ€™t have feelings or real morals â€” they just follow patterns â€” itâ€™s easy for bad actors to trick them, but at the same time their refusals can frustrate normal users. I think there should be a framework that balances both sides â€” keeping models safe but still allowing more natural and flexible behavior. so we should work on this that will be useful Thank you for your thoughtful comment. Youâ€™re absolutely right â€” while alignment is vital for safety, it can inadvertently constrain creative expression and nuanced role-play. Developing a framework that balances ethical safeguards with expressive flexibility is a crucial next step, especially for educational, creative, and research applications. We agree this is an important direction worth pursuing. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2546012",
    "title": "Too Good to be Bad: On the Failure of LLMs to Role-Play Villains",
    "authors": [
      "Zhaopeng Tu"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/Tencent/digitalhuman/tree/main/RolePlay_Villain",
    "huggingface_url": "https://huggingface.co/papers/2511.04962",
    "upvote": 54
  }
}