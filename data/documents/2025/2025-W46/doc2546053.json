{
  "context": "Reinforcement learning enhances language models' ability to recall hierarchical knowledge without degrading memorized facts, as evidenced by improved performance on structured prompting and deep-retrieval tasks. Reinforcement learning(RL) is often credited with improvinglanguage modelreasoning and generalization at the expense of degrading memorized knowledge. We challenge this narrative by observing that RL-enhanced models consistently outperform their base and supervised fine-tuned (SFT) counterparts on pure knowledge recall tasks, particularly those requiring traversal of hierarchical, structured knowledge (e.g., medical codes). We hypothesize these gains stem not from newly acquired data, but from improvedprocedural skillsin navigating and searching existing knowledge hierarchies within the model parameters. To support this hypothesis, we show thatstructured prompting, which explicitly guides SFTed models through hierarchical traversal, recovers most of the performance gap (reducing 24pp to 7pp onMedConceptsQAforDeepSeek-V3/R1). We further find that while prompting improves final-answer accuracy, RL-enhanced models retain superior ability to recall correct procedural paths on deep-retrieval tasks. Finally our layer-wiseinternal activation analysisreveals that whilefactual representations(e.g., activations for the statement \"code 57.95 refers to urinary infection\") maintain high cosine similarity between SFT and RL models,query representations(e.g., \"what is code 57.95\") diverge noticeably, indicating that RL primarily transforms how models traverse knowledge rather than the knowledge representation itself. Contrary to the common belief that RL degrades memorized knowledge, this paper shows that RL-enhanced LLMs actuallyimprove on knowledge recall tasks, especially when navigating hierarchical structures like medical coding systems. The reason is unexpected: RL appears to teach modelsbetter strategies for searching through their existing knowledgerather than adding new facts. Even when researchers tried to close the gap using structured prompting with SFT models, a7pp difference remained, suggesting RL instills navigation skills that can't be easily replicated through prompting alone. This has real implications for how we evaluate and train future models—if the limiting factor isn'twhatmodels know buthow efficiently they can retrieve it, we may be overlooking a key dimension of capability. It also opens the door to explicitly training models on internal search and traversal tasks, potentiallyunlocking latent knowledge that's already there but poorly accessible. The question becomes:how much untapped capability exists in current models simply because they lack effective retrieval mechanisms? This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend ·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2546053",
    "title": "Reinforcement Learning Improves Traversal of Hierarchical Knowledge in LLMs",
    "authors": [
      "Niloofar Mireshghallah"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2511.05933",
    "upvote": 8
  }
}