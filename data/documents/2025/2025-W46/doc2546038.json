{
  "context": "A fully automated data evolution framework, LoopTool, enhances tool-use capabilities of Large Language Models by iteratively refining data and model through a closed-loop process. AugmentingLarge Language Models(LLMs) with external tools enables them to execute complex, multi-step tasks. However, tool learning is hampered by the static synthetic data pipelines where data generation and model training are executed as two separate, non-interactive processes. This approach fails to adaptively focus on a model's specific weaknesses and allows noisy labels to persist, degrading training efficiency. We introduceLoopTool, a fully automated, model-aware data evolution framework that closes this loop by tightly integrating data synthesis and model training.LoopTooliteratively refines both the data and the model through three synergistic modules: (1)Greedy Capability Probing(GCP) diagnoses the model's mastered and failed capabilities; (2)Judgement-Guided Label Verification(JGLV) uses an open-source judge model to find and correct annotation errors, progressively purifying the dataset; and (3)Error-Driven Data Expansion(EDDE) generates new, challenging samples based on identified failures. This closed-loop process operates within a cost-effective, open-source ecosystem, eliminating dependence on expensive closed-source APIs. Experiments show that our 8B model trained withLoopToolsignificantly surpasses its 32B data generator and achieves new state-of-the-art results on theBFCL-v3andACEBenchbenchmarks for its scale. Our work demonstrates that closed-loop, self-refining data pipelines can dramatically enhance the tool-use capabilities of LLMs. Augmenting Large Language Models (LLMs) with external tools enables them to execute complex, multi-step tasks. However, tool learning is hampered by the static synthetic data pipelines where data generation and model training are executed as two separate, non-interactive processes. This approach fails to adaptively focus on a model's specific weaknesses and allows noisy labels to persist, degrading training efficiency. We introduce LoopTool, a fully automated, model-aware data evolution framework that closes this loop by tightly integrating data synthesis and model training. LoopTool iteratively refines both the data and the model through three synergistic modules: (1) Greedy Capability Probing (GCP) diagnoses the model's mastered and failed capabilities; (2) Judgement-Guided Label Verification (JGLV) uses an open-source judge model to find and correct annotation errors, progressively purifying the dataset; and (3) Error-Driven Data Expansion (EDDE) generates new, challenging samples based on identified failures. This closed-loop process operates within a cost-effective, open-source ecosystem, eliminating dependence on expensive closed-source APIs. Experiments show that our 8B model trained with LoopTool significantly surpasses its 32B data generator and achieves new state-of-the-art results on the BFCL-v3 and ACEBench benchmarks for its scale. Our work demonstrates that closed-loop, self-refining data pipelines can dramatically enhance the tool-use capabilities of LLMs. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2546038",
    "title": "LoopTool: Closing the Data-Training Loop for Robust LLM Tool Calls",
    "authors": [],
    "publication_year": 2025,
    "github_url": "https://github.com/Rednote-ExperienceAI-Lab/LoopTool",
    "huggingface_url": "https://huggingface.co/papers/2511.09148",
    "upvote": 16
  }
}