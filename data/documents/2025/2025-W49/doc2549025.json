{
  "context": "Integrating reasoning mechanisms into image editing models enhances performance by improving instruction understanding and result correction. Recent advances inimage editingmodels have shown remarkable progress. A common architectural design couples amultimodal large language model(MLLM) encoder with adiffusion decoder, as seen in systems such asStep1X-EditandQwen-Image-Edit, where theMLLMencodes both the reference image and the instruction but remains frozen during training. In this work, we demonstrate that unlocking the reasoning capabilities ofMLLMcan further push the boundaries of editing models. Specifically, we explore two reasoning mechanisms, thinking andreflection, which enhanceinstruction understandingandediting accuracy. Based on that, our proposed framework enablesimage editingin a thinking-editing-reflectionloop: thethinking mechanismleverages theworld knowledgeofMLLMto interpret abstract instructions, while thereflectionreviews editing results, automatically corrects unintended manipulations, and identifies the stopping round. Extensive experiments demonstrate that our reasoning approach achieves significant performance gains, with improvements ofImgEdit(+4.3%),GEdit(+4.7%), andKris(+8.2%) when initializing ourDiTfrom theStep1X-Edit(ReasonEdit-S), and also outperforms previous open-source methods on bothGEditandKriswhen integrated withQwen-Image-Edit(ReasonEdit-Q). Recent advances in image editing models have shown remarkable progress. A common architectural design couples a multimodal large language model (MLLM) encoder with a diffusion decoder, as seen in systems such as Step1X-Edit and Qwen-Image-Edit, where the MLLM encodes both the reference image and the instruction but remains frozen during training. In this work, we demonstrate that unlocking the reasoning capabilities of MLLM can further push the boundaries of editing models. Specifically, we explore two reasoning mechanisms, thinking and reflection, which enhance instruction understanding and editing accuracy. Based on that, our proposed framework enables image editing in a thinking-editing-reflection loop: the thinking mechanism leverages the world knowledge of MLLM to interpret abstract instructions, while the reflection reviews editing results, automatically corrects unintended manipulations, and identifies the stopping round. Extensive experiments demonstrate that our reasoning approach achieves significant performance gains, with improvements of ImgEdit (+4.3%), GEdit (+4.7%), and Kris (+8.2%) when initializing our DiT from the Step1X-Edit (ReasonEdit-S), and also outperforms previous open-source methods on both GEdit and Kris when integrated with Qwen-Image-Edit (ReasonEdit-Q). This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2549025",
    "title": "REASONEDIT: Towards Reasoning-Enhanced Image Editing Models",
    "authors": [
      "Wei Cheng",
      "Yingming Wang",
      "Zixin Yin",
      "Pengtao Chen",
      "Daxin Jiang",
      "Gang Yu"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/stepfun-ai/Step1X-Edit?tab=readme-ov-file#step1x-edit-v1p2-v12",
    "huggingface_url": "https://huggingface.co/papers/2511.22625",
    "upvote": 46
  }
}