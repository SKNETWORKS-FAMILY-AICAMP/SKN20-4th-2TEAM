{
  "context": "Light-X is a video generation framework that enables controllable rendering from monocular videos with both viewpoint and illumination control, outperforming existing methods in joint camera-illumination control and relighting. Recent advances in illumination control extend image-based methods to video, yet still facing a trade-off between lighting fidelity and temporal consistency. Moving beyond relighting, a key step toward generative modeling of real-world scenes is the joint control of camera trajectory and illumination, since visual dynamics are inherently shaped by both geometry and lighting. To this end, we presentLight-X, a video generation framework that enables controllable rendering from monocular videos with both viewpoint and illumination control. 1) We propose adisentangled designthat decouples geometry and lighting signals: geometry and motion are captured viadynamic point cloudsprojected along user-defined camera trajectories, while illumination cues are provided by arelit frameconsistently projected into the same geometry. These explicit, fine-grained cues enable effective disentanglement and guide high-quality illumination. 2) To address the lack of paired multi-view and multi-illumination videos, we introduceLight-Syn, adegradation-based pipelinewithinverse-mappingthat synthesizes training pairs from in-the-wild monocular footage. This strategy yields a dataset covering static, dynamic, andAI-generated scenes, ensuring robust training. Extensive experiments show thatLight-Xoutperforms baseline methods in joint camera-illumination control and surpasses prior video relighting methods under both text- and background-conditioned settings. Recent advances in illumination control extend image-based methods to video, yet still facing a trade-off between lighting fidelity and temporal consistency. Moving beyond relighting, a key step toward generative modeling of real-world scenes is the joint control of camera trajectory and illumination, since visual dynamics are inherently shaped by both geometry and lighting. To this end, we present Light-X, a video generation framework that enables controllable rendering from monocular videos with both viewpoint and illumination control. 1) We propose a disentangled design that decouples geometry and lighting signals: geometry and motion are captured via dynamic point clouds projected along user-defined camera trajectories, while illumination cues are provided by a relit frame consistently projected into the same geometry. These explicit, fine-grained cues enable effective disentanglement and guide high-quality illumination. 2) To address the lack of paired multi-view and multi-illumination videos, we introduce Light-Syn, a degradation-based pipeline with inverse-mapping that synthesizes training pairs from in-the-wild monocular footage. This strategy yields a dataset covering static, dynamic, and AI-generated scenes, ensuring robust training. Extensive experiments show that Light-X outperforms baseline methods in joint camera-illumination control and surpasses prior video relighting methods under both text- and background-conditioned settings. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2549091",
    "title": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control",
    "authors": [
      "Tianqi Liu",
      "Zihao Huang"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/TQTQliu/Light-X",
    "huggingface_url": "https://huggingface.co/papers/2512.05115",
    "upvote": 10
  }
}