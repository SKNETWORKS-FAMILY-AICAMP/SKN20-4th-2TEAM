{
  "context": "Flash-DMD is a framework that combines efficient timestep-aware distillation and reinforcement learning to accelerate and stabilize the training of generative diffusion models, achieving high generation quality with reduced computational cost. Diffusion Modelshave emerged as a leading class of generative models, yet their iterative sampling process remains computationally expensive.Timestep distillationis a promising technique to accelerate generation, but it often requires extensive training and leads to image quality degradation. Furthermore, fine-tuning these distilled models for specific objectives, such as aesthetic appeal or user preference, usingReinforcement Learning(RL) is notoriously unstable and easily falls into reward hacking. In this work, we introduce Flash-DMD, a novel framework that enables fast convergence with distillation and jointRL-based refinement. Specifically, we first propose an efficient timestep-aware distillation strategy that significantly reduces training cost with enhanced realism, outperforming DMD2 with only 2.1% its training cost. Second, we introduce a joint training scheme where the model is fine-tuned with anRLobjective while thetimestep distillationtraining continues simultaneously. We demonstrate that the stable, well-defined loss from the ongoing distillation acts as a powerful regularizer, effectively stabilizing theRLtraining process and preventingpolicy collapse. Extensive experiments on score-based andflow matching modelsshow that our proposed Flash-DMD not only converges significantly faster but also achieves state-of-the-art generation quality in thefew-step samplingregime, outperforming existing methods invisual quality,human preference, andtext-image alignmentmetrics. Our work presents an effective paradigm for training efficient, high-fidelity, and stable generative models. Codes are coming soon. Diffusion Models have emerged as a leading class of generative models, yet their iterative sampling process remains computationally expensive. Timestep distillation is a promising technique to accelerate generation, but it often requires extensive training and leads to image quality degradation. Furthermore, fine-tuning these distilled models for specific objectives, such as aesthetic appeal or user preference, using Reinforcement Learning (RL) is notoriously unstable and easily falls into reward hacking. In this work, we introduce Flash-DMD, a novel framework that enables fast convergence with distillation and joint RL-based refinement. Specifically, we first propose an efficient timestep-aware distillation strategy that significantly reduces training cost with enhanced realism, outperforming DMD2 with only 2.1% its training cost. Second, we introduce a joint training scheme where the model is fine-tuned with an RL objective while the timestep distillation training continues simultaneously. We demonstrate that the stable, well-defined loss from the ongoing distillation acts as a powerful regularizer, effectively stabilizing the RL training process and preventing policy collapse. Extensive experiments on score-based and flow matching models show that our proposed Flash-DMD not only converges significantly faster but also achieves state-of-the-art generation quality in the few-step sampling regime, outperforming existing methods in visual quality, human preference, and text-image alignment metrics. Our work presents an effective paradigm for training efficient, high-fidelity, and stable generative models. Codes are coming soon. (Samples from 4-step Flash-DMD on SDXL and SD3-Medium. Ours SDXL takes less than 3% training cost of DMD2.) This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2549050",
    "title": "Flash-DMD: Towards High-Fidelity Few-Step Image Generation with Efficient Distillation and Joint Reinforcement Learning",
    "authors": [],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2511.20549",
    "upvote": 25
  }
}