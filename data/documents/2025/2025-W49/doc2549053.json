{
  "context": "RePro, a novel process-level reward mechanism, enhances LLM reasoning by refining the optimization process underlying chain-of-thought prompting, thereby improving performance and reducing suboptimal behaviors. Recent advancements inlarge language models(LLMs) have been driven by their emergent reasoning capabilities, particularly throughlong chain-of-thought(CoT) prompting, which enables thorough exploration and deliberation. Despite these advances, long-CoT LLMs often exhibit suboptimal reasoning behaviors, such as overthinking and excessively protracted reasoning chains, which can impair performance. In this paper, we analyze reasoning processes through an optimization lens, framing CoT as agradient descentprocedure where each reasoning step constitutes an update toward problem resolution. Building on this perspective, we introduceRePro(Rectifying Process-level Reward), a novel approach to refine LLM reasoning during post-training.ReProdefines asurrogate objective functionto assess the optimization process underlying CoT, utilizing adual scoring mechanismto quantify its intensity and stability. These scores are aggregated into acomposite process-level reward, seamlessly integrated intoreinforcement learning with verifiable rewards(RLVR) pipelines to optimize LLMs. Extensive experiments across multiple reinforcement learning algorithms and diverse LLMs, evaluated on benchmarks spanning mathematics, science, and coding, demonstrate thatReProconsistently enhances reasoning performance and mitigates suboptimal reasoning behaviors. RePro (Rectifying Process-level Reward) is a novel post-training framework that aligns Chain-of-Thought (CoT) reasoning with gradient descent optimization principles. While long-CoT prompting facilitates thorough exploration, it frequently results in suboptimal behaviors such as overthinking, hallucination, and inefficient reasoning paths. RePro mitigates these issues by: Empirical evaluations across mathematics, science, and coding benchmarks demonstrate that RePro consistently enhances reasoning accuracy while significantly reducing redundancy. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2549053",
    "title": "Rectifying LLM Thought from Lens of Optimization",
    "authors": [
      "Junnan Liu"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/open-compass/RePro",
    "huggingface_url": "https://huggingface.co/papers/2512.01925",
    "upvote": 24
  }
}