{
  "context": "A small orchestrator using ToolOrchestra method coordinates various intelligent tools with reinforcement learning, achieving higher accuracy and efficiency in solving complex tasks like Humanity's Last Exam compared to larger models. Large language modelsare powerful generalists, yet solving deep and complex problems such as those of theHumanity's Last Exam(HLE) remains both conceptually challenging and computationally expensive. We show that smallorchestrators managing other models and a variety of tools can both push the upper bound of intelligence and improve efficiency in solving difficult agentic tasks. We introduceToolOrchestra, a method for training smallorchestrators that coordinate intelligent tools.ToolOrchestraexplicitly usesreinforcement learningwith outcome-, efficiency-, anduser-preference-aware rewards. UsingToolOrchestra, we produceOrchestrator, an 8B model that achieves higher accuracy at lower cost than previoustool-use agentswhile aligning with user preferences on which tools are to be used for a given query. On HLE,Orchestratorachieves a score of 37.1%, outperforming GPT-5 (35.1%) while being 2.5x more efficient. Ontau2-BenchandFRAMES,Orchestratorsurpasses GPT-5 by a wide margin while using only about 30% of the cost. Extensive analysis shows thatOrchestratorachieves the best trade-off between performance and cost under multiple metrics, and generalizes robustly to unseen tools. These results demonstrate that composing diverse tools with a lightweight orchestration model is both more efficient and more effective than existing methods, paving the way for practical and scalabletool-augmented reasoning systems.  üî• Agent fine-tuning is back‚Äî an 8B orchestrator carries GPT-5, hitting 37.1 on HLEOver the past year, we‚Äôve seen an explosion of ‚ÄúAI agents‚Äù built by chaining tools, APIs, and LLMs together ‚Äî with ever-more-sophisticated routing logic and workflows.But here‚Äôs the problem:Wiring components together ‚â† intelligence.Workflows alone don‚Äôt learn, don‚Äôt adapt, and don‚Äôt optimize. At some point, agents have to learn how to reason, not just be told when to call a model or how to retrieve a document.That‚Äôs where fine-tuning comes back ‚Äî this time, reinvented through reinforcement learning (RL).üß† From orchestration to optimizationWe at NVIDIA Research introduced ToolOrchestra, a framework that uses long-horizon RL to train small models (‚Äúorchestrators‚Äù) to manage big ones.Instead of hand-coded heuristics or fixed workflows, the Orchestrator learns through RL how to decide:‚Üí Which model to use including powerful ones (GPT-5, Claude, etc.)‚Üí When to invoke a tool (search, code interpreter, API call)‚Üí How long to reason before acting The orchestrator is rewarded not just for accuracy, but also for efficiency: balancing accuracy, latency, and cost.This makes it a truly adaptive controller, not just a scripted pipeline.‚öôÔ∏è RL optimizing your workflowThis is a direction the community has barely explored: reinforcement learning applied to orchestration itself.It‚Äôs long-horizon, multi-objective RL ‚Äî optimizing workflows, not just single-step predictions.And the results are striking.Our Orchestrator-8B outperforms frontier LLMs like GPT-5, Claude Opus 4.1, and Llama-3.3-70B on hard reasoning benchmarks (Humanity‚Äôs Last Exam, FRAMES, œÑ¬≤-Bench) while being cheaper and faster. It outperforms GPT-5 on HLE (37.1% v.s. 35.1%) while being 2.5√ó faster and 70% cheaper. üí° ‚ÄúFine-tuning is dead‚Äù? Think again.There‚Äôs been a popular narrative lately ‚Äî that fine-tuning is over, and that prompt engineering or workflow composition are enough.Our work proves that fine-tuning didn‚Äôt die ‚Äî it evolved.Now it‚Äôs RL-based, multi-objective, and long-horizon. ToolOrchestra marks a shift from ‚Äúmonolithic LLMs‚Äù to compound AI systems ‚Äî modular, adaptive, and self-optimizing. üöÄ The rise of compound AI systemsWe‚Äôre entering a new phase of AI system design:‚Üí From monolithic LLMs to compound AI systems ‚Äî modular, adaptive, and self-optimizing.‚Üí From static workflows to RL-trained orchestration.‚Üí From brute-force scale to intelligent coordination.Orchestration is no longer just a systems problem ‚Äî it‚Äôs a learning problem. Paper:https://lnkd.in/grjWkh6JHomepage:https://lnkd.in/gkyiZEaJModel:https://lnkd.in/gBmtif_fData:https://lnkd.in/g2HbQbcgCode:https://lnkd.in/gWF_m6Je This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend arXiv explained breakdown of this paper üëâhttps://arxivexplained.com/papers/toolorchestra-elevating-intelligence-via-efficient-model-and-tool-orchestration arXiv lens breakdown of this paper üëâhttps://arxivlens.com/PaperView/Details/toolorchestra-elevating-intelligence-via-efficient-model-and-tool-orchestration-3205-3f57a860 Hi,I wrote a 4-part deep dive reverse-engineering this paper for practitioners.I loved the paper and would love your feedback even more: Issue 1 (The Algorithm): GRPO mechanics, why the 8B orchestrator needs RL instead of SFT, and the multi-objective reward scalarization Issue 2 (The Factory): The ToolScale synthetic data pipeline and why \"the moat is the factory, not the model\" Issue 3 (The Behavior): Emergent coordination patterns the Escalation Ladder, the \"Give Up\" signal, and Context Saturation risk Issue 4 (The Reality): Production failure modes, breakeven analysis (orchestration pays off above ~75K queries/month), and a CTO decision tree Key finding: ToolOrchestra proves the Compound AI thesis an 8B model + GRPO + specialized tools beats GPT-5 on its own benchmarks at 30% of the cost. But the operational complexity tax is significant. Each post has a video tl;dr, a deep explainer podcast (press the audio toggle next to the video, highly recommended) and the post lies somewhere inbetween. Its posted here (issue 1):https://lambpetros.substack.com/p/the-orchestration-paradigm-series Wrote it since i was faschinated about the subject and wanted to learn. I would greatly appreciate any feedback. Sicerely,Petros Lamb ¬∑Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2549008",
    "title": "ToolOrchestra: Elevating Intelligence via Efficient Model and Tool Orchestration",
    "authors": [
      "Hongxu Yin"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/NVlabs/ToolOrchestra/",
    "huggingface_url": "https://huggingface.co/papers/2511.21689",
    "upvote": 111
  }
}