{
  "context": "R-Few, a guided Self-Play Challenger-Solver framework, enables stable and controllable model self-evolution with minimal human supervision, achieving performance improvements on math and reasoning benchmarks. AIself-evolutionhas long been envisioned as a path towardsuperintelligence, where models autonomously acquire, refine, and internalize knowledge from their own learning experiences. Yet in practice, unguided self-evolving systems often plateau quickly or even degrade as training progresses. These failures arise from issues such asconcept drift,diversity collapse, andmis-evolution, as models reinforce their own biases and converge toward low-entropy behaviors. To enable models to self-evolve in a stable and controllable manner while minimizing reliance on human supervision, we introduceR-Few, a guidedSelf-Play Challenger-Solverframework that incorporates lightweight human oversight throughin-context groundingandmixed training. At each iteration, the Challenger samples a small set of human-labeled examples to guidesynthetic question generation, while the Solver jointly trains on human and synthetic examples under an online, difficulty-based curriculum. Across math and general reasoning benchmarks,R-Fewachieves consistent and iterative improvements. For example,Qwen3-8B-Baseimproves by +3.0 points overR-Zeroon math tasks and achieves performance on par withGeneral-Reasoner, despite the latter being trained on 20 times more human data.Ablation studiesconfirm the complementary contributions of grounded challenger training and curriculum-based solver training, and further analysis shows thatR-Fewmitigates drift, yielding more stable and controllableco-evolutionary dynamics. AI self-evolution has long been envisioned as a path toward superintelligence, where models autonomously acquire, refine, and internalize knowledge from their own learning experiences. Yet in practice, unguided self-evolving systems often plateau quickly or even degrade as training progresses. These failures arise from issues such as concept drift, diversity collapse, and mis-evolution, as models reinforce their own biases and converge toward low-entropy behaviors. To enable models to self-evolve in a stable and controllable manner while minimizing reliance on human supervision, we introduce R-Few, a guided Self-Play Challenger-Solver framework that incorporates lightweight human oversight through in-context grounding and mixed training. At each iteration, the Challenger samples a small set of human-labeled examples to guide synthetic question generation, while the Solver jointly trains on human and synthetic examples under an online, difficulty-based curriculum. Across math and general reasoning benchmarks, R-Few achieves consistent and iterative improvements. For example, Qwen3-8B-Base improves by +3.0 points over R-Zero on math tasks and achieves performance on par with General-Reasoner, despite the latter being trained on 20 times more human data. Ablation studies confirm the complementary contributions of grounded challenger training and curriculum-based solver training, and further analysis shows that R-Few mitigates drift, yielding more stable and controllable co-evolutionary dynamics. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2549018",
    "title": "Guided Self-Evolving LLMs with Minimal Human Supervision",
    "authors": [
      "Wenhao Yu",
      "Chengsong Huang",
      "Kishan Panaganti"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2512.02472",
    "upvote": 51
  }
}