{
  "context": "DeepSeek-V3.2 introduces DeepSeek Sparse Attention and a scalable reinforcement learning framework, achieving superior reasoning and performance compared to GPT-5 and Gemini-3.0-Pro in complex reasoning tasks. We introduce DeepSeek-V3.2, a model that harmonizes highcomputational efficiencywith superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1)DeepSeek Sparse Attention(DSA): We introduceDSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance inlong-context scenarios. (2) ScalableReinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibitsreasoning proficiencyon par with Gemini-3.0-Pro, achievinggold-medal performancein both the 2025International Mathematical Olympiad(IMO) and theInternational Olympiad in Informatics(IOI). (3) Large-ScaleAgentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments. We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.  Thanks! This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend arXiv explained breakdown of this paper ðŸ‘‰https://arxivexplained.com/papers/deepseek-v32-pushing-the-frontier-of-open-large-language-models arXiv lens breakdown of this paper ðŸ‘‰https://arxivlens.com/PaperView/Details/deepseek-v3-2-pushing-the-frontier-of-open-large-language-models-3101-5e8f0ac5 Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2549002",
    "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models",
    "authors": [
      "Bingxuan Wang",
      "Chaofan Lin"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2512.02556",
    "upvote": 244
  }
}