{
  "context": "LongVT, an end-to-end framework, enhances long video reasoning by interleaving global and local analysis using multimodal tools, outperforming existing methods on challenging benchmarks. Largemultimodal models(LMMs) have shown great potential forvideo reasoningwithtextual Chain-of-Thought. However, they remain vulnerable tohallucinations, especially when processinglong-form videoswhere evidence is sparse and temporally dispersed. Inspired by how humans comprehend long videos - by first skimming globally and then examining relevant clips for details - we introduce LongVT, an end-to-end agentic framework that enables \"Thinking with Long Videos\" via interleaved Multimodal Chain-of-Tool-Thought. Specifically, we exploit LMMs' inherenttemporal groundingability as a nativevideo croppingtool to zoom in on a specific video clip and resample finer-grained video frames. This global-to-local reasoning loop continues until answers are grounded in retrieved visual evidence. Given the scarcity offine-grained question-answering(QA) data for the longvideo reasoningtask, we curate and will release a data suite namedVideoSIAHto facilitate both training and evaluation. Specifically, our training dataset consists of 247.9K samples fortool-integrated cold-start supervised fine-tuning, 1.6K samples foragentic reinforcement learning, and 15.4K samples foragentic reinforcement fine-tuning, respectively. Our evaluation benchmark consists of 1,280 QA pairs that are carefully curated through a semi-automatic data pipeline with human-in-the-loop validation. With a meticulously designed three-stage training strategy and extensive empirical validation, LongVT consistently outperforms existing strong baselines across four challenging long-video understanding and reasoning benchmarks. Our codes, data, and model checkpoints are publicly available at https://github.com/EvolvingLMMs-Lab/LongVT . Project PageÔºöhttps://evolvinglmms-lab.github.io/LongVT/Tech ReportÔºöhttps://arxiv.org/abs/2511.20785Github RepoÔºöhttps://github.com/EvolvingLMMs-Lab/LongVTData and ModelÔºöhttps://huggingface.co/collections/lmms-lab/longvtDemo App:https://huggingface.co/spaces/longvideotool/LongVT-DemoBlog Post:https://www.lmms-lab.com/posts/longvt/ If you find our work helpful, please consider starring the GitHub repository and upvoting this paper. Your support is greatly appreciated! This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend very good jobÔºÅÂÅöÊï∞ÊçÆÁöÑÈÉΩÊòØÂ•Ω‰∫∫ Thanks for the kind words! We firmly believe that open data is the bedrock of AI progress. We are dedicated to full transparency and accessibility to serve as a catalyst for the entire open-source ecosystem. arXiv lens breakdown of this paper üëâhttps://arxivlens.com/PaperView/Details/longvt-incentivizing-thinking-with-long-videos-via-native-tool-calling-8088-c5226c18 arXiv explained breakdown of this paper üëâhttps://arxivexplained.com/papers/longvt-incentivizing-thinking-with-long-videos-via-native-tool-calling ¬∑Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2549004",
    "title": "LongVT: Incentivizing \"Thinking with Long Videos\" via Native Tool Calling",
    "authors": [
      "Zuhao Yang",
      "Sudong Wang",
      "Kaichen Zhang",
      "Keming Wu",
      "Lidong Bing"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/EvolvingLMMs-Lab/LongVT",
    "huggingface_url": "https://huggingface.co/papers/2511.20785",
    "upvote": 182
  }
}