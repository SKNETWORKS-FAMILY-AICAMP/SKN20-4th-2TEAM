{
  "context": "LVLM-based T2I systems exhibit higher social bias compared to non-LVLM models, with system prompts identified as a key factor; FairPro reduces demographic bias without sacrificing alignment. Large vision-language model(LVLM) basedtext-to-image(T2I) systems have become the dominant paradigm in image generation, yet whether they amplifysocial biasesremains insufficiently understood. In this paper, we show that LVLM-based models produce markedly more socially biased images than non-LVLM-based models. We introduce a 1,024 promptbenchmarkspanning four levels oflinguistic complexityand evaluatedemographic biasacross multiple attributes in a systematic manner. Our analysis identifiessystem prompts, the predefined instructions guiding LVLMs, as a primary driver of biased behavior. Throughdecoded intermediate representations,token-probability diagnostics, andembedding-association analyses, we reveal howsystem promptsencode demographic priors that propagate into image synthesis. To this end, we proposeFairPro, a training-free meta-prompting framework that enables LVLMs to self-audit and construct fairness-awaresystem promptsat test time. Experiments on two LVLM-based T2I models, SANA and Qwen-Image, show thatFairProsubstantially reducesdemographic biaswhile preservingtext-image alignment. We believe our findings provide deeper insight into the central role ofsystem promptsin bias propagation and offer a practical, deployable approach for building more socially responsible T2I systems. We introduce:1Ô∏è‚É£ A 1,024-prompt benchmark across 4 linguistic complexity levels2Ô∏è‚É£ Fine-grained, systematic demographic (gender, age, ethnicity, physical appearance) bias diagnostics3Ô∏è‚É£ FairPRO, a training-free meta-prompting framework that enables self-auditing and fairness-aware system prompts üå≥ Project page:https://fairpro-t2i.github.ioüë©üèª‚Äçüíª Github:https://github.com/nahyeonkaty/fairpro This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend ¬∑Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2549104",
    "title": "Aligned but Stereotypical? The Hidden Influence of System Prompts on Social Bias in LVLM-Based Text-to-Image Models",
    "authors": [
      "Kunhee Kim"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/nahyeonkaty/fairpro",
    "huggingface_url": "https://huggingface.co/papers/2512.04981",
    "upvote": 7
  }
}