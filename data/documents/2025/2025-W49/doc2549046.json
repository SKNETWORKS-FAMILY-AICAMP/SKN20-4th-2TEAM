{
  "context": "The proposed Attention Interaction Alignment (AIA) loss improves cross-modal attention and performance in unified multimodal models for image generation and understanding without decoupling. Unifiedmultimodal modelsforimage generationandunderstandingrepresent a significant step toward AGI and have attracted widespread attention from researchers. The main challenge of this task lies in the difficulty in establishing an optimal training paradigm due to inherent conflicting targets inunderstandingand generation tasks. To alleviate these conflicts and pursue higher performance, many researchers adopt varying degrees of model decoupling (e.g., Double image encoders, MOE/MOT architecture, or frozen MLLM). However, excessive model decoupling can lead to the loss of interleave generation ability, undermining the original intent of unified models. In this work, we aim to explore how to mitigate task conflicts without resorting to model decoupling. Firstly, we analyze why decoupling alleviates conflicts by studying thecross-modal attentionbehavior of models. We observe that model decoupling essentially drives models towardtask-specific multimodal interactionpatterns, as seen inQwen-VLandHunyuanImage, and that the more thorough the decoupling, the more consistent the behavior becomes. Motivated by this observation, we proposeAttention Interaction Alignment (AIA) loss, which explicitly learnsTask-Specific multimodal interactionpatterns during training. To demonstrate the generalizability of our AIA loss, we apply it toEmu3andJanus-ProduringSFTandpost-training stagerespectively. Without bells and whistles, AIA not only refinescross-modal attentionpatterns, but also boosts both generation andunderstandingperformance. Homepage:https://zhengdian1.github.io/AIA-project/Code:https://github.com/zhengdian1/AIA This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Very interesting idea! A few questions for my understanding: Hi, thank you for your attention and for pointing out our typos. First, you are correct that $N$ is unnecessary; this was a writing error on our part. As for $K$, the formulation is correct. Taking image generation as an example, we aim to calculate the attention of each generated image token towards all text tokens. Therefore, we only need to sum them up (since the attention scores are processed by softmax and range from 0 to 1). Second, we believe that the conflict between understanding and generation tasks is reflected in the cross-modal interactions within the network. For instance, generation involves creating images based on text, while understanding involves generating text based on images. Thus, observing these cross-modal interaction patterns allows us to gain insights into their underlying mechanisms. Finally, the color description in the caption of Figure 5 is indeed flipped. Thank you for pointing this out. Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2549046",
    "title": "Architecture Decoupling Is Not All You Need For Unified Multimodal Model",
    "authors": [
      "Dian Zheng",
      "Hongyu Li",
      "Kaituo Feng",
      "Yexin Liu"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/zhengdian1/AIA",
    "huggingface_url": "https://huggingface.co/papers/2511.22663",
    "upvote": 29
  }
}