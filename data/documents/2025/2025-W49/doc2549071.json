{
  "context": "InternVideo-Next uses a two-stage pretraining scheme with an Encoder-Predictor-Decoder framework to achieve state-of-the-art video representation learning by combining pixel-level fidelity and high-level semantics. Large-scale video-text pretraining achieves strong performance but depends on noisy, synthetic captions with limited semantic coverage, often overlooking implicitworld knowledgesuch as object motion, 3D geometry, and physical cues. In contrast,masked video modeling(MVM) directly exploitsspatiotemporal structuresbut trails text-supervised methods on general tasks. We find this gap arises from overlooked architectural issues: pixel-level reconstruction struggles with convergence and its low-level requirement often conflicts with semantics, while latent prediction often encouragesshortcut learning. To address these, we disentangle the traditionalencoder-decoder designinto anEncoder-Predictor-Decoder(EPD) framework, where the predictor acts as alatent world model, and propose InternVideo-Next, a two-stage pretraining scheme that builds a semantically consistent yet detail-preserving latent space for this world model. First, conventional linear decoder in pixel MVM enforces the predictor output latent to be linearly projected to, thus separable in pixel space, causing the conflict withsemantic abstraction. Our Stage 1 proposes aconditional diffusion decoderand injects reliableimage-level semantic priorsto enhance semantics and convergence, thus bridging pixel-level fidelity with high-levelsemantic abstraction. Stage 2 further learnsworld knowledgeby predicting frozen Stage 1 targets within this space, mitigatingshortcut learning. Trained on public, unlabeled videos, InternVideo-Next achieves state-of-the-art results across benchmarks and provides a scalable path toward generalvideo representation learning. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2549071",
    "title": "InternVideo-Next: Towards General Video Foundation Models without Video-Text Supervision",
    "authors": [],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2512.01342",
    "upvote": 16
  }
}