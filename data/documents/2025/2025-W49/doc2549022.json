{
  "context": "PretrainZero is a reinforcement active learning framework that enhances general reasoning capabilities by pretraining large models on a corpus without verifiable labels, improving performance on benchmarks compared to domain-specific training. Mimicking human behavior to actively learning from general experience and achieve artificial general intelligence has always been a human dream. Recentreinforcement learning(RL) based large-thinking models demonstrate impressive expert-level abilities, i.e., software and math, but still rely heavily on verifiable rewards in specific domains, placing a significant bottleneck to extend the performance boundary of general reasoning capabilities. In this work, we propose PretrainZero, a reinforcementactive learningframework built on the pretraining corpus to extendRLfrom domain-specific post-training to general pretraining. PretrainZero features the following characteristics: 1)Active pretraining: inspired by theactive learningability of humans, PretrainZero learns a unified reasoning policy to actively identify reasonable and informative contents from pretraining corpus, and reason to predict these contents byRL. 2)Self-supervised learning: without any verifiable labels,pretrained reward models, or supervised fine-tuning, we directly pretrain reasoners from 3 to 30B base models on the general Wikipedia corpus usingRL, significantly breaking the verification data-wall for general reasoning. 3) Verification scaling: by tackling increasingly challenging masked spans, PretrainZero substantially enhances the general reasoning abilities of pretrained base models. In reinforcement pretraining, PretrainZero improvesQwen3-4B-Basefor 8.43, 5.96 and 10.60 onMMLU-Pro,SuperGPQAandmath average benchmarks. In post-training, the pretrained models can also serve as reasoning foundation models for downstreamRLVR tasks. Mimicking human behavior to actively learning from general experience and achieve artificial general intelligence has always been a human dream. Recent reinforcement learning (RL) based large-thinking models demonstrate impressive expert-level abilities, i.e., software and math, but still rely heavily on verifiable rewards in specific domains, placing a significant bottleneck to extend the performance boundary of general reasoning capabilities. In this work, we propose PretrainZero, a reinforcement active learning framework built on the pretraining corpus to extend RL from domain-specific post-training to general pretraining. PretrainZero features the following characteristics: 1) Active pretraining: inspired by the active learning ability of humans, PretrainZero learns a unified reasoning policy to actively identify reasonable and informative contents from pretraining corpus, and reason to predict these contents by RL. 2) Self-supervised learning: without any verifiable labels, pretrained reward models, or supervised fine-tuning, we directly pretrain reasoners from 3 to 30B base models on the general Wikipedia corpus using RL, significantly breaking the verification data-wall for general reasoning. 3) Verification scaling: by tackling increasingly challenging masked spans, PretrainZero substantially enhances the general reasoning abilities of pretrained base models. In reinforcement pretraining, PretrainZero improves Qwen3-4B-Base for 8.43, 5.96 and 10.60 on MMLU-Pro, SuperGPQA and math average benchmarks. In post-training, the pretrained models can also serve as reasoning foundation models for downstream RLVR tasks. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend arXiv explained breakdown of this paper ðŸ‘‰https://arxivexplained.com/papers/pretrainzero-reinforcement-active-pretraining Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2549022",
    "title": "PretrainZero: Reinforcement Active Pretraining",
    "authors": [
      "Xingrun Xing",
      "Zhiyuan Fan",
      "Debing Zhang"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2512.03442",
    "upvote": 47
  }
}