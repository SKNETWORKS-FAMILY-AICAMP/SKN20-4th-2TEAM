{
  "context": "SpeContext leverages a distilled language model for efficient long-context reasoning, reducing parameters and improving throughput with minimal accuracy loss in both cloud and edge environments. In this paper, we point out that the objective of theretrieval algorithmsis to align with theLLM, which is similar to the objective ofknowledge distillationinLLMs. We analyze the similarity in information focus between thedistilled language model(DLM) and the originalLLMfrom the perspective of information theory, and thus propose a novel paradigm that leverages aDLMas the retrieval algorithm. Based on the insight, we present SpeContext, an algorithm and system co-design forlong-context reasoning. (1) At the algorithm level, SpeContext proposeslightweight retrieval headbased on thehead-level attention weightsofDLM, achieving > 90% parameters reduction by pruning the redundancy. (2) At the system level, SpeContext designs anasynchronous prefetch dataflowvia theelastic loading strategy, effectively overlappingKV cache retrievalwith theLLMcomputation. (3) At the compilation level, SpeContext constructs the theoretical memory model and implements anadaptive memory managementsystem to achieve acceleration by maximizingGPU memory utilization. We deploy and evaluate SpeContext in two resourceconstrained environments, cloud and edge. Extensive experiments show that, compared with theHuggingface framework, SpeContext achieves up to 24.89x throughput improvement in cloud and 10.06x speedup in edge with negligible accuracy loss, pushing the Pareto frontier of accuracy and throughput. SpeContext accpeted by ASPLOS'26, which is the third paper in the Spec series (SpecEE [ISCA'25], SpecDiff [AAAI'25 Oral]). We apply speculative methods to sparse contexts and demonstrate from an information theory perspective that model distillation indirectly enables small models to learn the context importance focus of the original LLM. Therefore, we use the distilled small model to predict the important contexts of original LLM in advance. In resource-constrained cloud and edge scenarios, we have respectively achieved up to  22x throughput improvement and  10x speedup. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2549074",
    "title": "SpeContext: Enabling Efficient Long-context Reasoning with Speculative Context Sparsity in LLMs",
    "authors": [],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2512.00722",
    "upvote": 15
  }
}