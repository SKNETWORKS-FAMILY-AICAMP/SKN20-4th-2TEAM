{
  "context": "A new video generation model, TV2TV, integrates text and video generation using a Mixture-of-Transformers to improve visual quality and controllability by leveraging language modeling for high-level reasoning. Video generation modelsare rapidly advancing, but can still struggle with complex video outputs that require significant semantic branching or repeated high-level reasoning about what should happen next. In this paper, we introduce a new class ofomni video-text modelsthat integrate ideas from recent LM reasoning advances to address this challenge. More specifically, we presentTV2TV, aunified generative modeling frameworkwhich decomposes video generation into an interleavedtext and video generation process.TV2TVjointly learnslanguage modeling(next-token prediction) andvideo flow matching(next-frame prediction) using aMixture-of-Transformers(MoT) architecture. At inference time,TV2TVdecides when to alternate between generating text and video frames, allowing the model to \"think in words\" about subsequent content before ``acting in pixels'' to produce frames. This design offloads much of the responsibility for deciding what should happen next to thelanguage modeling tower, enabling improvedvisual qualityandprompt alignmentof generated videos. It also enables fine-grainedcontrollability, allowing users to modify the video generation trajectory through text interventions at any point in the process. In controlled experiments onvideo game data,TV2TVdemonstrates substantial improvements in bothvisual qualityandcontrollability.TV2TValso scales tonatural videos, as we show by augmenting sports videos with interleaved natural language action descriptions usingvision-language models(VLMs). TrainingTV2TVon this corpus yields strongvisual qualityandprompt alignment, showcasing the model's ability to reason about and generate complex real-world action sequences. Together, these results highlightTV2TVas a promising step toward video generation with open-endedtextual reasoningand control. Video generation models are rapidly advancing, but can still struggle with complex video outputs that require significant semantic branching or repeated high-level reasoning about what should happen next. In this paper, we introduce a new class of omni video-text models that integrate ideas from recent LM reasoning advances to address this challenge. More specifically, we present TV2TV, a unified generative modeling framework which decomposes video generation into an interleaved text and video generation process. TV2TV jointly learns language modeling (next-token prediction) and video flow matching (next-frame prediction) using a Mixture-of-Transformers (MoT) architecture. At inference time, TV2TV decides when to alternate between generating text and video frames, allowing the model to \"think in words\" about subsequent content before ``acting in pixels'' to produce frames. This design offloads much of the responsibility for deciding what should happen next to the language modeling tower, enabling improved visual quality and prompt alignment of generated videos. It also enables fine-grained controllability, allowing users to modify the video generation trajectory through text interventions at any point in the process. In controlled experiments on video game data, TV2TV demonstrates substantial improvements in both visual quality and controllability. TV2TV also scales to natural videos, as we show by augmenting sports videos with interleaved natural language action descriptions using vision-language models (VLMs). Training TV2TV on this corpus yields strong visual quality and prompt alignment, showcasing the model's ability to reason about and generate complex real-world action sequences. Together, these results highlight TV2TV as a promising step toward video generation with open-ended textual reasoning and control. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2549065",
    "title": "TV2TV: A Unified Framework for Interleaved Language and Video Generation",
    "authors": [
      "Xiaochuang Han",
      "John Nguyen",
      "Karthik Padthe",
      "Maha Elbayad",
      "Shang-Wen Li",
      "Marjan Ghazvininejad"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2512.05103",
    "upvote": 18
  }
}