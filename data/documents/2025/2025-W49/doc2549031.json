{
  "context": "LFM2, a family of compact foundation models, achieves high efficiency and performance on-device through hardware-in-the-loop architecture search and advanced training techniques, supporting various tasks including multimodal applications. We present LFM2, a family ofLiquid Foundation Modelsdesigned for efficient on-device deployment and strong task capabilities. Usinghardware-in-the-loop architecture searchunder edge latency and memory constraints, we obtain a compact hybrid backbone that combinesgated short convolutionswith a small number ofgrouped query attention blocks, delivering up to 2x fasterprefillanddecodeonCPUscompared to similarly sized models. The LFM2 family covers 350M-8.3B parameters, includingdense models(350M, 700M, 1.2B, 2.6B) and amixture-of-expertsvariant (8.3B total, 1.5B active), all with 32K context length. LFM2's training pipeline includes atempered,decoupled Top-K knowledge distillationobjective that avoids support mismatch;curriculum learningwith difficulty-ordered data; and a three-stage post-training recipe ofsupervised fine-tuning,length-normalized preference optimization, andmodel merging. Pre-trained on 10-12T tokens, LFM2 models achieve strong results across diverse benchmarks; for example, LFM2-2.6B reaches 79.56% onIFEvaland 82.41% onGSM8K. We further build multimodal and retrieval variants: LFM2-VL forvision-language tasks, LFM2-Audio for speech, and LFM2-ColBERT for retrieval. LFM2-VL supports tunable accuracy-latency tradeoffs viatoken-efficient visual processing, while LFM2-Audio separates audio input and output pathways to enablereal-timespeech-to-speech interactioncompetitive with models 3x larger. LFM2-ColBERT provides alow-latency encoderfor queries and documents, enabling high-performance retrieval across multiple languages. All models are released with open weights and deployment packages forExecuTorch,llama.cpp, andvLLM, making LFM2 a practical base for edge applications that need fast, memory-efficient inference and strong task capabilities. All models are available on Hugging Face:https://huggingface.co/LiquidAI/ This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend arXiv explained breakdown of this paper ðŸ‘‰https://arxivexplained.com/papers/lfm2-technical-report Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2549031",
    "title": "LFM2 Technical Report",
    "authors": [
      "Arthur BÃ¶Ã¶k",
      "Fernando Fernandes",
      "Yuri Khrustalev",
      "Zetian Li"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2511.23404",
    "upvote": 41
  }
}