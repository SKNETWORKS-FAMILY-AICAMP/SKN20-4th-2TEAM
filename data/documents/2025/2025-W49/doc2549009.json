{
  "context": "The paper provides a theoretical foundation for optimizing sequence-level rewards in reinforcement learning using token-level objectives, highlighting the importance of techniques like importance sampling correction, clipping, and Routing Replay for stabilizing training, especially with large language models. This paper proposes a novel formulation forreinforcement learning(RL) withlarge language models, explaining why and under what conditions the truesequence-level rewardcan be optimized via a surrogatetoken-level objectiveinpolicy gradient methodssuch asREINFORCE. Specifically, through afirst-order approximation, we show that this surrogate becomes increasingly valid only when both thetraining-inference discrepancyandpolicy stalenessare minimized. This insight provides a principled explanation for the crucial role of several widely adopted techniques in stabilizing RL training, includingimportance sampling correction,clipping, and particularlyRouting ReplayforMixture-of-Experts(MoE) models. Through extensive experiments with a 30B MoE model totaling hundreds of thousands of GPU hours, we show that foron-policy training, the basic policy gradient algorithm withimportance sampling correctionachieves the highest training stability. Whenoff-policy updatesare introduced to accelerate convergence, combiningclippingandRouting Replaybecomes essential to mitigate the instability caused bypolicy staleness. Notably, once training is stabilized, prolonged optimization consistently yields comparable final performance regardless of cold-start initialization. We hope that the shared insights and the developed recipes for stable RL training will facilitate future research. From the simple and intuitive perspective of \"first-order approximation\", we formulate and explain the rationale behind optimizing sequence-level rewards using token-level objectives, and highlight that the validity of this approximation requires minimizing both the \"train-inference gap\" and \"policy staleness\". Our formulation provides a principled explanation: stabilization techniques such as importance sampling (IS) correction, clipping, and Routing Replay all fundamentally serve to maintain the validity of this first-order approximation. We conduct extensive experiments using a 30B MoE model (over Ã—00,000 GPU hours, with FP8 inference and BF16 training), which strongly validate the above predictions and help us identify effective recipes for stable RL training. In particular, we demonstrate that as long as training remains stable over the long term, different cold-start initializations consistently converge to similar performance levels. We firmly believe that stability is the key to scaling reinforcement learning! This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend From a garage in Mexico: How your research became our pivot pointHi Qwen Team.Iâ€™m writing this simply to express my profound gratitude for your paper \"Stabilizing Reinforcement Learning with LLMs\".I am a 20-year-old founder and CS student from Monterrey, Mexico. Iâ€™m currently building Eidon, a startup still in its \"garage phase\". To give you an idea of how much this resonated with me: I discovered your technology yesterday morning (Dec 16). It is now 4:29 AM on Dec 17, and I haven't stopped reading, analyzing, and realizing the massive opportunity you've created. I literally canâ€™t sleep seeing the doors that have been opened.We were initially focused on simple restaurant automation, but we hit a wall when considering more complex, critical tasks due to the instability of training models on local hardware. Your findings on MiniRL and Routing Replay (R2/R3) were the breakthrough we needed.Reading about \"Cold-Start Independence\" was the pivotal moment that convinced me to shift our entire focus toward Industrial Edge AI. It proved that we don't need massive compute clusters to build specialized, high-reasoning agents for the manufacturing sector here.You haven't just published a paper; you've empowered a young team in Latin America to aim much higher. We are building our future stack on top of your insights.Thank you for sharing this work with the community.Best regards,Sebastian GrajalesFounder, Eidon Monterrey, Mexico arXiv lens breakdown of this paper ðŸ‘‰https://arxivlens.com/PaperView/Details/stabilizing-reinforcement-learning-with-llms-formulation-and-practices-2271-7e5c71b5 Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2549009",
    "title": "Stabilizing Reinforcement Learning with LLMs: Formulation and Practices",
    "authors": [
      "Chujie Zheng",
      "Huiqiang Jiang",
      "An Yang"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2512.01374",
    "upvote": 96
  }
}