{
  "context": "SCALE selectively allocates computational resources for large language models based on sub-problem difficulty, improving performance and resource utilization. Test-time compute scaling has emerged as a powerful paradigm for enhancing mathematical reasoning inlarge language models(LLMs) by allocating additionalcomputational resourcesduring inference. However, current methods employ uniform resource distribution across all reasoning sub-problems, creating fundamental bottlenecks where challenging sub-problems receive insufficient attention while routine operations consume disproportionate resources. This uniform allocation creates performance bottlenecks where additionalcomputational resourcesyield diminishing returns. Inspired by dual-process theory, we proposeSCALE(Selective Resource Allocation), a framework that selectively allocatescomputational resourcesbased on sub-problem difficulty.SCALEoperates through four stages: (1)problem decompositioninto sequential reasoning sub-problems, (2)difficulty assessmentof each sub-problem to distinguish between routine operations and computationally challenging sub-problems, (3) selective processing mode assignment betweenSystem 1for simple sub-problems andSystem 2for complex ones, and (4)sequential executionwithcontext propagation. By concentrating resources on challenging sub-problems while processing routine operations efficiently,SCALEachieves substantial performance improvements with superior resource utilization. Extensive experiments demonstrate thatSCALEsignificantly outperforms uniform scaling baselines, achieving accuracy improvements of up to 13.75 percentage points (57.50% to 71.25% onAIME25) while reducing computational costs by 33%-53%, representing a major advance intest-time scalingthat addresses fundamental limitations of current approaches. accepted to AAAI 2026 This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2549097",
    "title": "SCALE: Selective Resource Allocation for Overcoming Performance Bottlenecks in Mathematical Test-time Scaling",
    "authors": [
      "Yang Xiao"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/XiaoYang66/DualThinking",
    "huggingface_url": "https://huggingface.co/papers/2512.00466",
    "upvote": 9
  }
}