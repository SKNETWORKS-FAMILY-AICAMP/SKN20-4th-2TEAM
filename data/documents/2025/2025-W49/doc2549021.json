{
  "context": "ARM-Thinker, an agentic reward model, uses external tools for verification, improving accuracy and interpretability in complex multimodal reasoning tasks. Reward modelsare critical for aligningvision-language systemswith human preferences, yet current approaches suffer fromhallucination, weakvisual grounding, and an inability to use tools for verification, limiting their reliability on complex multimodal reasoning tasks. We present ARM-Thinker, an A}gentic multimodal Reward Model that autonomously invokesexternal tools(e.g.,image cropping,doc page retrieval) to ground judgments in verifiable evidence, replacing static, non-interactive reward scoring. This enables the model to verify fine-grained visual details, cross-reference multi-page evidence, and validate reasoning claims, which are capabilities absent in existingreward models. We train ARM-Thinker withmulti-stage reinforcement learning, jointly optimizingtool-calling decisionsandjudgment accuracy. To evaluateagentic reward modeling, we introduceARMBench-VL, comprising three benchmarks that assessfine-grained visual grounding(image-level tools),multi-page document understanding(retrieval tools), andinstruction following(text-level verification). ARM-Thinker achieves +16.2% average improvement on reward modeling benchmarks, +9.6% on tool-use tasks, and outperforms baselines onmultimodal mathandlogical reasoningbenchmarks. Our results demonstrate that agentic capabilities significantly enhance both accuracy and interpretability ofreward models. üè† Github Repo:https://github.com/InternLM/ARM-Thinker‚≠êÔ∏è For Agent Evaluation:https://github.com/open-compass/VLMEvalKit/pull/1334(We added a new feature to VLMEvalKit that supports evaluating diverse models within the ARM-Thinker agent flow, including full tool-use and multi-step reasoning.) This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend ¬∑Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2549021",
    "title": "ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning",
    "authors": [
      "Shengyuan Ding",
      "Xinyu Fang",
      "Yuhang Zang"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/InternLM/ARM-Thinker",
    "huggingface_url": "https://huggingface.co/papers/2512.05111",
    "upvote": 47
  }
}