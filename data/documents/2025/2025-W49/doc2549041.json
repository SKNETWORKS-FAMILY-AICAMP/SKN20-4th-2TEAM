{
  "context": "The study identifies key architectural factors and efficient operators to optimize small language models for real-device latency, introducing the Nemotron-Flash family for improved accuracy and efficiency. Efficient deployment ofsmall language models(SLMs) is essential for numerous real-world applications with stringent latency constraints. While previous work on SLM design has primarily focused on reducing the number of parameters to achieve parameter-optimalSLMs, parameter efficiency does not necessarily translate into proportional real-device speed-ups. This work aims to identify the key determinants ofSLMs' real-device latency and offer generalizable principles and methodologies for SLM design and training when real-device latency is the primary consideration. Specifically, we identify two central architectural factors:depth-width ratiosandoperator choices. The former is crucial for small-batch-size latency, while the latter affects both latency and large-batch-size throughput. In light of this, we first study latency-optimaldepth-width ratios, with the key finding that although deep-thin models generally achieve better accuracy under the same parameter budget, they may not lie on the accuracy-latency trade-off frontier. Next, we explore emergingefficient attention alternativesto evaluate their potential as candidate building operators. Using the identified promising operators, we construct anevolutionary search frameworkto automatically discover latency-optimal combinations of these operators within hybridSLMs, thereby advancing the accuracy-latency frontier. In addition to architectural improvements, we further enhance SLM training using aweight normalizationtechnique that enables more effective weight updates and improves final convergence. Combining these methods, we introduce a new family of hybridSLMs, called Nemotron-Flash, which significantly advances the accuracy-efficiency frontier of state-of-the-artSLMs, e.g., achieving over +5.5% average accuracy, 1.3x/1.9x lower latency, and 18.7x/45.6x higher throughput compared to Qwen3-1.7B/0.6B, respectively. ðŸ‘€ Small LMs often look lightweight, but on real GPUs they are not necessarily faster. At NVIDIA Research, we asked:What if small language models were designed around real-world latency rather than parameter count? ðŸš€Releasing Nemotron-FlashNemotron-Flash is a hybrid SLM family designed around real-world latency and trained from scratch at 1B/3B sizes, achieving SOTA accuracy, latency, and throughput. It has been integrated into TRTLLM for production-grade inference with up to 41K tokens/second on a single H100 GPU. ðŸ“ŠModel Performance ðŸ’¡Why is Nemotron-Flash so fast? Paper Link:https://arxiv.org/pdf/2511.18890 HF models: This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2549041",
    "title": "Nemotron-Flash: Towards Latency-Optimal Hybrid Small Language Models",
    "authors": [],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2511.18890",
    "upvote": 32
  }
}