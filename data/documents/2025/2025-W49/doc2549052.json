{
  "context": "WorldMM, a novel multimodal memory agent with episodic, semantic, and visual memory, outperforms existing methods in long video question-answering by adaptively retrieving from multiple temporal scales and memory sources. Recent advances invideo large language modelshave demonstrated strong capabilities in understanding short clips. However, scaling them to hours- or days-long videos remains highly challenging due to limited context capacity and the loss of critical visual details during abstraction. Existingmemory-augmented methodsmitigate this by leveragingtextual summariesof video segments, yet they heavily rely on text and fail to utilizevisual evidencewhen reasoning over complex scenes. Moreover, retrieving from fixed temporal scales further limits their flexibility in capturing events that span variable durations. To address this, we introduce WorldMM, a novelmultimodal memory agentthat constructs and retrieves from multiple complementary memories, encompassing both textual and visual representations. WorldMM comprises three types of memory:episodic memoryindexes factual events across multiple temporal scales,semantic memorycontinuously updates high-level conceptual knowledge, andvisual memorypreserves detailed information about scenes. During inference, anadaptive retrieval agentiteratively selects the most relevant memory source and leverages multiple temporal granularities based on the query, continuing until it determines that sufficient information has been gathered. WorldMM significantly outperforms existing baselines across fivelong video question-answering benchmarks, achieving an average 8.4% performance gain over previous state-of-the-art methods, showing its effectiveness on long video reasoning. We presentWorldMM, a novel dynamic multimodal memory agent designed for long video reasoning. It constructs multimodal, multi-scale memories that capture both textual and visual information, and employs adaptive retrieval across multiple memories with reasoning. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2549052",
    "title": "WorldMM: Dynamic Multimodal Memory Agent for Long Video Reasoning",
    "authors": [
      "Woongyeong Yeo",
      "Jaehong Yoon"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/wgcyeo/WorldMM",
    "huggingface_url": "https://huggingface.co/papers/2512.02425",
    "upvote": 24
  }
}