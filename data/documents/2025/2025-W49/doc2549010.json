{
  "context": "A benchmark for chained text-to-multi-image generation assesses models' ability to model dynamic causal processes and world knowledge, revealing that unified multimodal models outperform specialized ones but still struggle with spatiotemporal consistency. Currentmultimodal modelsaim to transcend the limitations of single-modality representations by unifying understanding and generation, often usingtext-to-image (T2I)tasks to calibrate semantic consistency. However, their reliance on static, single-image generation in training and evaluation leads to overfitting to static pattern matching and semantic fusion, while fundamentally hindering their ability to model dynamic processes that unfold over time. To address these constraints, we proposeEnvision-acausal event progressionbenchmark for chained text-to-multi-image generation. Grounded in world knowledge and structured byspatiotemporal causality, it reorganizes existing evaluation dimensions and includes 1,000 four-stage prompts spanning six scientific and humanities domains. To transition evaluation from single images to sequential frames and assess whether models truly internalize world knowledge while adhering to causal-temporal constraints, we introduceEnvision-Score, a holistic metric integratingmulti-dimensional consistency,physicality, andaesthetics. Comprehensive evaluation of 15 models (10 specialized T2I models, 5 unified models) uncovers: specialized T2I models demonstrate proficiency in aesthetic rendering yet lack intrinsic world knowledge. Unifiedmultimodal modelsbridge this gap, consistently outperforming specialized counterparts incausal narrative coherence. However, even these unified architectures remain subordinate to closed-source models and struggle to overcome the core challenge ofspatiotemporal consistency. This demonstrates that a focus on causally-isolated single images impedesmulti-frame reasoningand generation, promoting static pattern matching overdynamic world modeling-ultimately limiting world knowledge internalization, generation. Current multimodal models excel at static image generation but struggle to capture the dynamic, causal processes that define real-world events. While text-to-image (T2I) benchmarks assess semantic consistency and aesthetic quality, they often overlook the temporal and causal reasoning required for simulating event progression. To bridge this gap, we introduce Envision, a novel benchmark designed to evaluate modelsâ€™ ability to generate coherent multi-image sequences that reflect causal, spatiotemporal processes grounded in world knowledge. Envision shifts the evaluation paradigm from single-image generation to text-to-multi-image (T2MI) synthesis, requiring models to produce a sequence of four images that depict a logical event progression across six domains: Physics, Chemistry, Biology, Geography, Meteorology, and History & Culture. Each sequence is structured around causal continuityâ€”whether continuous (smooth transitions) or discrete (temporal leaps)â€”challenging models to internalize and apply world knowledge dynamically. Welcome everyone to upvote and star. Thank you very much! This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend  arXiv lens breakdown of this paper ðŸ‘‰https://arxivlens.com/PaperView/Details/envision-benchmarking-unified-understanding-generation-for-causal-world-process-insights-2442-577a6aea Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2549010",
    "title": "Envision: Benchmarking Unified Understanding & Generation for Causal World Process Insights",
    "authors": [
      "Juanxi Tian",
      "Cheng Tan"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/opendatalab-raiser/Envision",
    "huggingface_url": "https://huggingface.co/papers/2512.01816",
    "upvote": 88
  }
}