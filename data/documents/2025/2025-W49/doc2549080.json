{
  "context": "SignRoundV2, a post-training quantization framework, achieves competitive accuracy for Large Language Models at extremely low-bit quantization through layer-wise bit allocation and pre-tuning scale search. Extremelow-bit quantizationis critical for efficiently deployingLarge Language Models(LLMs), yet it often leads to severe performance degradation at 2-bits and even 4-bits (e.g., MXFP4). We presentSignRoundV2, apost-training quantizationframework that is highly effective even withoutmixed-precision.SignRoundV2introduces (1) a fast sensitivity metric that combinesgradient informationwithquantization-induced deviationsto guidelayer-wise bit allocation, and (2) a lightweightpre-tuningsearch for quantization scales to improve extremelylow-bit quantization. These components allowSignRoundV2to close the gap with full-precision models. Extensive experiments indicate that our method sustains competitive accuracy for LLMs, achievingproduction-grade performancewith about 1 percent variance at 4-5 bits and strong results even at 2 bits. The implementation is available at https://github.com/intel/auto-round. Extremely low-bit quantization for LLMs. Check outhttps://github.com/intel/auto-round This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2549080",
    "title": "SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs",
    "authors": [
      "Wenhua Cheng",
      "Heng Guo",
      "Haihao Shen"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/intel/auto-round",
    "huggingface_url": "https://huggingface.co/papers/2512.04746",
    "upvote": 13
  }
}