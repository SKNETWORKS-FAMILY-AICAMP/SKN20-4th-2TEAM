{
  "context": "Double Interactive Reinforcement Learning (DIRL) enables Vision Language Models (VLMs) to coordinate multiple tools for precise spatial reasoning, achieving state-of-the-art performance on benchmarks and real-world tasks. Vision Language Models(VLMs) demonstrate strong qualitative visual understanding, but struggle with metrically precise spatial reasoning required for embodied applications. Theagentic paradigmpromises thatVLMscan use a wide variety of tools that could augment these capabilities, such asdepth estimators,segmentation models, andpose estimators. Yet it remains an open challenge how to realize this vision without solely relying on handcrafted prompting strategies or enforcing fixed, predefined tool pipelines that limitVLMs' ability to discover optimal tool-use patterns.Reinforcement Learningcould overcome this gap, but has so far been limited to reasoning with a single visual tool due to the large search space in multi-tool reasoning. We introduce Double InteractiveReinforcement Learning(DIRL), a two-phase training framework whereVLMslearn to coordinate multiple tools through interactive exploration and feedback. In theteaching phase, we combine demonstrations from a single tool specialist trained via interactive RL with traces from a frontier model using all tools. In theexploration phase, the model further refines multi-tool coordination through continued RL. Our model,SpaceTools, with tool-augmented spatial reasoning ability, achieves state-of-the-art performance on spatial understanding benchmarks (RoboSpatial-Home,BLINK,BOP-ASK) and demonstrates reliablereal-world manipulationusing a7-DOF robotas a tool.DIRLprovides substantial improvements over the vanilla SFT (+12% on RoboSpatial) and RL (+16% on RoboSpatial) baselines. Project page: https://spacetools.github.io/. TL;DR: SpaceTools empowers VLMs with vision and robotic tools for spatial reasoning via Double Interactive Reinforcement Learning (DIRL), enabled by our Toolshed infrastructure. Achieves state-of-the-art performance on spatial reasoning benchmarks and enables precise real-world robot manipulation. Project page:https://spacetools.github.io/  This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2549059",
    "title": "SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL",
    "authors": [
      "Siyi Chen",
      "Chan Hee Song",
      "Faisal Ladhak",
      "Valts Blukis"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/spacetools/SpaceTools",
    "huggingface_url": "https://huggingface.co/papers/2512.04069",
    "upvote": 21
  }
}