{
  "context": "A comprehensive guide to code LLMs, covering their lifecycle from data curation to deployment, including techniques, trade-offs, and research-practice gaps. Large language models (LLMs) have fundamentally transformed automated software development by enabling direct translation of natural language descriptions into functional code, driving commercial adoption through tools like Github Copilot (Microsoft), Cursor (Anysphere), Trae (ByteDance), andClaudeCode (Anthropic). While the field has evolved dramatically from rule-based systems toTransformer-based architectures, achieving performance improvements from single-digit to over 95\\% success rates on benchmarks likeHumanEval. In this work, we provide a comprehensive synthesis and practical guide (a series of analytic and probing experiments) about code LLMs, systematically examining the complete model life cycle from data curation to post-training through advancedprompting paradigms,code pre-training,supervised fine-tuning,reinforcement learning, andautonomous coding agents. We analyze the code capability of the general LLMs (GPT-4,Claude,LLaMA) and code-specialized LLMs (StarCoder,Code LLaMA,DeepSeek-Coder, andQwenCoder), critically examining the techniques, design decisions, and trade-offs. Further, we articulate the research-practice gap between academic research (e.g., benchmarks and tasks) and real-world deployment (e.g.,software-related code tasks), includingcode correctness,security,contextual awarenessof large codebases, and integration with development workflows, and map promising research directions to practical needs. Last, we conduct a series of experiments to provide a comprehensive analysis ofcode pre-training,supervised fine-tuning, andreinforcement learning, coveringscaling law,framework selection,hyperparameter sensitivity,model architectures, anddataset comparisons. üìú Paper OverviewThis is an authoritative survey on code large language models. Jointly authored by dozens of leading institutions worldwide, it systematically outlines the complete technical pathway for code intelligence, from foundational models to agent applications. ‚è≥ Evolutionary ContextThe paper divides programming evolution into six stages: from manual coding and tool-assisted development to framework-driven and AI-assisted (current stage), with prospects for AI-driven and AI-autonomous programming in the future. üß¨ Core Framework Foundation Models: Compares general-purpose LLMs with specialized code models, detailing the full lifecycle training from data preprocessing to reinforcement learning. Evaluation System: Covers diverse task benchmarks ranging from code completion to repository-level development. Capability Optimization: Focuses on reinforcement learning with verifiable rewards and multimodal code generation. Engineering Agents: Explores autonomous agent systems operating across the software development lifecycle. Safety & Governance: Proposes a full-chain risk framework covering data auditing to runtime monitoring. Practical Guidelines: Provides specific configurations and optimization strategies for model training. üî≠ Key Insights A gap exists between current research benchmarks and industrial practice. Long-context understanding and agent collaboration are critical for advancement. The demand for code safety and compliance is increasingly prominent. üí° Value PropositionThis work serves as both a systematic summary of field development and a practical guide bridging academic research with industrial implementation, offering a clear technical roadmap for the further advancement of code intelligence. My work's been hacked. This is a lot of my last 2 years work. My name's Matt Goodhand 81. Please get ahold of me. Oh, my electronic down at the moment. I'm had a lot of problems I need help –ü—Ä–∏–≤–µ—Ç –¥—Ä–∞—Ç—É—Ç–∏ –ß—Ç–æ –∏–Ω—Ç–µ—Ä–µ—Å—Ç–Ω–æ–≥–æ Can't open the PDF.Please repair the file or provide a valid link. i can open it,https://arxiv.org/pdf/2511.18538, please check your network. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend It's truly remarkable that the organizer who brought together so many authors from various institutions exists. Why not to create HTML version`s? it is very strange in the training recipes for code llm.  in the chart, \\alpha_n & \\alpha_d is smaller, however the paper say that the python with highest \\alpha_n & \\alpha_d? arXiv lens breakdown of this paper üëâhttps://arxivlens.com/PaperView/Details/from-code-foundation-models-to-agents-and-applications-a-practical-guide-to-code-intelligence-6776-5ce8cd6a ¬∑Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2549001",
    "title": "From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence",
    "authors": [
      "Jian Yang",
      "Jiajun Wu",
      "Wei Zhang",
      "Terry Yue Zhuo"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2511.18538",
    "upvote": 282
  }
}