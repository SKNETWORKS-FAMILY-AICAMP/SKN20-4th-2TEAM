{
  "context": "Z-Image, a 6B-parameter Scalable Single-Stream Diffusion Transformer (S3-DiT) model, achieves high-performance image generation with reduced computational cost, offering sub-second inference and compatibility with consumer hardware. The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Image, Hunyuan-Image-3.0 and FLUX.2, are characterized by massive parameter counts (20B to 80B), making them impractical for inference, and fine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon aScalable Single-Stream Diffusion Transformer(S3-DiT) architecture that challenges the \"scale-at-all-costs\" paradigm. By systematically optimizing the entire model lifecycle -- from a curated data infrastructure to a streamlined training curriculum -- we complete the full training workflow in just 314KH800 GPUhours (approx. $630K). Our few-stepdistillation schemewithreward post-trainingfurther yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-gradeH800 GPUand compatibility with consumer-grade hardware (<16GBVRAM). Additionally, ouromni-pre-trainingparadigm also enables efficient training of Z-Image-Edit, an editing model with impressiveinstruction-following capabilities. Both qualitative and quantitative experiments demonstrate that our model achieves performance comparable to or surpassing that of leading competitors across various dimensions. Most notably, Z-Image exhibits exceptional capabilities inphotorealistic image generationandbilingual text rendering, delivering results that rival top-tier commercial models, thereby demonstrating that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly, yet state-of-the-art generative models. GitHub:https://github.com/Tongyi-MAI/Z-ImageModelScope:https://modelscope.ai/models/Tongyi-MAI/Z-Image-Turbo/summaryHuggingFace:https://huggingface.co/Tongyi-MAI/Z-Image-TurboZ-Image gallery :https://modelscope.cn/studios/Tongyi-MAI/Z-Image-GalleryComfyUI:https://huggingface.co/Comfy-Org/z_image_turbo This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Hi, and thanks for the work on Z-Image â€” Iâ€™ve been having a lot of fun with it. I wanted to clarify something about adapter support: Does Z-Image currently support embedding / textual-inversionâ€“style adapters (i.e., token-based adapters), or does it only support LoRA-style adapters at the moment? I'm trying to understand which adapter types the Z-Image architecture can make use of now, and which types might be possible in the future. If I train a new token embedding inQwen3-4Bfor a novel concept â€” for example<glimmerwolf>â€” using text like: A glimmerwolf is a luminous wolf-like creature with crystalline fur that glows softly in the dark, similar to bioluminescent jellyfish or frosted crystal. Glimmerwolves behave like normal wolves but leave shimmering mist trails as they move. Does Z-Image only receive the final learned embedding vector for<glimmerwolf>at inference time, or does the diffusion model benefit in any way from the semantic components(e.g.,wolf + glow + crystal + mist) that shaped that embedding during training? In other words: Is Z-Image conditioned solely on the resulting vector produced by Qwen3-4B, or does the model inherit any of the conceptual decomposition used during embedding training? Thanks! æ‚¨å¥½ï¼Œéå¸¸æ„Ÿè°¢ Z-Image é¡¹ç›®çš„å‡ºè‰²å·¥ä½œâ€”â€”æˆ‘ä¸€ç›´åœ¨ä½¿ç”¨å®ƒï¼Œä½“éªŒéå¸¸æ„‰å¿«ã€‚ æˆ‘æƒ³æ¾„æ¸…å…³äºé€‚é…å™¨ï¼ˆadapterï¼‰æ”¯æŒæ–¹é¢çš„ä¸€äº›é—®é¢˜ï¼š Z-Image ç›®å‰æ˜¯å¦æ”¯æŒ embedding / textual-inversionï¼ˆæ–‡æœ¬åæ¼”ï¼‰ç±»å‹çš„é€‚é…å™¨ï¼ˆå³åŸºäº token çš„é€‚é…å™¨ï¼‰ï¼Œè¿˜æ˜¯ç›®å‰ä»…æ”¯æŒ LoRA ç±»å‹çš„é€‚é…å™¨ï¼Ÿ æˆ‘å¸Œæœ›äº†è§£ Z-Image æ¶æ„ç›®å‰èƒ½å¤Ÿä½¿ç”¨å“ªäº›é€‚é…å™¨ç±»å‹ï¼Œä»¥åŠæœªæ¥å¯èƒ½æ”¯æŒå“ªäº›ç±»å‹ã€‚ å¦‚æœæˆ‘åœ¨Qwen3-4Bä¸­ä¸ºä¸€ä¸ªå…¨æ–°çš„æ¦‚å¿µè®­ç»ƒä¸€ä¸ªæ–°çš„ token embeddingï¼Œä¾‹å¦‚<glimmerwolf>ï¼Œå¹¶ä½¿ç”¨å¦‚ä¸‹æ–‡æœ¬è¿›è¡Œè®­ç»ƒï¼š Glimmerwolfï¼ˆå¾®å…‰ç‹¼ï¼‰æ˜¯ä¸€ç§å…·æœ‰å‘å…‰ç‰¹æ€§çš„ç‹¼çŠ¶ç”Ÿç‰©ï¼Œå®ƒçš„æ™¶ä½“çŠ¶çš®æ¯›ä¼šåœ¨é»‘æš—ä¸­æŸ”å’Œåœ°å‘å…‰ï¼Œç±»ä¼¼äºå‘å…‰æ°´æ¯æˆ–ç£¨ç ‚æ™¶ä½“ã€‚å¾®å…‰ç‹¼çš„è¡Œä¸ºç±»ä¼¼æ™®é€šç‹¼ï¼Œä½†åœ¨ç§»åŠ¨æ—¶ä¼šç•™ä¸‹é—ªçƒçš„é›¾çŠ¶è½¨è¿¹ã€‚ é‚£ä¹ˆåœ¨æ¨ç†é˜¶æ®µï¼š Z-Image æ˜¯å¦åªä¼šæ¥æ”¶åˆ°<glimmerwolf>æœ€ç»ˆè®­ç»ƒå¾—åˆ°çš„ embedding å‘é‡ï¼Ÿè¿˜æ˜¯è¯´æ‰©æ•£æ¨¡å‹ä¹Ÿä¼šä»è®­ç»ƒè¯¥ embedding æ—¶æ‰€ä½¿ç”¨çš„è¯­ä¹‰æˆåˆ†ä¸­å—ç›Šï¼Ÿï¼ˆä¾‹å¦‚ï¼šç‹¼ + å‘å…‰ç‰¹æ€§ + æ™¶ä½“æè´¨ + é›¾æ•ˆï¼‰ æ¢å¥è¯è¯´ï¼š Z-Image æ˜¯ä»…æ ¹æ® Qwen3-4B è¾“å‡ºçš„æœ€ç»ˆå‘é‡è¿›è¡Œæ¡ä»¶åŒ–ï¼Œè¿˜æ˜¯ä¼šç»§æ‰¿ embedding è®­ç»ƒè¿‡ç¨‹ä¸­å½¢æˆçš„è¯­ä¹‰åˆ†è§£ä¿¡æ¯ï¼Ÿ è°¢è°¢ï¼ I found this paper on the website interesting it explains a new way to build image generation models that work fast and could help make tools that run on normal computers arXiv lens breakdown of this paper ğŸ‘‰https://arxivlens.com/PaperView/Details/z-image-an-efficient-image-generation-foundation-model-with-single-stream-diffusion-transformer-9846-b5faf99f Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2549003",
    "title": "Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer",
    "authors": [
      "Huanqia Cai",
      "Sihan Cao",
      "Ruoyi Du",
      "Dengyang Jiang",
      "Xin Jin",
      "Zhen Li",
      "Zhong-Yu Li",
      "Junhan Shi",
      "Qilong Wu",
      "Chi Zhang",
      "Shilin Zhou"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/Tongyi-MAI/Z-Image",
    "huggingface_url": "https://huggingface.co/papers/2511.22699",
    "upvote": 223
  }
}