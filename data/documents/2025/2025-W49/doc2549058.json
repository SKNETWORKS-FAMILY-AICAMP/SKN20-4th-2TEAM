{
  "context": "Adversarial flow models unify adversarial and flow-based generative models, offering stable training, efficient generation, and high performance on image datasets. We presentadversarial flow models, a class ofgenerative modelsthat unifiesadversarial modelsandflow models. Our method supports native one-step or multi-step generation and is trained using the adversarial objective. Unlike traditional GANs, where thegeneratorlearns an arbitrary transport plan between the noise and the data distributions, ourgeneratorlearns a deterministicnoise-to-data mapping, which is the sameoptimal transportas inflow-matching models. This significantly stabilizesadversarial training. Also, unlikeconsistency-based methods, our model directly learnsone-step or few-step generationwithout needing to learn the intermediate timesteps of theprobability flowfor propagation. This saves model capacity, reduces training iterations, and avoids error accumulation. Under the same 1NFE setting onImageNet-256px, our B/2 model approaches the performance of consistency-based XL/2 models, while our XL/2 model creates a new bestFIDof 2.38. We additionally show the possibility ofend-to-end trainingof 56-layer and 112-layer models throughdepth repetitionwithout any intermediate supervision, and achieveFIDs of 2.08 and 1.94 using a singleforward pass, surpassing their 2NFE and 4NFE counterparts. Adversarial Flow Models (AF) unify Adversarial Models and Flow Models. It natively supports single-step or multi-step training and generation. Unlike GANs, which learn arbitrary transport plans, AF learns a deterministic Wasserstein-2 transport plan, the same as flow matching. This allows stable training on a standard transformer architecture. Unlike Consistency Models, AF does not need to be trained on all timesteps for the consistency constraint. This saves model capacity and avoids error propagation. On ImageNet 256px, AF-B/2 can approach the performance of Consistency-XL/2, while AF-XL/2 1NFE achieves a new best FID of 2.38! Unlike Flow Matching, whose MSE objective minimizes the Euclidean distance and causes OOD generation without guidance, AF minimizes a learned discriminator distance, which better represents the semantic distance on the data manifold. AF can significantly surpass Flow Matching in the no-guidance setting. We also demonstrate end-to-end training of 56-layer and 112-layer 1NFE networks. No intermediate supervision. No teacher forcing. No manual timestep discretization. They surpass the 2NFE and 4NFE 28-layer counterparts, achieving a best FID of 1.94! This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2549058",
    "title": "Adversarial Flow Models",
    "authors": [
      "Shanchuan Lin"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/ByteDance-Seed/Adversarial-Flow-Models",
    "huggingface_url": "https://huggingface.co/papers/2511.22475",
    "upvote": 22
  }
}