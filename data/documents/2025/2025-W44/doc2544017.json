{
  "context": "Game-TARS, a generalist game agent trained with a unified action space, achieves superior performance across various domains and benchmarks through large-scale pre-training and efficient reasoning strategies. We present Game-TARS, a generalist game agent trained with a unified,\nscalable action space anchored tohuman-aligned native keyboard-mouse inputs.\nUnlike API- or GUI-based approaches, this paradigm enables large-scale\ncontinual pre-training across heterogeneous domains, including OS, web, and\nsimulation games. Game-TARS is pre-trained on over 500B tokens with diverse\ntrajectories andmultimodal data. Key techniques include a decaying continual\nloss to reducecausal confusionand an efficientSparse-Thinking strategythat\nbalances reasoning depth and inference cost. Experiments show that Game-TARS\nachieves about 2 times the success rate over the previous sota model onopen-world Minecraft tasks, is close to the generality of fresh humans inunseen web 3d games, and outperforms GPT-5, Gemini-2.5-Pro, and Claude-4-Sonnet\ninFPS benchmarks. Scaling results on training-time and test-time confirm that\ntheunified action spacesustains improvements when scaled tocross-gameandmultimodal data. Our results demonstrate that simple, scalable action\nrepresentations combined with large-scale pre-training provide a promising path\ntowardgeneralist agentswith broadcomputer-use abilities. We present Game-TARS, a generalist game agent trained with a unified, scalable action space anchored to human-aligned native keyboard-mouse inputs. Unlike API- or GUI-based approaches, this paradigm enables large-scale continual pre-training across heterogeneous domains, including OS, web, and simulation games. Game-TARS is pre-trained on over 500B tokens with diverse trajectories and multimodal data. Key techniques include a decaying continual loss to reduce causal confusion and an efficient Sparse-Thinking strategy that balances reasoning depth and inference cost. Experiments show that Game-TARS achieves about 2 times the success rate over the previous sota model on open-world Minecraft tasks, is close to the generality of fresh humans in unseen web 3d games, and outperforms GPT-5, Gemini-2.5-Pro, and Claude-4-Sonnet in FPS benchmarks. Scaling results on training-time and test-time confirm that the unified action space sustains improvements when scaled to cross-game and multimodal data. Our results demonstrate that simple, scalable action representations combined with large-scale pre-training provide a promising path toward generalist agents with broad computer-use abilities. Thank you for your attention. Here are our evaluation demos on untrained games. Game-TARS is a versatile game agent capable of playing games across simulators, web platforms, and operating systems using human-compatible devices.  We also test the agent's ability to follow human instructions in open-world Minecraft. Game-TARS has mastered advanced combat skills, possesses good memory for navigation and tracking, and can freely switch and interact between 3D embodied and 2D GUI interfaces through a universal action space.  Find more details in our blog and paper. Blog:https://seed-tars.com/game-tars/Paper:https://arxiv.org/abs/2510.23691 How are the performance on some 3A masterpieces, such as Black Myth: Wu Kong, or some FPS games like CS2, Valorant? Thanks for your comments. We are trying to set up a complete evaluation chain in the Windows OS Sandbox.Please stay tuned. Congratulations on the achievement! Do you have plans to make it open source? Game-TARS is continuously pre-trained based onSeed-VL-1.5, which is a proprietary commercial model.We have a plan to reproduce it on an open-source model if it can meet our performance requirements. I was waiting for such an AI! Is there any planned release date? same question! tbh this model looks very great Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2544017",
    "title": "Game-TARS: Pretrained Foundation Models for Scalable Generalist\n  Multimodal Game Agents",
    "authors": [
      "Zihao Wang",
      "Haoming Wang",
      "Jiazhan Feng",
      "Zhongkai Zhao"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2510.23691",
    "upvote": 53
  }
}