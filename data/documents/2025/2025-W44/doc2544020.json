{
  "context": "Supervised Reinforcement Learning (SRL) enhances small-scale LLMs' multi-step reasoning by generating internal monologues and using step-wise expert actions for richer learning signals, outperforming SFT and RLVR. Large Language Models (LLMs) often struggle with problems that require\nmulti-step reasoning. For small-scale open-source models, Reinforcement\nLearning with Verifiable Rewards (RLVR) fails when correct solutions are rarely\nsampled even after many attempts, whileSupervised Fine-Tuning(SFT) tends to\noverfit long demonstrations through rigid token-by-token imitation. To address\nthis gap, we proposeSupervised Reinforcement Learning(SRL), a framework that\nreformulates problem solving as generating a sequence of logical \"actions\". SRL\ntrains the model to generate aninternal reasoning monologuebefore committing\nto each action. It provides smoother rewards based on the similarity between\nthe model's actions andexpert actionsextracted from the SFT dataset in a\nstep-wise manner. This supervision offers richer learning signals even when all\nrollouts are incorrect, while encouraging flexible reasoning guided by expert\ndemonstrations. As a result, SRL enables small models to learn challenging\nproblems previously unlearnable by SFT or RLVR. Moreover, initializing training\nwith SRL before refining with RLVR yields the strongest overall performance.\nBeyond reasoning benchmarks, SRL generalizes effectively to agentic software\nengineering tasks, establishing it as a robust and versatile training framework\nfor reasoning-oriented LLMs. Large Language Models (LLMs) often struggle with problems that require multi-step reasoning. Forsmall-scale open-source models, Reinforcement Learning with Verifiable Rewards (RLVR) fails whencorrect solutions are rarely sampled even after many attempts, while Supervised Fine-Tuning (SFT)tends to overfit long demonstrations through rigid token-by-token imitation. To address this gap, wepropose Supervised Reinforcement Learning (SRL), a framework that reformulates problem solvingas generating a sequence of logical “actions”. SRL trains the model to generate an internal reasoningmonologue before committing to each action. It provides smoother rewards based on the similaritybetween the model’s actions and expert actions extracted from the SFT dataset in a step-wise manner.This supervision offers richer learning signals even when all rollouts are incorrect, while encouragingflexible reasoning guided by expert demonstrations. As a result, SRL enables small models to learnchallenging problems previously unlearnable by SFT or RLVR. Moreover, initializing training with SRLbefore refining with RLVR yields the strongest overall performance. Beyond reasoning benchmarks, SRLgeneralizes effectively to agentic software engineering tasks, establishing it as a robust and versatiletraining framework for reasoning-oriented LLMs. For a deeper understanding of this paper, you can refer to the arXiv explanation page:https://arxivexplained.com/papers/supervised-reinforcement-learning-from-expert-trajectories-to-step-wise-reasoning Is the dataset they used public? ·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2544020",
    "title": "Supervised Reinforcement Learning: From Expert Trajectories to Step-wise\n  Reasoning",
    "authors": [],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2510.25992",
    "upvote": 45
  }
}