{
  "context": "VITA-E, a dual-model embodied interaction framework, enables concurrent and interruptible vision-language-action capabilities, enhancing real-time user interaction. Current Vision-Language-Action (VLA) models are often constrained by a rigid,\nstatic interaction paradigm, which lacks the ability to see, hear, speak, and\nact concurrently as well as handle real-time user interruptions dynamically.\nThis hinders seamless embodied collaboration, resulting in an inflexible and\nunresponsive user experience. To address these limitations, we introduce\nVITA-E, a novelembodied interaction frameworkdesigned for both behavioral\nconcurrency and nearly real-time interruption. The core of our approach is adual-model architecturewhere two parallel VLA instances operate as an ``Active\nModel'' and a ``Standby Model'', allowing the embodied agent to observe its\nenvironment, listen to user speech, provide verbal responses, and execute\nactions, all concurrently and interruptibly, mimicking human-like multitasking\ncapabilities. We further propose a ``model-as-controller'' paradigm, where we\nfine-tune theVLMto generatespecial tokensthat serve as direct system-level\ncommands, coupling the model's reasoning with the system's behavior.\nExperiments conducted on a physical humanoid platform demonstrate that VITA-E\ncan reliably handle complex interactive scenarios. Our framework is compatible\nwith various dual-system VLA models, achieving an extremely high success rate\nonemergency stopsandspeech interruptionswhile also successfully performingconcurrent speech and action. This represents a significant step towards more\nnatural and capable embodied assistants. We introduceVITA-E‚Äî a natural human-robot interaction framework with the ability to observe, listen, speak, and act simultaneously. üß† Dual-Model Architecture ‚Äì Inspired by brain hemispheres: oneactive modelexecutes tasks while astandby modelmonitors for new instructions.üéØ Model Control Paradigm ‚Äì Fine-tuned VLM generates special tokens as system-level commands for precise, instant control.üó£Ô∏è Seamless Interaction ‚Äì Answer questions mid-task, interrupt actions with voice commands, natural transitions, all in nearly real-time bidirectional dialogue (bilingual: EN/CN).‚ú® Real-World Validated ‚Äì Tested on physical humanoid robots with competitive performance across interaction and manipulation benchmarks. Compatible with mainstream VLA models. üîó Project Page:https://lxysl.github.io/VITA-E/ See More of Our Research VITA-VLA: Exploring distilling Action Expert capabilities into VLM to enhance training efficiency and capabilities. VITA-1.5: An open-source omni model with near real-time video-to-speech capability.  ¬∑Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2544026",
    "title": "VITA-E: Natural Embodied Interaction with Concurrent Seeing, Hearing,\n  Speaking, and Acting",
    "authors": [
      "Yi-Fan Zhang",
      "Xing Sun"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/Tencent/VITA/tree/VITA-E",
    "huggingface_url": "https://huggingface.co/papers/2510.21817",
    "upvote": 41
  }
}