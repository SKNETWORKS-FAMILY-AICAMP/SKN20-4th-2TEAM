{
  "context": "ReForm, a reflective autoformalization method, improves the semantic accuracy of formal statements generated from natural language mathematics through iterative refinement and semantic consistency evaluation. Autoformalization, which translates natural language mathematics into\nmachine-verifiable formal statements, is critical for using formal mathematical\nreasoning to solve math problems stated in natural language. While Large\nLanguage Models can generate syntactically correct formal statements, they\noften fail to preserve the original problem's semantic intent. This limitation\narises from the LLM approaches' treatingautoformalizationas a simplistic\ntranslation task which lacks mechanisms for self-reflection and iterative\nrefinement that human experts naturally employ. To address these issues, we\npropose ReForm, a ReflectiveAutoformalizationmethod that tightly integratessemantic consistency evaluationinto theautoformalizationprocess. This\nenables the model to iteratively generate formal statements, assess its\nsemantic fidelity, and self-correct identified errors through progressive\nrefinement. To effectively train this reflective model, we introduceProspective Bounded Sequence Optimization(PBSO), which employs different\nrewards at different sequence positions to ensure that the model develops both\naccurateautoformalizationand correct semantic validations, preventing\nsuperficial critiques that would undermine the purpose of reflection. Extensive\nexperiments across fourautoformalizationbenchmarks demonstrate that ReForm\nachieves an average improvement of 17.2 percentage points over the strongest\nbaselines. To further ensure evaluation reliability, we introduceConsistencyCheck, a benchmark of 859 expert-annotated items that not only\nvalidates LLMs as judges but also reveals thatautoformalizationis inherently\ndifficult: even human experts produce semantic errors in up to 38.5% of cases. ReFormis a reflectiveAutoformalizationframework that enables LLMs to iteratively generate, validate, and self-correct formal mathematical statements (Lean4) through an integrated generation-validation loop. Reflective Autoformalization Paradigm: Introduces an iterative \"generate → validate → refine\" cycle that enables models to autonomously identify and correct semantic errors, unifying generation and verification in a single process. Prospective Bounded Sequence Optimization (PBSO): A novel RL algorithm designed for heterogeneous rewards at different sequence positions, enabling stable training of models with both accurate autoformalization and reliable semantic validation. ConsistencyCheck Benchmark: 859 expert-annotated items for evaluating semantic consistency, revealing that even human experts produce errors in up to 38.5% of cases. Model:https://huggingface.co/collections/GuoxinChen/reformConsistencyCheck Benchmark:https://huggingface.co/datasets/GuoxinChen/ConsistencyCheckGithub:https://github.com/Chen-GX/ReForm ·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2544073",
    "title": "ReForm: Reflective Autoformalization with Prospective Bounded Sequence\n  Optimization",
    "authors": [
      "Guoxin Chen",
      "Chengxi Li"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/Chen-GX/ReForm",
    "huggingface_url": "https://huggingface.co/papers/2510.24592",
    "upvote": 16
  }
}