{
  "context": "LoopLM, a family of pre-trained Looped Language Models, enhances reasoning by integrating iterative computation and entropy regularization during pre-training, achieving superior performance with better knowledge manipulation. Modern LLMs are trained to \"think\" primarily via explicit text generation,\nsuch aschain-of-thought(CoT), which defers reasoning to post-training and\nunder-leverages pre-training data. We present and open-source Ouro, named after\nthe recursive Ouroboros, a family of pre-trainedLooped Language Models(LoopLM) that instead build reasoning into the pre-training phase through (i)iterative computationinlatent space, (ii) anentropy-regularized objectiveforlearned depth allocation, and (iii) scaling to 7.7T tokens. Ouro 1.4B and\n2.6B models enjoy superior performance that match the results of up to 12B SOTA\nLLMs across a wide range of benchmarks. Through controlled experiments, we show\nthis advantage stems not from increased knowledge capacity, but from superiorknowledge manipulationcapabilities. We also show thatLoopLMyields reasoning\ntraces more aligned with final outputs than explicit CoT. We hope our results\nshow the potential ofLoopLMas a novel scaling direction in the reasoning era.\nOur model could be found in: http://ouro-llm.github.io. Modern LLMs are trained to \"think\" primarily via explicit text generation, such as chain-of-thought (CoT), which defers reasoning to post-training and under-leverages pre-training data. We present and open-source Ouro, named after the recursive Ouroboros, a family of pre-trained Looped Language Models (LoopLM) that instead build reasoning into the pre-training phase through (i) iterative computation in latent space, (ii) an entropy-regularized objective for learned depth allocation, and (iii) scaling to 7.7T tokens. Ouro 1.4B and 2.6B models enjoy superior performance that match the results of up to 12B SOTA LLMs across a wide range of benchmarks. Through controlled experiments, we show this advantage stems not from increased knowledge capacity, but from superior knowledge manipulation capabilities. We also show that LoopLM yields reasoning traces more aligned with final outputs than explicit CoT. We hope our results show the potential of LoopLM as a novel scaling direction in the reasoning era. Very interesting yet logical results. The instability at higher recursions during training is unfortunate, though I suppose it cannot be helped. I wonder if increasing recursion gradually over training would be able to sidestep the issue? Very amazing paper, thank you for publishing! Other similar concepts (such as HRM and TRM) use stop gradients / detached tensors to improve efficiency and gradient stability. Inhttps://arxiv.org/pdf/2502.05171they indeed scale up the recurrent depth a lot higher than 8 by sampling the depth randomly with a heavy tail distribution arXiv explained breakdown of this paper ðŸ‘‰https://arxivexplained.com/papers/scaling-latent-reasoning-via-looped-language-models One question, why do you think we see an improvement when finetuning using thinking? Do you think we could still scale the loops even further or is something in the thinking that couldn't be learned only by increase the loops? This slow the inference,loop only part of model or use two models, one small looped other normal or relu for particular weights computation ,inference speed / model quality ratio is main/essential always.Observed that 2.6B model is shown 10.1B in chatllm ,mean is computed 4 times(or implementation is not well understanded???). @librarian-botrecommend arXiv lens breakdown of this paper ðŸ‘‰https://arxivlens.com/PaperView/Details/scaling-latent-reasoning-via-looped-language-models-7828-faf4e73d Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2544001",
    "title": "Scaling Latent Reasoning via Looped Language Models",
    "authors": [
      "Rui-Jie Zhu",
      "Kai Hua",
      "Tianyu Zhang",
      "Ziniu Li",
      "Boyi Wei",
      "Lu Li",
      "Qiyang Min"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2510.25741",
    "upvote": 221
  }
}