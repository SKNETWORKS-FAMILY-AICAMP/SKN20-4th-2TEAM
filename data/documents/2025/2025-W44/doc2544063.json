{
  "context": "ProMoE, an MoE framework with conditional and prototypical routing, enhances expert specialization in Diffusion Transformers, achieving state-of-the-art performance on ImageNet. Mixture-of-Experts(MoE) has emerged as a powerful paradigm for scaling model\ncapacity while preserving computational efficiency. Despite its notable success\nin large language models (LLMs), existing attempts to applyMoEto Diffusion\nTransformers (DiTs) have yielded limited gains. We attribute this gap to\nfundamental differences between language and visual tokens. Language tokens are\nsemantically dense with pronounced inter-token variation, while visual tokens\nexhibit spatial redundancy and functional heterogeneity, hindering expert\nspecialization in visionMoE. To this end, we presentProMoE, anMoEframework\nfeaturing atwo-step routerwith explicit routing guidance that promotes expert\nspecialization. Specifically, this guidance encourages the router to partition\nimage tokens into conditional and unconditional sets viaconditional routingaccording to their functional roles, and refine the assignments of conditional\nimage tokens throughprototypical routingwithlearnable prototypesbased on\nsemantic content. Moreover, the similarity-based expert allocation in latent\nspace enabled byprototypical routingoffers a natural mechanism for\nincorporating explicit semantic guidance, and we validate that such guidance is\ncrucial for visionMoE. Building on this, we propose arouting contrastive lossthat explicitly enhances theprototypical routingprocess, promotingintra-expert coherenceandinter-expert diversity. Extensive experiments on\nImageNet benchmark demonstrate thatProMoEsurpasses state-of-the-art methods\nunder bothRectified FlowandDDPMtraining objectives. Code and models will be\nmade publicly available. Paper:https://arxiv.org/abs/2510.24711 Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2544063",
    "title": "Routing Matters in MoE: Scaling Diffusion Transformers with Explicit\n  Routing Guidance",
    "authors": [],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2510.24711",
    "upvote": 19
  }
}