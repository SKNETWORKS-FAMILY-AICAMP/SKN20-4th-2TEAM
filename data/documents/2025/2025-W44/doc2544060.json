{
  "context": "ParallelMuse enhances problem-solving by efficiently integrating parallel thinking with deep exploration through uncertainty-guided path reuse and reasoning compression. Parallel thinking expands exploration breadth, complementing the deep\nexploration of information-seeking (IS) agents to further enhance\nproblem-solving capability. However, conventional parallel thinking faces two\nkey challenges in this setting: inefficiency from repeatedly rolling out from\nscratch, and difficulty in integrating long-horizon reasoning trajectories\nduring answer generation, as limited context capacity prevents full\nconsideration of the reasoning process. To address these issues, we propose\nParallelMuse, a two-stage paradigm designed for deep IS agents. The first\nstage,Functionality-Specified Partial Rollout, partitions generated sequences\ninto functional regions and performsuncertainty-guided path reuseand\nbranching to enhance exploration efficiency. The second stage, Compressed\nReasoning Aggregation, exploitsreasoning redundancyto losslessly compress\ninformation relevant to answer derivation and synthesize a coherent final\nanswer. Experiments across multiple open-source agents and benchmarks\ndemonstrate up to 62% performance improvement with a 10--30% reduction in\nexploratory token consumption. Parallel thinking expands exploration breadth, complementing the deep exploration of information-seeking (IS) agents to further enhance problem-solving capability. However, conventional parallel thinking faces two key challenges in this setting: inefficiency from repeatedly rolling out from scratch, and difficulty in integrating long-horizon reasoning trajectories during answer generation, as limited context capacity prevents full consideration of the reasoning process. To address these issues, we propose ParallelMuse, a two-stage paradigm designed for deep IS agents. The first stage, Functionality-Specified Partial Rollout, partitions generated sequences into functional regions and performs uncertainty-guided path reuse and branching to enhance exploration efficiency. The second stage, Compressed Reasoning Aggregation, exploits reasoning redundancy to losslessly compress information relevant to answer derivation and synthesize a coherent final answer. Experiments across multiple open-source agents and benchmarks demonstrate up to 62% performance improvement with a 10--30% reduction in exploratory token consumption. Check our projects athttps://github.com/Alibaba-NLP/DeepResearch Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2544060",
    "title": "ParallelMuse: Agentic Parallel Thinking for Deep Information Seeking",
    "authors": [
      "Yida Zhao"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/Alibaba-NLP/DeepResearch",
    "huggingface_url": "https://huggingface.co/papers/2510.24698",
    "upvote": 20
  }
}