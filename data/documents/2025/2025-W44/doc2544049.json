{
  "context": "Entity-aware Group Relative Policy Optimization (E-GRPO) enhances search agents by incorporating entity information into the reward function, improving accuracy and efficiency in complex, knowledge-intensive tasks. LLM-based search agents are increasingly trained on entity-centric synthetic\ndata to solve complex, knowledge-intensive tasks. However, prevailing training\nmethods likeGroup Relative Policy Optimization (GRPO)discard this rich entity\ninformation, relying instead on sparse, outcome-based rewards. This critical\nlimitation renders them unable to distinguish informative \"near-miss\"\nsamples-those with substantially correct reasoning but a flawed final\nanswer-from complete failures, thus discarding valuable learning signals. We\naddress this by leveraging the very entities discarded during training. Our\nempirical analysis reveals a strong positive correlation between the number of\nground-truth entities identified during an agent's reasoning process and final\nanswer accuracy. Building on this insight, we introduce Entity-aware Group\nRelative Policy Optimization (E-GRPO), a novel framework that formulates a\ndense entity-aware reward function. E-GRPO assigns partial rewards to incorrect\nsamples proportional to theirentity match rate, enabling the model to\neffectively learn from these \"near-misses\". Experiments on diversequestion-answering (QA)anddeep research benchmarksshow that E-GRPO\nconsistently and significantly outperforms the GRPO baseline. Furthermore, our\nanalysis reveals that E-GRPO not only achieves superior accuracy but also\ninduces more efficient reasoning policies that require fewertool calls,\ndemonstrating a more effective and sample-efficient approach to aligning search\nagents. LLM-based search agents are increasingly trained on entity-centric synthetic data to solve complex, knowledge-intensive tasks. However, prevailing training methods like Group Relative Policy Optimization (GRPO) discard this rich entity information, relying instead on sparse, outcome-based rewards. This critical limitation renders them unable to distinguish informative \"near-miss\" samples-those with substantially correct reasoning but a flawed final answer-from complete failures, thus discarding valuable learning signals. We address this by leveraging the very entities discarded during training. Our empirical analysis reveals a strong positive correlation between the number of ground-truth entities identified during an agent's reasoning process and final answer accuracy. Building on this insight, we introduce Entity-aware Group Relative Policy Optimization (E-GRPO), a novel framework that formulates a dense entity-aware reward function. E-GRPO assigns partial rewards to incorrect samples proportional to their entity match rate, enabling the model to effectively learn from these \"near-misses\". Experiments on diverse question-answering (QA) and deep research benchmarks show that E-GRPO consistently and significantly outperforms the GRPO baseline. Furthermore, our analysis reveals that E-GRPO not only achieves superior accuracy but also induces more efficient reasoning policies that require fewer tool calls, demonstrating a more effective and sample-efficient approach to aligning search agents. Check our projects athttps://github.com/Alibaba-NLP/DeepResearch Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2544049",
    "title": "Repurposing Synthetic Data for Fine-grained Search Agent Supervision",
    "authors": [
      "Yida Zhao",
      "Xixi Wu"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/Alibaba-NLP/DeepResearch",
    "huggingface_url": "https://huggingface.co/papers/2510.24694",
    "upvote": 24
  }
}