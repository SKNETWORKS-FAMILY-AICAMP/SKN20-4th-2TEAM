{
  "context": "STAR-Bench measures audio 4D intelligence by evaluating sound dynamics in time and 3D space, revealing gaps in fine-grained perceptual reasoning among existing models. Despite rapid progress inMulti-modal Large Language Modelsand Large\nAudio-Language Models, existingaudio benchmarkslargely test semantics that\ncan be recovered from text captions, masking deficits in fine-grained\nperceptualreasoning. We formalize audio 4D intelligence that is defined asreasoningoversound dynamicsin time and3D space, and introduceSTAR-Benchto\nmeasure it.STAR-Benchcombines aFoundational Acoustic Perceptionsetting (six\nattributes under absolute and relative regimes) with a Holistic Spatio-TemporalReasoningsetting that includessegment reorderingfor continuous and discrete\nprocesses and spatial tasks spanningstatic localization, multi-source\nrelations, anddynamic trajectories. Our data curation pipeline uses two\nmethods to ensure high-quality samples. For foundational tasks, we useprocedurally synthesizedandphysics-simulated audio. For holistic data, we\nfollow a four-stage process that includeshuman annotationand final selection\nbased onhuman performance. Unlike prior benchmarks where caption-only\nanswering reduces accuracy slightly,STAR-Benchinduces far larger drops\n(-31.5\\% temporal, -35.2\\% spatial), evidencing its focus on linguistically\nhard-to-describe cues. Evaluating 19 models reveals substantial gaps compared\nwith humans and a capability hierarchy:closed-source modelsare bottlenecked\nbyfine-grained perception, whileopen-source modelslag across perception,knowledge, andreasoning. OurSTAR-Benchprovides critical insights and a clear\npath forward for developing future models with a more robust understanding of\nthe physical world. Despite rapid progress in Multi-modal Large Language Models and Large Audio-Language Models, existing audio benchmarks largely test semantics that can be recovered from text captions, masking deficits in fine-grained perceptual reasoning. We formalize audio 4D intelligence that is defined as reasoning over sound dynamics in time and 3D space, and introduce STAR-Bench to measure it. STAR-Bench combines a Foundational Acoustic Perception setting (six attributes under absolute and relative regimes) with a Holistic Spatio-Temporal Reasoning setting that includes segment reordering for continuous and discrete processes and spatial tasks spanning static localization, multi-source relations, and dynamic trajectories. Our data curation pipeline uses two methods to ensure high-quality samples. For foundational tasks, we use procedurally synthesized and physics-simulated audio. For holistic data, we follow a four-stage process that includes human annotation and final selection based on human performance. Unlike prior benchmarks where caption-only answering reduces accuracy slightly, STAR-Bench induces far larger drops (-31.5% temporal, -35.2% spatial), evidencing its focus on linguistically hard-to-describe cues. Evaluating 19 models reveals substantial gaps compared with humans and a capability hierarchy: closed-source models are bottlenecked by fine-grained perception, while open-source models lag across perception, knowledge, and reasoning. Our STAR-Bench provides critical insights and a clear path forward for developing future models with a more robust understanding of the physical world. Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2544067",
    "title": "STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D\n  Intelligence",
    "authors": [
      "Yuhang Zang",
      "Jiaqi Wang"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/InternLM/StarBench",
    "huggingface_url": "https://huggingface.co/papers/2510.24693",
    "upvote": 18
  }
}