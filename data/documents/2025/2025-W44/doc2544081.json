{
  "context": "PairUni enhances unified vision-language models by organizing data into understanding-generation pairs and using Pair-GPRO to optimize policy learning, achieving balanced performance improvements. Unified vision-language models(UVLMs) must perform both understanding and\ngeneration within a single architecture, but these tasks rely on heterogeneous\ndata and supervision, making it difficult to balance them during reinforcement\nlearning (RL). We propose PairUni, a unified framework that reorganizes data\ninto understanding-generation (UG) pairs and aligns optimization accordingly.\nWe first useGPT-o3to augment single-task data, generating captions for\nunderstanding samples and question-answer (QA) pairs for generation samples,\nforming aligned pairs from the same instance. Additionally, for each generation\nsample, we retrieve a semantically related understanding example to form a\nretrieved pair, linking different but related data points. These paired\nstructures expose cross-tasksemantic correspondencesand support consistent\npolicy learning. To leverage this structure, we presentPair-GPRO, a pair-aware\nvariant based onGroup Relative Policy Optimization. It assigns a similarity\nscore to each pair to modulate the advantage, strengthening learning from\nwell-aligned examples and reducing task interference. We curate a high-quality\ndataset of 16K UG pairs named PairUG for RL fine-tuning and evaluate PairUni on\nthe powerfulJanus-Pro UVLMs. Our approach achieves balanced improvements on\nvarious UVLMs, outperforming strong UVLM RL baselines. Code:\nhttps://github.com/Haochen-Wang409/PairUni{github.com/Haochen-Wang409/PairUni} Unified vision-language models (UVLMs) must perform both understanding and generation within a single architecture, but these tasks rely on heterogeneous data and supervision, making it difficult to balance them during reinforcement learning (RL). We propose PairUni, a unified framework that reorganizes data into understanding-generation (UG) pairs and aligns optimization accordingly. We first use GPT-o3 to augment single-task data, generating captions for understanding samples and question-answer (QA) pairs for generation samples, forming aligned pairs from the same instance. Additionally, for each generation sample, we retrieve a semantically related understanding example to form a retrieved pair, linking different but related data points. These paired structures expose cross-task semantic correspondences and support consistent policy learning. To leverage this structure, we present Pair-GPRO, a pair-aware variant based on Group Relative Policy Optimization. It assigns a similarity score to each pair to modulate the advantage, strengthening learning from well-aligned examples and reducing task interference. We curate a high-quality dataset of 16K UG pairs named PairUG for RL fine-tuning and evaluate PairUni on the powerful Janus-Pro UVLMs. Our approach achieves balanced improvements on various UVLMs, outperforming strong UVLM RL baselines. Code:https://github.com/Haochen-Wang409/PairUni Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2544081",
    "title": "PairUni: Pairwise Training for Unified Multimodal Language Models",
    "authors": [
      "Xiangtai Li",
      "Haochen Wang"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/Haochen-Wang409/PairUni",
    "huggingface_url": "https://huggingface.co/papers/2510.25682",
    "upvote": 13
  }
}