{
  "context": "RoboOmni, a Perceiver-Thinker-Talker-Executor framework using end-to-end omni-modal LLMs, improves robotic manipulation by inferring user intentions from spoken dialogue, environmental sounds, and visual cues. Recent advances inMultimodal Large Language Models(MLLMs) have driven rapid\nprogress in Vision-Language-Action (VLA) models for robotic manipulation.\nAlthough effective in many scenarios, current approaches largely rely on\nexplicit instructions, whereas in real-world interactions, humans rarely issue\ninstructions directly. Effective collaboration requires robots to infer user\nintentions proactively. In this work, we introduce cross-modal contextual\ninstructions, a new setting where intent is derived from spoken dialogue,\nenvironmental sounds, and visual cues rather than explicit commands. To address\nthis new setting, we present RoboOmni, aPerceiver-Thinker-Talker-Executorframework based onend-to-end omni-modal LLMsthat unifies intention\nrecognition,interaction confirmation, andaction execution. RoboOmni fuses\nauditory and visual signals spatiotemporally for robustintention recognition,\nwhile supporting direct speech interaction. To address the absence of training\ndata forproactive intention recognitionin robotic manipulation, we buildOmniAction, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640\nbackgrounds, and six contextual instruction types. Experiments in simulation\nand real-world settings show that RoboOmni surpasses text- and ASR-based\nbaselines in success rate, inference speed,intention recognition, and\nproactive assistance. RoboOmni: Robot Manipulation in Omni-modal Context ğŸ¤– Robots shouldn't just wait for commands - it's time for them to be PROACTIVE! Introducing RoboOmni: The breakthrough omni-modal embodied foundation model that enables robots to actively serve users through natural human-robot interaction! ğŸš€ ğŸ§  PROBLEM: Current VLAs rely on \"explicit instructions,\" but humans rarely give direct commands in real-world scenarios. Robots need to integrate audio, visual, and contextual cues to infer user intent - what we call \"contextual instructions.\" ğŸ› ï¸ SOLUTION - RoboOmni Features:ğŸ™ï¸ğŸ‘ï¸ Unified omni-modal modeling: Fuses speech, environmental audio, vision & text - NO ASR needed!ğŸ—£ï¸ğŸ¤– Bidirectional interaction: Supports both speech responses AND action executionğŸ”ğŸ¦¾ Closed-loop embodied intelligence: Complete \"perceive-infer-confirm-execute\" pipeline ğŸ“Š RESULTS that speak for themselves:âœ… 85.6% task success rate (vs 25.9% baseline)âš¡ 2.04x faster inference speedğŸ¯ 88.9% intent recognition accuracy (vs 55.6% GPT-4o+ASR) ğŸ“¦ Plus OmniAction dataset: 140K real robot episodes, 5K+ speaker voices, 2.4K environmental sounds across 6 contextual scenarios! ğŸ”— Paper:https://huggingface.co/papers/2510.23763ğŸ’» Code:https://github.com/OpenMOSS/RoboOmniğŸŒ Project:https://OpenMOSS.github.io/RoboOmniğŸ¬ Video:https://www.youtube.com/watch?v=1NCfXxTEMls Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2544016",
    "title": "RoboOmni: Proactive Robot Manipulation in Omni-modal Context",
    "authors": [
      "Siyin Wang",
      "Junhao Shi"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/OpenMOSS/RoboOmni",
    "huggingface_url": "https://huggingface.co/papers/2510.23763",
    "upvote": 53
  }
}