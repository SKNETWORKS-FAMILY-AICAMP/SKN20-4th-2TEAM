{
  "context": "Latent Sketchpad enhances Multimodal Large Language Models with an internal visual scratchpad, enabling generative visual thought and improved reasoning performance. WhileMultimodal Large Language Models(MLLMs) excel at visual understanding,\nthey often struggle in complex scenarios that requirevisual planningandimagination. Inspired by how humans usesketchingas a form ofvisual thinkingto develop and communicate ideas, we introduceLatent Sketchpad, a framework\nthat equips MLLMs with aninternal visual scratchpad. The internal visual\nrepresentations of MLLMs have traditionally been confined to perceptual\nunderstanding. We repurpose them to supportgenerative visual thoughtwithout\ncompromising reasoning ability. Building on frontier MLLMs, our approach\nintegrates visual generation directly into their native autoregressive\nreasoning process. It allows the model to interleave textual reasoning with the\ngeneration ofvisual latents. These latents guide the internal thought process\nand can be translated intosketch imagesfor interpretability. To realize this,\nwe introduce two components: aContext-Aware Vision Headautoregressively\nproduces visual representations, and a pretrainedSketch Decoderrenders these\ninto human-interpretable images. We evaluate the framework on our new datasetMazePlanning. Experiments across various MLLMs show thatLatent Sketchpaddelivers comparable or even superior reasoning performance to their backbone.\nIt further generalizes across distinct frontier MLLMs, includingGemma3andQwen2.5-VL. By extending model's textual reasoning tovisual thinking, our\nframework opens new opportunities for richerhuman-computer interactionand\nbroader applications. More details and resources are available on our project\npage: https://latent-sketchpad.github.io/. Ever wished frontier MLLM could actually “think in images”?While pretrained MLLMs like Gemma3 and Qwen-VL series excel at understanding images, they do not support visual generation natively.  We introduceLatent Sketchpad— a framework that gives MLLMs an internal visual scratchpad for “imagining while reasoning.”It lets modelsgenerate visual latents alongside text tokens, seamlessly integrating visual generation into the autoregressive reasoning process.The design is fullyplug-and-play, enabling visual generationwithout compromising the backbone’s reasoning capability. These internal visual thoughts can then bedecoded into images via our pretrained Sketch Decoderfor interpretability.Our experiments demonstrate that Latent Sketchpad isbroadly applicable across various MLLMs, includingGemma3 and Qwen2.5-VL. We are also releasing our code, pretrained Sketch Decoder weights, and related resources. Paper:https://arxiv.org/abs/2510.24514Project Page:https://latent-sketchpad.github.io/GitHub:https://github.com/hwanyu112/Latent-SketchpadHugging Face:https://huggingface.co/huanyu112/Latent-Sketchpad.Sketch_Decoder ·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2544058",
    "title": "Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal\n  Reasoning in MLLMs",
    "authors": [
      "Li Dong"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/hwanyu112/Latent-Sketchpad",
    "huggingface_url": "https://huggingface.co/papers/2510.24514",
    "upvote": 21
  }
}