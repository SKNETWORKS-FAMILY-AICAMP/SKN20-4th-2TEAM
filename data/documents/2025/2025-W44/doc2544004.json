{
  "context": "Kimi Linear, a hybrid linear attention architecture, outperforms full attention in various scenarios with improved efficiency and performance, using Kimi Delta Attention and Multi-Head Latent Attention. We introduce Kimi Linear, a hybridlinear attentionarchitecture that, for\nthe first time, outperforms full attention under fair comparisons across\nvarious scenarios -- including short-context, long-context, and reinforcement\nlearning (RL) scaling regimes. At its core liesKimi Delta Attention(KDA), an\nexpressivelinear attentionmodule that extendsGated DeltaNetwith a\nfiner-grained gating mechanism, enabling more effective use of limitedfinite-state RNN memory. Our bespokechunkwise algorithmachieves high hardware\nefficiency through a specialized variant of theDiagonal-Plus-Low-Rank(DPLR)\ntransition matrices, which substantially reduces computation compared to the\ngeneral DPLR formulation while remaining more consistent with the classical\ndelta rule.\n  We pretrain a Kimi Linear model with 3B activated parameters and 48B total\nparameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention\n(MLA). Our experiments show that with an identical training recipe, Kimi Linear\noutperforms fullMLAwith a sizeable margin across all evaluated tasks, while\nreducingKV cacheusage by up to 75% and achieving up to 6 times decoding\nthroughput for a 1M context. These results demonstrate that Kimi Linear can be\na drop-in replacement for full attention architectures with superior\nperformance and efficiency, including tasks with longer input and output\nlengths.\n  To support further research, we open-source the KDA kernel andvLLMimplementations, and release the pre-trained and instruction-tuned model\ncheckpoints. We introduce Kimi Linear, a hybrid linear attention architecture that, for the first time, outperforms full attention under fair comparisons across various scenarios -- including short-context, long-context, and reinforcement learning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an expressive linear attention module that extends Gated DeltaNet with a finer-grained gating mechanism, enabling more effective use of limited finite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware efficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR) transition matrices, which substantially reduces computation compared to the general DPLR formulation while remaining more consistent with the classical delta rule.We pretrain a Kimi Linear model with 3B activated parameters and 48B total parameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention (MLA). Our experiments show that with an identical training recipe, Kimi Linear outperforms full MLA with a sizeable margin across all evaluated tasks, while reducing KV cache usage by up to 75% and achieving up to 6 times decoding throughput for a 1M context. These results demonstrate that Kimi Linear can be a drop-in replacement for full attention architectures with superior performance and efficiency, including tasks with longer input and output lengths.To support further research, we open-source the KDA kernel and vLLM implementations, and release the pre-trained and instruction-tuned model checkpoints. Hi, whether the MLA attention layer in the hybrid ratio 0:1 configuration described in Table 1 includes positional embeddings? If not, the 0:1 configuration seems to be unfair  for MLA. @BillionZhengWe don't use any NoPE for any non-hybrid models arXiv explained breakdown of this paper ðŸ‘‰https://arxivexplained.com/papers/kimi-linear-an-expressive-efficient-attention-architecture Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2544004",
    "title": "Kimi Linear: An Expressive, Efficient Attention Architecture",
    "authors": [
      "Yu Zhang",
      "Jiaxi Hu",
      "Zhiyuan Li",
      "Weixin Xu",
      "Yejie Wang",
      "Longguang Zhong"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/MoonshotAI/Kimi-Linear",
    "huggingface_url": "https://huggingface.co/papers/2510.26692",
    "upvote": 119
  }
}