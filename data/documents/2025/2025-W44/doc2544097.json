{
  "context": "A new pipeline enhances LLM-generated code aesthetics using a large-scale dataset, multi-agent feedback, and integrated optimization, outperforming existing models. Large Language Models(LLMs) have become valuable assistants for developers\nin code-related tasks. WhileLLMsexcel at traditional programming tasks such\nas code generation and bug fixing, they struggle with visually-oriented coding\ntasks, often producing suboptimal aesthetics. In this paper, we introduce a new\npipeline to enhance the aesthetic quality of LLM-generated code. We first\nconstructAesCode-358K, a large-scale instruction-tuning dataset focused oncode aesthetics. Next, we proposeagentic reward feedback, amulti-agent systemthat evaluates executability, static aesthetics, and interactive aesthetics.\nBuilding on this, we developGRPO-AR, which integrates these signals into theGRPO algorithmfor joint optimization of functionality andcode aesthetics.\nFinally, we developOpenDesign, a benchmark for assessingcode aesthetics.\nExperimental results show that combiningsupervised fine-tuningonAesCode-358Kwithreinforcement learningusingagentic reward feedbacksignificantly\nimproves performance onOpenDesignand also enhances results on existing\nbenchmarks such asPandasPlotBench. Notably, ourAesCoder-4BsurpassesGPT-4oandGPT-4.1, and achieves performance comparable to large open-source models\nwith 480B-685B parameters, underscoring the effectiveness of our approach. Large Language Models (LLMs) have become valuable assistants for developers in code-related tasks. While LLMs excel at traditional programming tasks such as code generation and bug fixing, they struggle with visually-oriented coding tasks, often producing suboptimal aesthetics. In this paper, we introduce a new pipeline to enhance the aesthetic quality of LLM-generated code. We first construct AesCode-358K, a large-scale instruction-tuning dataset focused on code aesthetics. Next, we propose agentic reward feedback, a multi-agent system that evaluates executability, static aesthetics, and interactive aesthetics. Building on this, we develop GRPO-AR, which integrates these signals into the GRPO algorithm for joint optimization of functionality and code aesthetics. Finally, we develop OpenDesign, a benchmark for assessing code aesthetics. Experimental results show that combining supervised fine-tuning on AesCode-358K with reinforcement learning using agentic reward feedback significantly improves performance on OpenDesign and also enhances results on existing benchmarks such as PandasPlotBench. Notably, our AesCoder-4B surpasses GPT-4o and GPT-4.1, and achieves performance comparable to large open-source models with 480Bâ€“685B parameters, underscoring the effectiveness of our approach. arXiv explained breakdown of this paper ðŸ‘‰https://arxivexplained.com/papers/code-aesthetics-with-agentic-reward-feedback Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2544097",
    "title": "Code Aesthetics with Agentic Reward Feedback",
    "authors": [],
    "publication_year": 2025,
    "github_url": "https://github.com/bangx7/code_aesthetics",
    "huggingface_url": "https://huggingface.co/papers/2510.23272",
    "upvote": 8
  }
}