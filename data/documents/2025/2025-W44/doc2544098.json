{
  "context": "A new metric, UDCG, is introduced to better evaluate Retrieval Augmented Generation systems by accounting for both the utility of relevant documents and the distraction of irrelevant ones, improving correlation with end-to-end answer accuracy. Traditional Information Retrieval (IR) metrics, such asnDCG,MAP, andMRR,\nassume that human users sequentially examine documents with diminishing\nattention to lower ranks. This assumption breaks down in Retrieval Augmented\nGeneration (RAG) systems, where search results are consumed by Large Language\nModels (LLMs), which, unlike humans, process all retrieved documents as a whole\nrather than sequentially. Additionally, traditional IR metrics do not account\nfor related but irrelevant documents that actively degrade generation quality,\nrather than merely being ignored. Due to these two major misalignments, namely\nhuman vs. machine position discount and human relevance vs. machine utility,\nclassical IR metrics do not accurately predict RAG performance. We introduce autility-based annotation schemathat quantifies both the positive contribution\nof relevant passages and the negative impact of distracting ones. Building on\nthis foundation, we proposeUDCG(Utility and Distraction-aware Cumulative\nGain), a metric using an LLM-orientedpositional discountto directly optimize\nthe correlation with theend-to-end answer accuracy. Experiments on five\ndatasets and six LLMs demonstrate thatUDCGimproves correlation by up to 36%\ncompared to traditional metrics. Our work provides a critical step toward\naligning IR evaluation with LLM consumers and enables more reliable assessment\nof RAG components We introduce UDCG (Utility and Distraction-aware Cumulative Gain), a novel metric specifically designed for evaluating retrieval systems in RAG pipelines. Traditional IR metrics like nDCG, MAP, and MRR fail in RAG settings due to two critical misalignments:(1) they assume monotonically decreasing attention with rank position, unlike LLMs;(2) they treat all irrelevant documents equally, ignoring that some actively distract LLMs and degrade generation quality while others are harmless. UDCG addresses these limitations by replacing binary relevance with continuous utility scores that capture both positive contributions from relevant passages and negative impacts from distracting ones, while using an LLM-oriented positional discount that directly optimizes correlation with end-to-end answer accuracy.Experiments across 5 QA datasets (NQ, TriviaQA, PopQA, BioASQ, NoMIRACL) and 6 LLMs demonstrate that UDCG improves correlation with RAG performance by up to 36% compared to traditional metrics.Code and data: github.com/GiovanniTRA/UDCG I really like the idea how well it discussed that due to self attention model actually get whole bunch of ranked docs but we humans see each rank sequentially so i think it really good way to put it this way ndcg mrr and map are good but diminishes , this consider a philosophical aspect i wonder when model will get bigger a new emergent behaviour  will emerge and will not take each chunk as same that will be another problem. Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2544098",
    "title": "Redefining Retrieval Evaluation in the Era of LLMs",
    "authors": [
      "Florin Cuconasu"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/GiovanniTRA/UDCG",
    "huggingface_url": "https://huggingface.co/papers/2510.21440",
    "upvote": 8
  }
}