{
  "context": "Multi-Agent Evolve (MAE) framework uses reinforcement learning to enhance LLM reasoning across diverse tasks with minimal human supervision. Reinforcement Learning (RL)has demonstrated significant potential in\nenhancing the reasoning capabilities oflarge language models (LLMs). However,\nthe success of RL for LLMs heavily relies on human-curated datasets and\nverifiable rewards, which limit their scalability and generality. RecentSelf-Play RLmethods, inspired by the success of the paradigm in games and Go,\naim to enhance LLM reasoning capabilities without human-annotated data.\nHowever, their methods primarily depend on agrounded environmentfor feedback\n(e.g., a Python interpreter or a game engine); extending them to general\ndomains remains challenging. To address these challenges, we proposeMulti-Agent Evolve (MAE), a framework that enables LLMs to self-evolve in\nsolving diverse tasks, including mathematics, reasoning, and general knowledge\nQ&A. The core design of MAE is based on a triplet of interacting agents\n(Proposer,Solver,Judge) that are instantiated from a single LLM, and applies\nreinforcement learning to optimize their behaviors. TheProposergenerates\nquestions, theSolverattempts solutions, and theJudgeevaluates both whileco-evolving. Experiments onQwen2.5-3B-Instructdemonstrate that MAE achieves\nan average improvement of 4.54% on multiple benchmarks. These results highlight\nMAE as a scalable, data-efficient method for enhancing the general reasoning\nabilities of LLMs with minimal reliance on human-curated supervision. Reinforcement Learning (RL) has demonstrated significant potential in enhancing the reasoning capabilities of large language models (LLMs). However, the success of RL for LLMs heavily relies on human-curated datasets and verifiable rewards, which limit their scalability and generality. Recent Self-Play RL methods, inspired by the success of the paradigm in games and Go, aim to enhance LLM reasoning capabilities without human-annotated data. However, their methods primarily depend on a grounded environment for feedback (e.g., a Python interpreter or a game engine); extending them to general domains remains challenging. To address these challenges, we propose Multi-Agent Evolve (MAE), a framework that enables LLMs to self-evolve in solving diverse tasks, including mathematics, reasoning, and general knowledge Q&A. The core design of MAE is based on a triplet of interacting agents (Proposer, Solver, Judge) that are instantiated from a single LLM, and applies reinforcement learning to optimize their behaviors. The Proposer generates questions, the Solver attempts solutions, and the Judge evaluates both while co-evolving. Experiments on Qwen2.5-3B-Instruct demonstrate that MAE achieves an average improvement of 4.54% on multiple benchmarks. These results highlight MAE as a scalable, data-efficient method for enhancing the general reasoning abilities of LLMs with minimal reliance on human-curated supervision. Very interesting, thanks a lot. I think the same framework could be used for distillation as well Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2544085",
    "title": "Multi-Agent Evolve: LLM Self-Improve through Co-evolution",
    "authors": [
      "Siqi Zhu"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/ulab-uiuc/Multi-agent-evolve",
    "huggingface_url": "https://huggingface.co/papers/2510.23595",
    "upvote": 11
  }
}