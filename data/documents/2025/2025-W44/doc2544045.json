{
  "context": "A framework transfers knowledge from video generation to 3D human motion generation using a large dataset, a flow-matching-based diffusion transformer, and a hierarchical benchmark, improving generalization and performance. Despite recent advances in3D human motion generation(MoGen) on standard\nbenchmarks, existing models still face a fundamental bottleneck in their\ngeneralization capability. In contrast, adjacent generative fields, most\nnotablyvideo generation(ViGen), have demonstrated remarkable generalization\nin modeling human behaviors, highlighting transferable insights that MoGen can\nleverage. Motivated by this observation, we present a comprehensive framework\nthat systematically transfers knowledge from ViGen to MoGen across three key\npillars: data, modeling, and evaluation. First, we introduceViMoGen-228K, a\nlarge-scale dataset comprising 228,000 high-quality motion samples that\nintegrates high-fidelity optical MoCap data with semantically annotated motions\nfrom web videos and synthesized samples generated by state-of-the-art ViGen\nmodels. The dataset includes both text-motion pairs and text-video-motion\ntriplets, substantially expanding semantic diversity. Second, we propose\nViMoGen, aflow-matching-based diffusion transformerthat unifies priors from\nMoCap data and ViGen models throughgated multimodal conditioning. To enhance\nefficiency, we further developViMoGen-light, a distilled variant that\neliminatesvideo generationdependencies while preserving strong\ngeneralization. Finally, we presentMBench, ahierarchical benchmarkdesigned\nfor fine-grained evaluation across motion quality, prompt fidelity, and\ngeneralization ability. Extensive experiments show that our framework\nsignificantly outperforms existing approaches in both automatic and human\nevaluations. The code, data, and benchmark will be made publicly available. Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2544045",
    "title": "The Quest for Generalizable Motion Generation: Data, Model, and\n  Evaluation",
    "authors": [
      "Ziqi Huang"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/oneScotch/ViMoGen",
    "huggingface_url": "https://huggingface.co/papers/2510.26794",
    "upvote": 26
  }
}