{
  "context": "OSWorld-MCP is a benchmark that evaluates multimodal agents' tool invocation, GUI operation, and decision-making abilities, highlighting the importance of assessing tool usage in real-world scenarios. With advances in decision-making and reasoning capabilities, multimodal\nagents show strong potential in computer application scenarios. Past\nevaluations have mainly assessed GUI interaction skills, while tool invocation\nabilities, such as those enabled by theModel Context Protocol (MCP), have been\nlargely overlooked. Comparing agents with integrated tool invocation to those\nevaluated only on GUI interaction is inherently unfair. We presentOSWorld-MCP,\nthe first comprehensive and fair benchmark for assessing computer-use agents'\ntool invocation,GUI operation, and decision-making abilities in a real-world\nenvironment. We design a novelautomated code-generation pipelineto create\ntools and combine them with a curated selection from existing tools. Rigorous\nmanual validation yields 158 high-quality tools (covering 7 common\napplications), each verified for correct functionality, practical\napplicability, and versatility. Extensive evaluations of state-of-the-artmultimodal agentsonOSWorld-MCPshow that MCP tools generally improve task\nsuccess rates (e.g., from 8.3% to 20.4% for OpenAI o3 at 15 steps, from 40.1%\nto 43.3% for Claude 4 Sonnet at 50 steps), underscoring the importance of\nassessing tool invocation capabilities. However, even the strongest models have\nrelatively lowtool invocation rates, Only 36.3%, indicating room for\nimprovement and highlighting the benchmark's challenge. By explicitly measuring\nMCP tool usage skills,OSWorld-MCPdeepens understanding ofmultimodal agentsand sets a new standard for evaluating performance in complex, tool-assisted\nenvironments. Our code, environment, and data are publicly available at\nhttps://osworld-mcp.github.io. With advances in decision-making and reasoning capabilities, multimodal agents show strong potential in computer application scenarios. Past evaluations have mainly assessed GUI interaction skills, while tool invocation abilities, such as those enabled by the Model Context Protocol (MCP), have been largely overlooked. Comparing agents with integrated tool invocation to those evaluated only on GUI interaction is inherently unfair. We present OSWorld-MCP, the first comprehensive and fair benchmark for assessing computer-use agents' tool invocation, GUI operation, and decision-making abilities in a real-world environment. We design a novel automated code-generation pipeline to create tools and combine them with a curated selection from existing tools. Rigorous manual validation yields 158 high-quality tools (covering 7 common applications), each verified for correct functionality, practical applicability, and versatility. Extensive evaluations of state-of-the-art multimodal agents on OSWorld-MCP show that MCP tools generally improve task success rates (e.g., from 8.3% to 20.4% for OpenAI o3 at 15 steps, from 40.1% to 43.3% for Claude 4 Sonnet at 50 steps), underscoring the importance of assessing tool invocation capabilities. However, even the strongest models have relatively low tool invocation rates, Only 36.3%, indicating room for improvement and highlighting the benchmark's challenge. By explicitly measuring MCP tool usage skills, OSWorld-MCP deepens understanding of multimodal agents and sets a new standard for evaluating performance in complex, tool-assisted environments. Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2544054",
    "title": "OSWorld-MCP: Benchmarking MCP Tool Invocation In Computer-Use Agents",
    "authors": [
      "Haiyang Xu"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/X-PLUG/OSWorld-MCP",
    "huggingface_url": "https://huggingface.co/papers/2510.24563",
    "upvote": 22
  }
}