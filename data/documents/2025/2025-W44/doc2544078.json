{
  "context": "BeTaL, a framework using LLMs to automate dynamic benchmark design, creates benchmarks with desired difficulty levels more accurately and efficiently than static methods. The rapid progress and widespread deployment ofLLMsandLLM-powered agentshas outpaced our ability to evaluate them. Hand-crafted,static benchmarksare\nthe primary tool for assessing model capabilities, but these quickly become\nsaturated. In contrast,dynamic benchmarksevolve alongside the models they\nevaluate, but are expensive to create and continuously update. To address these\nchallenges, we develop BeTaL (Benchmark Tuning with an LLM-in-the-loop), a\nframework that leveragesenvironment design principlesto automate the process\nof dynamic benchmark design. BeTaL works by parameterizing key design choices\nin base benchmark templates and usesLLMsto reason through the resultingparameter spaceto obtain target properties (such asdifficultyandrealism) in\na cost-efficient manner. We validate this approach on its ability to create\nbenchmarks with desireddifficultylevels. Using BeTaL, we create two new\nbenchmarks and extend a popular agentic benchmark tau-bench. Extensive\nevaluation on these three tasks and multiple targetdifficultylevels shows\nthat BeTaL produces benchmarks much closer to the desireddifficulty, with\naverage deviations ranging from 5.3% to 13.2% -- a 2-4x improvement over the\nbaselines. Static, human-curated benchmarks like GPQA and HLE are costly to develop and quickly become obsolete as models improve faster than evaluation can keep pace. We introduce BeTaL (Benchmark Tuning with An LLM-in-the-loop), a framework that automates dynamic benchmark design by parameterizing benchmark templates and using LLMs to reason over the design space, in an iterative manner. BeTaL generates benchmarks with target properties, such as specific difficulty levels or realism constraints, achieving significantly better alignment with desired difficulty targets than baseline methods. Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2544078",
    "title": "Automating Benchmark Design",
    "authors": [],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2510.25039",
    "upvote": 15
  }
}