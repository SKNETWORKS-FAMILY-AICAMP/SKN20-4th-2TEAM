{
  "context": "PixelRefer is a unified region-level multimodal large language model framework that enhances fine-grained object-centric understanding and efficiency through a Scale-Adaptive Object Tokenizer and Object-Centric Infusion module. Multimodal large language models(MLLMs) have demonstrated strong\ngeneral-purpose capabilities in open-world visual comprehension. However, most\nexistingMLLMsprimarily focus on holistic, scene-level understanding, often\noverlooking the need for fine-grained,object-centric reasoning. In this paper,\nwe presentPixelRefer, a unified region-level MLLM framework that enables\nadvanced fine-grained understanding over user-specified regions across both\nimages and videos. Motivated by the observation that LLM attention\npredominantly focuses on object-level tokens, we propose a Scale-Adaptive\nObject Tokenizer (SAOT) to generate compact and semantically rich object\nrepresentations from free-form regions. Our analysis reveals that global visual\ntokens contribute mainly in early LLM layers, inspiring the design ofPixelRefer-Lite, an efficient variant that employs an Object-Centric Infusion\nmodule to pre-fuse global context intoobject tokens. This yields a lightweight\nObject-Only Framework that substantially reduces computational cost while\nmaintaining high semantic fidelity. To facilitate fine-grained instruction\ntuning, we curatePixelRefer-2.2M, a high-quality object-centric instruction\ndataset. Extensive experiments across a range of benchmarks validate thatPixelReferachieves leading performance with fewer training samples, whilePixelRefer-Liteoffers competitive accuracy with notable gains in efficiency. Multimodal large language models (MLLMs) have demonstrated strong general-purpose capabilities in open-world visual comprehension. However, most existing MLLMs primarily focus on holistic, scene-level understanding, often overlooking the need for fine-grained, object-centric reasoning. In this paper, we present PixelRefer, a unified region-level MLLM framework that enables advanced fine-grained understanding over user-specified regions across both images and videos. Motivated by the observation that LLM attention predominantly focuses on object-level tokens, we propose a Scale-Adaptive Object Tokenizer (SAOT) to generate compact and semantically rich object representations from free-form regions. Our analysis reveals that global visual tokens contribute mainly in early LLM layers, inspiring the design of PixelRefer-Lite, an efficient variant that employs an Object-Centric Infusion module to pre-fuse global context into object tokens. This yields a lightweight Object-Only Framework that substantially reduces computational cost while maintaining high semantic fidelity. To facilitate fine-grained instruction tuning, we curate PixelRefer-2.2M, a high-quality object-centric instruction dataset. Extensive experiments across a range of benchmarks validate that PixelRefer achieves leading performance with fewer training samples, while PixelRefer-Lite offers competitive accuracy with notable gains in efficiency. Homepage:https://circleradon.github.io/PixelReferDemo:https://huggingface.co/spaces/lixin4ever/PixelReferCode:https://github.com/alibaba-damo-academy/PixelReferHuggingFace:https://huggingface.co/collections/Alibaba-DAMO-Academy/pixelrefer Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2544055",
    "title": "PixelRefer: A Unified Framework for Spatio-Temporal Object Referring\n  with Arbitrary Granularity",
    "authors": [
      "Yuqian Yuan",
      "Wentong Li"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/alibaba-damo-academy/PixelRefer",
    "huggingface_url": "https://huggingface.co/papers/2510.23603",
    "upvote": 22
  }
}