{
  "context": "Parallel decoding for transformers is improved through a progressive distillation method that maintains causal inference properties while achieving significant speedup, combined with multi-block decoding that further reduces inference latency. Multi-token generation has emerged as a promising paradigm for accelerating transformer-based large model inference. Recent efforts primarily explorediffusion Large Language Models(dLLMs) forparallel decodingto reduce inference latency. To achieveAR-level generation quality, many techniques adapt AR models into dLLMs to enableparallel decoding. However, they suffer from limited speedup compared to AR models due to a pretrain-to-posttrain mismatch. Specifically, themasked data distributionin post-training deviates significantly from the real-world data distribution seen during pretraining, and dLLMs rely onbidirectional attention, which conflicts with thecausal priorlearned during pretraining and hinders the integration of exactKV cache reuse. To address this, we introduceJacobi Forcing, aprogressive distillationparadigm where models are trained on their own generatedparallel decodingtrajectories, smoothly shifting AR models into efficient parallel decoders while preserving their pretrained causal inference property. The models trained under this paradigm,Jacobi ForcingModel, achieves 3.8x wall-clock speedup on coding and math benchmarks with minimal loss in performance. Based onJacobi ForcingModels' trajectory characteristics, we introducemulti-block decodingwithrejection recycling, which enables up to 4.5x higher token acceptance count per iteration and nearly 4.0x wall-clock speedup, effectively trading additional compute for lower inference latency. Our code is available at https://github.com/hao-ai-lab/JacobiForcing. Multi-token generation has emerged as a promising paradigm for accelerating transformer-based large model inference. Recent efforts primarily explore diffusion Large Language Models (dLLMs) for parallel decoding to reduce inference latency. To achieve AR-level generation quality, many techniques adapt AR models into dLLMs to enable parallel decoding. However, they suffer from either generation quality or limited wall-clock speedup compared to AR models due to a pretrain-to-posttrain mismatch. Specifically, the masked data distribution in post-training deviates significantly from the real-world data distribution seen during pretraining, and dLLMs rely on bidirectional attention, which conflicts with the causal prior learned during pretraining and hinders the integration of exact KV cache reuse. To address this, we introduce Jacobi Forcing, a progressive distillation paradigm where models are trained on their own generated parallel decoding trajectories, smoothly shifting AR models into efficient parallel decoders while preserving their pretrained causal inference property. The models trained under this paradigm, Jacobi Forcing Model, achieves 3.8x wall-clock speedup on coding benchmarks with minimal loss in performance. Based on Jacobi Forcing Modelâ€™s trajectory characteristics, we introduce multi-block decoding with rejection recycling, which enables up to 4.5x higher token acceptance count per iteration and nearly 4.0x wall-clock speedup, effectively trading additional compute for lower inference latency. Our code is available athttps://github.com/hao-ai-lab/JacobiForcing. arXiv lens breakdown of this paper ðŸ‘‰https://arxivlens.com/PaperView/Details/fast-and-accurate-causal-parallel-decoding-using-jacobi-forcing-9815-f3d12058 Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2551026",
    "title": "Fast and Accurate Causal Parallel Decoding using Jacobi Forcing",
    "authors": [
      "Lanxiang Hu"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/hao-ai-lab/JacobiForcing",
    "huggingface_url": "https://huggingface.co/papers/2512.14681",
    "upvote": 39
  }
}