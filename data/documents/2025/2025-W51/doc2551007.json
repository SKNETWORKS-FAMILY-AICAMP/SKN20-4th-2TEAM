{
  "context": "A unified visual tokenizer pre-training framework (VTP) improves generative performance by optimizing image-text contrastive, self-supervised, and reconstruction losses, leading to better scaling properties and higher zero-shot accuracy and faster convergence. The quality of thelatent spaceinvisual tokenizers(e.g.,VAEs) is crucial for modern generative models. However, the standardreconstruction-based trainingparadigm produces alatent spacethat is biased towardslow-level information, leading to a foundation flaw: better pixel-level accuracy does not lead to higher-quality generation. This implies that pouring extensive compute into visual tokenizer pre-training translates poorly to improved performance in generation. We identify this as the ``pre-training scaling problem`` and suggest a necessary shift: to be effective for generation, alatent spacemust concisely representhigh-level semantics. We presentVTP, a unified visual tokenizer pre-training framework, pioneering the joint optimization ofimage-text contrastive,self-supervised, andreconstruction losses. Our large-scale study reveals two principal findings: (1) understanding is a key driver of generation, and (2) much better scaling properties, wheregenerative performancescales effectively with compute, parameters, and data allocated to the pretraining of the visual tokenizer. After large-scale pre-training, our tokenizer delivers a competitive profile (78.2zero-shot accuracyand 0.36rFIDonImageNet) and 4.1 times faster convergence on generation compared toadvanced distillation methods. More importantly, it scales effectively: without modifying standardDiTtraining specs, solely investing moreFLOPSin pretrainingVTPachieves 65.8\\% FID improvement in downstream generation, while conventional autoencoder stagnates very early at 1/10FLOPS. Our pre-trained models are available at https://github.com/MiniMax-AI/VTP. GitHub codes:https://github.com/MiniMax-AI/VTPHuggingface weights:https://huggingface.co/collections/MiniMaxAI/vtpcollaborated with HUST Vision Lab:https://github.com/hustvl This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend This gets at a core bottleneck in generative vision: tokenizers optimized for pixels don‚Äôt scale cognition. VTP‚Äôs shift toward semantic-first latent spaces mirrors what we‚Äôve already learned in language ‚Äî understanding must precede generation. The fact that generation quality now scales with tokenizer pretraining FLOPs is the real breakthrough here. This feels like a necessary correction to the ‚Äújust reconstruct better‚Äù era of VAEs and a strong signal that vision models are finally being trained to think, not just compress. arXiv lens breakdown of this paper üëâhttps://arxivlens.com/PaperView/Details/towards-scalable-pre-training-of-visual-tokenizers-for-generation-8866-f2bc9df0 ¬∑Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2551007",
    "title": "Towards Scalable Pre-training of Visual Tokenizers for Generation",
    "authors": [
      "Jingfeng Yao",
      "Yuda Song",
      "Yucong Zhou",
      "Xinggang Wang"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/MiniMax-AI/VTP",
    "huggingface_url": "https://huggingface.co/papers/2512.13687",
    "upvote": 99
  }
}