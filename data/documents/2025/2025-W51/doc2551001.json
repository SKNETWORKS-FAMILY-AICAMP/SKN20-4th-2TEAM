{
  "context": "Kling-Omni is a versatile generative framework that synthesizes high-quality videos from multimodal inputs, integrating generation, editing, and reasoning into a unified system. We present Kling-Omni, a generalistgenerative frameworkdesigned to synthesize high-fidelity videos directly frommultimodal visual language inputs. Adopting anend-to-endperspective, Kling-Omni bridges the functional separation among diversevideo generation,editing, andintelligent reasoningtasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into aunified multimodal representationto delivercinematic-qualityand highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered byefficient large-scale pre-trainingstrategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities inin-context generation,reasoning-based editing, andmultimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement towardmultimodal world simulatorscapable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds. We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds. arXiv lens breakdown of this paper üëâhttps://arxivlens.com/PaperView/Details/kling-omni-technical-report-1204-ba515c58 arXiv explained breakdown of this paper üëâhttps://arxivexplained.com/papers/kling-omni-technical-report This is way more than a video generator üëÄ Unifying generation + editing + reasoning into a single multimodal system feels like a real step toward world simulators, not just content tools. The end-to-end design + in-context reasoning is especially interesting ‚Äî pipelines always felt like a bottleneck here. Feels like the line between ‚Äúvideo model‚Äù and ‚Äúinteractive world model‚Äù is starting to blur. Excited to see how the community experiments with this next. üöÄüß† ¬∑Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2551001",
    "title": "Kling-Omni Technical Report",
    "authors": [],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2512.16776",
    "upvote": 164
  }
}