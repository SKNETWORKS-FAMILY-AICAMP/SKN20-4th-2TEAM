{
  "context": "Puzzle Curriculum GRPO enhances visual reasoning in Vision Language Models through self-supervised environments and a difficulty-aware curriculum, improving consistency and accuracy without external annotations. Recentreinforcement learning(RL) approaches likeoutcome-supervised GRPOhave advancedchain-of-thought reasoninginVision Language Models(VLMs), yet key issues linger: (i) reliance on costly and noisy hand-curated annotations or external verifiers; (ii) flat and sparse reward schemes in GRPO; and (iii) logical inconsistency between a chain's reasoning and its final answer. We present Puzzle Curriculum GRPO (PC-GRPO), a supervision-free recipe for RL withVerifiable Rewards(RLVR) that strengthens visual reasoning in VLMs without annotations or external verifiers. PC-GRPO replaces labels with threeself-supervisedpuzzle environments:PatchFit,Rotation(with binary rewards) andJigsaw(withgraded partial creditmitigatingreward sparsity). To counter flat rewards and vanishing group-relative advantages, we introduce adifficulty-aware curriculumthat dynamically weights samples and peaks at medium difficulty. We further monitorReasoning-Answer Consistency(RAC) duringpost-training: mirroring reports for vanilla GRPO inLLMs, RAC typically rises early then degrades; our curriculum delays this decline, and consistency-enforcing reward schemes further boost RAC. RAC correlates with downstream accuracy. Across diverse benchmarks and onQwen-7BandQwen-3Bbackbones, PC-GRPO improves reasoning quality, training stability, and end-task accuracy, offering a practical path to scalable, verifiable, and interpretable RLpost-trainingfor VLMs. Recent reinforcement learning (RL) approaches like outcome-supervised GRPO have advanced chain-of-thought reasoning in Vision Language Models (VLMs), yet key issues linger: (i) reliance on costly and noisy hand-curated annotations or external verifiers; (ii) flat and sparse reward schemes in GRPO; and (iii) logical inconsistency between a chain's reasoning and its final answer. We present Puzzle Curriculum GRPO (PC-GRPO), a supervision-free recipe for RL with Verifiable Rewards (RLVR) that strengthens visual reasoning in VLMs without annotations or external verifiers. PC-GRPO replaces labels with three self-supervised puzzle environments: PatchFit, Rotation (with binary rewards) and Jigsaw (with graded partial credit mitigating reward sparsity). To counter flat rewards and vanishing group-relative advantages, we introduce a difficulty-aware curriculum that dynamically weights samples and peaks at medium difficulty. We further monitor Reasoning-Answer Consistency (RAC) during post-training: mirroring reports for vanilla GRPO in LLMs, RAC typically rises early then degrades; our curriculum delays this decline, and consistency-enforcing reward schemes further boost RAC. RAC correlates with downstream accuracy. Across diverse benchmarks and on Qwen-7B and Qwen-3B backbones, PC-GRPO improves reasoning quality, training stability, and end-task accuracy, offering a practical path to scalable, verifiable, and interpretable RL post-training for VLMs. arXiv lens breakdown of this paper ðŸ‘‰https://arxivlens.com/PaperView/Details/puzzle-curriculum-grpo-for-vision-centric-reasoning-6295-339e950e Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2551032",
    "title": "Puzzle Curriculum GRPO for Vision-Centric Reasoning",
    "authors": [
      "Ahmadreza Jeddi",
      "Hakki Can Karaimer",
      "Ke Zhao"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/SamsungLabs/PC-GRPO",
    "huggingface_url": "https://huggingface.co/papers/2512.14944",
    "upvote": 34
  }
}