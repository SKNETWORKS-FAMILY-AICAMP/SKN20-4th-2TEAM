{
  "context": "Cascaded domain-wise reinforcement learning (Cascade RL) is proposed to enhance general-purpose reasoning models, achieving state-of-the-art performance across benchmarks and outperforming the teacher model in coding competitions. Building general-purpose reasoning models withreinforcement learning(RL) entails substantial cross-domain heterogeneity, including large variation in inference-time response lengths and verification latency. Such variability complicates theRLinfrastructure, slows training, and makes training curriculum (e.g., response length extension) and hyperparameter selection challenging. In this work, we propose cascaded domain-wisereinforcement learning(Cascade RL) to develop general-purpose reasoning models,Nemotron-Cascade, capable of operating in both instruct anddeep thinking modes. Departing from conventional approaches that blend heterogeneous prompts from different domains,Cascade RLorchestrates sequential, domain-wiseRL, reducing engineering complexity and delivering state-of-the-art performance across a wide range of benchmarks. Notably,RLHFforalignment, when used as a pre-step, boosts the model's reasoning ability far beyond mere preference optimization, and subsequent domain-wiseRLVRstages rarely degrade the benchmark performance attained in earlier domains and may even improve it (see an illustration in Figure 1). Our 14B model, afterRL, outperforms its SFT teacher, DeepSeek-R1-0528, onLiveCodeBenchv5/v6/Pro and achieves silver-medal performance in the 2025International Olympiad in Informatics (IOI). We transparently share our training and data recipes. The Nemotron-Cascade models and the full collection of training data are released at:https://huggingface.co/collections/nvidia/nemotron-cascade Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2551038",
    "title": "Nemotron-Cascade: Scaling Cascaded Reinforcement Learning for General-Purpose Reasoning Models",
    "authors": [
      "Yang Chen",
      "Yangyi Chen",
      "Zhuolin Yang"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2512.13607",
    "upvote": 28
  }
}