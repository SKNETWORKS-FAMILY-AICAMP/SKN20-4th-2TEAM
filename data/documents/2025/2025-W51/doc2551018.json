{
  "context": "KlingAvatar 2.0 addresses inefficiencies in generating long-duration, high-resolution videos by using a spatio-temporal cascade framework with a Co-Reasoning Director and Negative Director for improved multimodal instruction alignment. Avatar video generation models have achieved remarkable progress in recent years. However, prior work exhibits limited efficiency in generating long-durationhigh-resolutionvideos, suffering from temporal drifting, quality degradation, and weak prompt following as video length increases. To address these challenges, we propose KlingAvatar 2.0, aspatio-temporal cascadeframework that performs upscaling in both spatial resolution and temporal dimension. The framework first generates low-resolutionblueprint video keyframesthat capture global semantics and motion, and then refines them intohigh-resolution,temporally coherent sub-clipsusing afirst-last frame strategy, while retainingsmooth temporal transitionsin long-form videos. To enhance cross-modal instruction fusion and alignment in extended videos, we introduce aCo-Reasoning Directorcomposed of threemodality-specific large language model(LLM) experts. These experts reason about modality priorities and infer underlying user intent, converting inputs into detailed storylines throughmulti-turn dialogue. ANegative Directorfurther refines negative prompts to improve instruction alignment. Building on these components, we extend the framework to support ID-specific multi-character control. Extensive experiments demonstrate that our model effectively addresses the challenges of efficient, multimodally aligned long-formhigh-resolutionvideo generation, delivering enhanced visual clarity, realistic lip-teeth rendering with accuratelip synchronization, strongidentity preservation, and coherentmultimodal instruction following. Avatar video generation models have achieved remarkable progress in recent years. However, prior work exhibits limited efficiency in generating long-duration high-resolution videos, suffering from temporal drifting, quality degradation, and weak prompt following as video length increases. To address these challenges, we propose KlingAvatar 2.0, a spatio-temporal cascade framework that performs upscaling in both spatial resolution and temporal dimension. The framework first generates low-resolution blueprint video keyframes that capture global semantics and motion, and then refines them into high-resolution, temporally coherent sub-clips using a first-last frame strategy, while retaining smooth temporal transitions in long-form videos. To enhance cross-modal instruction fusion and alignment in extended videos, we introduce a Co-Reasoning Director composed of three modality-specific large language model (LLM) experts. These experts reason about modality priorities and infer underlying user intent, converting inputs into detailed storylines through multi-turn dialogue. A Negative Director further refines negative prompts to improve instruction alignment. Building on these components, we extend the framework to support ID-specific multi-character control. Extensive experiments demonstrate that our model effectively addresses the challenges of efficient, multimodally aligned long-form high-resolution video generation, delivering enhanced visual clarity, realistic lip-teeth rendering with accurate lip synchronization, strong identity preservation, and coherent multimodal instruction following. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2551018",
    "title": "KlingAvatar 2.0 Technical Report",
    "authors": [
      "Jingyun Hua"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2512.13313",
    "upvote": 42
  }
}