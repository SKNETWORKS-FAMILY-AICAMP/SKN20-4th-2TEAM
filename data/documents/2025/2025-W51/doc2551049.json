{
  "context": "N3D-VLM integrates native 3D perception and reasoning in vision-language models, enabling precise 3D localization and spatial understanding with a large-scale dataset. While current multimodal models can answer questions based on 2D images, they lack intrinsic3D object perception, limiting their ability to comprehend spatial relationships and depth cues in 3D scenes. In this work, we propose N3D-VLM, a novel unified framework that seamlessly integratesnative 3D object perceptionwith3D-aware visual reasoning, enabling both precise3D groundingand interpretablespatial understanding. Unlike conventional end-to-end models that directly predict answers from RGB/RGB-D inputs, our approach equips the model withnative 3D object perceptioncapabilities, enabling it to directly localize objects in 3D space based on textual descriptions. Building upon accurate3D object localization, the model further performsexplicit reasoningin 3D, achieving more interpretable and structuredspatial understanding. To support robust training for these capabilities, we develop a scalable data construction pipeline that leveragesdepth estimationto lift large-scale 2D annotations into 3D space, significantly increasing the diversity and coverage for 3D object grounding data, yielding over six times larger than the largest existing single-image 3D detection dataset. Moreover, the pipeline generates spatial question-answering datasets that target chain-of-thought (CoT) reasoning in 3D, facilitating joint training for both3D object localizationand3D spatial reasoning. Experimental results demonstrate that our unified framework not only achieves state-of-the-art performance on3D groundingtasks, but also consistently surpasses existing methods in3D spatial reasoninginvision-language model. Project page:https://n3d-vlm.github.io/Code:https://github.com/W-Ted/N3D-VLM arXiv lens breakdown of this paper ðŸ‘‰https://arxivlens.com/PaperView/Details/n3d-vlm-native-3d-grounding-enables-accurate-spatial-reasoning-in-vision-language-models-3658-8915a8ae Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2551049",
    "title": "N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models",
    "authors": [
      "Yuxin Wang",
      "Dan Xu"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/W-Ted/N3D-VLM",
    "huggingface_url": "https://huggingface.co/papers/2512.16561",
    "upvote": 19
  }
}