{
  "context": "DiffusionVL, a family of diffusion vision language models derived from autoregressive models through fine-tuning, achieves performance improvements and faster inference speeds compared to existing models. In recent multimodal research, thediffusion paradigmhas emerged as a promising alternative to theautoregressive paradigm(AR), owing to its unique decoding advantages. However, due to the capability limitations of the base diffusion language model, the performance of thediffusion vision language model(dVLM) still lags significantly behind that of mainstream models. This leads to a simple yet fundamental question: Is it possible to constructdVLMs based on existing powerful AR models? In response, we proposeDiffusionVL, adVLMfamily that could be translated from any powerful AR models. Through simplefine-tuning, we successfully adapt AR pre-trained models into thediffusion paradigm. This approach yields two key observations: (1) The paradigm shift from AR-based multimodal models to diffusion is remarkably effective. (2) Direct conversion of an AR language model to adVLMis also feasible, achieving performance competitive with LLaVA-style visual-instruction-tuning. Further, we introduce ablock-decoding designintodVLMs that supports arbitrary-length generation andKV cache reuse, achieving a significant inference speedup. We conduct a large number of experiments. Despite training with less than 5% of the data required by prior methods,DiffusionVLachieves a comprehensive performance improvement-a 34.4% gain on theMMMU-Pro(vision) bench and 37.5% gain on theMME(Cog.) bench-alongside a 2x inference speedup. The model and code are released at https://github.com/hustvl/DiffusionVL. In recent multimodal research, the diffusion paradigm has emerged as a promising alternative to the autoregressive paradigm (AR), owing to its unique decoding advantages. However, due to the capability limitations of the base diffusion language model, the performance of the diffusion vision language model (dVLM) still lags significantly behind that of mainstream models. This leads to a simple yet fundamental question: Is it possible to construct dVLMs based on existing powerful AR models? In response, we propose DiffusionVL, a dVLM family that could be translated from any powerful AR models. Through simple fine-tuning, we successfully adapt AR pre-trained models into the diffusion paradigm. This approach yields two key observations: (1) The paradigm shift from AR-based multimodal models to diffusion is remarkably effective. (2) Direct conversion of an AR language model to a dVLM is also feasible, achieving performance competitive with LLaVA-style visual-instruction-tuning. Further, we introduce a block-decoding design into dVLMs that supports arbitrary-length generation and KV cache reuse, achieving a significant inference speedup. We conduct a large number of experiments. Despite training with less than 5% of the data required by prior methods, DiffusionVL achieves a comprehensive performance improvement-a 34.4% gain on the MMMU-Pro (vision) bench and 37.5% gain on the MME (Cog.) bench-alongside a 2x inference speedup. arXiv lens breakdown of this paper ðŸ‘‰https://arxivlens.com/PaperView/Details/diffusionvl-translating-any-autoregressive-models-into-diffusion-vision-language-models-1219-cfa4bf67 Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2551059",
    "title": "DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models",
    "authors": [
      "Lunbin Zeng",
      "Hongyuan Tao"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/hustvl/DiffusionVL",
    "huggingface_url": "https://huggingface.co/papers/2512.15713",
    "upvote": 16
  }
}