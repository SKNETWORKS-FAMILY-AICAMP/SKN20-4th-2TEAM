{
  "context": "The Sequential SFT-then-RL pipeline is identified as optimal for integrating expert trajectories, with guidelines for scaling and trajectory selection based on performance metrics. While effective post-training integratesSupervised Fine-Tuning(SFT) andReinforcement Learning(RL), the optimal mechanism for utilizing expert trajectories remains unresolved. We propose thePlasticity-Ceiling Frameworkto theoretically ground this landscape, decomposing performance into foundational SFT performance and the subsequent RL plasticity. Through extensive benchmarking, we establish theSequential SFT-then-RLpipeline as the superior standard, overcoming the stability deficits of synchronized approaches. Furthermore, we derive precise scaling guidelines: (1) Transitioning to RL at theSFT StableorMild Overfitting Sub-phasemaximizes the final ceiling by securing foundational SFT performance without compromising RL plasticity; (2) Refuting ``Less is More'' in the context of SFT-then-RL scaling, we demonstrate thatData Scaledetermines the primary post-training potential, whileTrajectory Difficultyacts as a performance multiplier; and (3) Identifying that theMinimum SFT Validation Lossserves as a robust indicator for selecting the expert trajectories that maximize the final performance ceiling. Our findings provide actionable guidelines for maximizing the value extracted from expert trajectories. The systematic study of expert trajectory utilization in LLM post-training. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Nice work. Well aligned with my experience. The pattern might be different for base model with adequate mid-training (requires less sft training). But I agree the validation loss is a good indicator. arXiv lens breakdown of this paper ðŸ‘‰https://arxivlens.com/PaperView/Details/rethinking-expert-trajectory-utilization-in-llm-post-training-1383-d4972d1b Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2551101",
    "title": "Rethinking Expert Trajectory Utilization in LLM Post-training",
    "authors": [],
    "publication_year": 2025,
    "github_url": "https://github.com/LINs-lab/RETU",
    "huggingface_url": "https://huggingface.co/papers/2512.11470",
    "upvote": 7
  }
}