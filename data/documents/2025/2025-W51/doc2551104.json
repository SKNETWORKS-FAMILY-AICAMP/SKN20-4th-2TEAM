{
  "context": "Nemotron-Math, a large-scale mathematical reasoning dataset, enhances performance and robustness through diverse problem integration and efficient long-context training strategies. High-quality mathematical reasoning supervision requires diverse reasoning styles, long-form traces, and effective tool integration, capabilities that existing datasets provide only in limited form. Leveraging themulti-mode generationability ofgpt-oss-120b, we introduce Nemotron-Math, a large-scalemathematical reasoning datasetcontaining 7.5Msolution tracesacross high, medium, and lowreasoning modes, each available both with and withoutPython tool-integrated reasoning (TIR).\n  The dataset integrates 85K curatedAoPS problemswith 262K community-sourcedStackExchange-Math problems, combining structured competition tasks with diverse real-world mathematical queries. We conductcontrolled evaluationsto assess the dataset quality.\n  Nemotron-Math consistently outperforms the original OpenMathReasoning on matchedAoPS problems. Incorporating StackExchange-Math substantially improves robustness and generalization, especially on HLE-Math, while preserving accuracy on math competition benchmarks.\n  To support efficientlong-context training, we develop asequential bucketed strategythat accelerates 128K context-length fine-tuning by 2--3times without significant accuracy loss. Overall, Nemotron-Math enablesstate-of-the-art performance, including 100\\%maj@16 accuracyonAIME 2024 and 2025with Python TIR. Nemotron-Math is a large-scale mathematical reasoning dataset with 7.5 million solution traces spanning high, medium, and low reasoning modes, each available with and without Python tool-integrated reasoning (TIR). It combines 85K curated AoPS competition problems with 262K community-sourced StackExchange-Math questions, balancing structured tasks and real-world mathematical queries. Experiments show that Nemotron-Math consistently outperforms prior datasets, improving robustness and generalization—especially on HLE-Math—while maintaining strong performance on competition benchmarks. With an efficient long-context training strategy, it enables state-of-the-art results, including 100% maj@16 accuracy on AIME 2024 and 2025 using Python TIR. ·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2551104",
    "title": "Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision",
    "authors": [],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2512.15489",
    "upvote": 6
  }
}