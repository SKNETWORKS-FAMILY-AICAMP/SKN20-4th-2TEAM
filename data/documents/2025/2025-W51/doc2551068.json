{
  "context": "AR-to-dLM conversion enhances diffusion language models' efficiency and speed while maintaining task accuracy through refined attention patterns and token masking strategies. Diffusion language models(dLMs) have emerged as a promising paradigm that enables parallel, non-autoregressive generation, but their learning efficiency lags behind that of autoregressive (AR) language models when trained from scratch. To this end, we study AR-to-dLM conversion to transformpretrained AR modelsinto efficient dLMs that excel in speed while preserving AR models' task accuracy. We achieve this by identifying limitations in theattention patternsand objectives of existing AR-to-dLM methods and then proposing principles and methodologies for more effective AR-to-dLM conversion. Specifically, we first systematically compare differentattention patternsand find that maintaining pretrained AR weight distributions is critical for effective AR-to-dLM conversion. As such, we introduce acontinuous pretraining schemewith ablock-wise attentionpattern, which remains causal across blocks while enabling bidirectional modeling within each block. We find that this approach can better preservepretrained AR models' weight distributions than fully bidirectional modeling, in addition to its known benefit of enablingKV caching, and leads to a win-win in accuracy and efficiency. Second, to mitigate the training-test gap in mask token distributions (uniform vs. highly left-to-right), we propose aposition-dependent token maskingstrategy that assigns higher masking probabilities to later tokens during training to better mimic test-time behavior. Leveraging this framework, we conduct extensive studies of dLMs'attention patterns, training dynamics, and other design choices, providing actionable insights into scalable AR-to-dLM conversion. These studies lead to theEfficient-DLM family, which outperforms state-of-the-art AR models and dLMs, e.g., our Efficient-DLM 8B achieves +5.4%/+2.7% higher accuracy with 4.5x/2.7x higher throughput compared toDream 7BandQwen3 4B, respectively. Proposes Efficient-DLM: converting autoregressive LMs to fast diffusion LMs via block-wise continuous pretraining and token masking, achieving higher accuracy and throughput than AR and existing dLMs. Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2551068",
    "title": "Efficient-DLM: From Autoregressive to Diffusion Language Models, and Beyond in Speed",
    "authors": [],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2512.14067",
    "upvote": 13
  }
}