{
  "context": "SVG-T2I framework enables high-quality text-to-image synthesis by training diffusion models within visual foundation model representation space, achieving competitive performance on benchmark datasets. Visual generation grounded in Visual Foundation Model (VFM) representations offers a highly promising unified pathway for integrating visual understanding, perception, and generation. Despite this potential, training large-scaletext-to-image diffusion modelsentirely within the VFMrepresentation spaceremains largely unexplored. To bridge this gap, we scale the SVG (Self-supervised representations for Visual Generation) framework, proposing SVG-T2I to support high-quality text-to-image synthesis directly in the VFM feature domain. By leveraging a standard text-to-imagediffusion pipeline, SVG-T2I achieves competitive performance, reaching 0.75 onGenEvaland 85.78 onDPG-Bench. This performance validates the intrinsic representational power of VFMs for generative tasks. We fully open-source the project, including theautoencoderand generation model, together with their training, inference, evaluation pipelines, and pre-trained weights, to facilitate further research in representation-driven visual generation. Visual generation grounded in Visual Foundation Model (VFM) representations offers a highly promising unified pathway for integrating visual understanding, perception, and generation. Despite this potential, training large-scale text-to-image diffusion models entirely within the VFM representation space remains largely unexplored. To bridge this gap, we scale the SVG (Self-supervised representations for Visual Generation) framework, proposing SVG-T2I to support high-quality text-to-image synthesis directly in the VFM feature domain. By leveraging a standard text-to-image diffusion pipeline, SVG-T2I achieves competitive performance, reaching 0.75 on GenEval and 85.78 on DPG-Bench. This performance validates the intrinsic representational power of VFMs for generative tasks. We fully open-source the project, including the autoencoder and generation model, together with their training, inference, evaluation pipelines, and pre-trained weights, to facilitate further research in representation-driven visual generation. Hi everyone! üëãWe‚Äôre excited to introduceSVG-T2I, anexperimental research projectaimed at providing the community with arepresentation-based text-to-image generation frameworkfor further exploration and study. Allcode and model weights are fully open-sourced. If you find this work interesting or useful, we‚Äôd greatly appreciate your support with anUpvote on Hugging Faceand aStar on GitHub‚≠êü§ó Links: SVG-T2Iis apure VFM-based text-to-image generation frameworkthat performs diffusion modelingdirectly in the representation space, completely removing the need for traditional VAEs.The primary goal of this work is tovalidate the scalability and effectiveness of representation-based generation at scale, while also providing the community with afully open and end-to-end solution, including training code, inference and evaluation pipelines, and pre-trained checkpoints. We hope this project can serve as a useful foundation for future research onrepresentation generationand related directions. üöÄ This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend ¬∑Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2551028",
    "title": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder",
    "authors": [
      "Minglei Shi",
      "Haolin Wang",
      "Bohan Zeng"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/KlingTeam/SVG-T2I",
    "huggingface_url": "https://huggingface.co/papers/2512.11749",
    "upvote": 38
  }
}