{
  "context": "The V-REX evaluation suite assesses vision-language models' multi-step reasoning and exploration capabilities through a Chain-of-Questions framework, revealing their strengths and weaknesses in planning and following. While manyvision-language models(VLMs) are developed to answer well-defined, straightforward questions with highly specified targets, as in most benchmarks, they often struggle in practice with complex open-ended tasks, which usually require multiple rounds of exploration and reasoning in the visual space. Such visual thinking paths not only provide step-by-step exploration and verification as an AI detective but also produce better interpretations of the final answers. However, these paths are challenging to evaluate due to the large exploration space of intermediate steps. To bridge the gap, we develop an evaluation suite, ``Visual Reasoningwithmulti-step EXploration(V-REX)'', which is composed of a benchmark of challengingvisual reasoningtasks requiring nativemulti-step explorationand anevaluation protocol. V-REX covers rich application scenarios across diverse domains. V-REX casts the multi-step exploratory reasoning into aChain-of-Questions(CoQ) and disentangles VLMs' capability to (1)Planning: breaking down an open-ended task by selecting a chain of exploratory questions; and (2)Following: answering curated CoQ sequentially to collect information for deriving the final answer. By curating finite options of questions and answers per step, V-REX achieves a reliable quantitative and fine-grained analysis of the intermediate steps. By assessing SOTA proprietary and open-sourced VLMs, we reveal consistent scaling trends, significant differences betweenplanningandfollowingabilities, and substantial room for improvement in multi-step exploratory reasoning. While many vision-language models (VLMs) are developed to answer well-defined, straightforward questions with highly specified targets, as in most benchmarks, they often struggle in practice with complex open-ended tasks, which usually require multiple rounds of exploration and reasoning in the visual space. Such visual thinking paths not only provide step-by-step exploration and verification as an AI detective but also produce better interpretations of the final answers. However, these paths are challenging to evaluate due to the large exploration space of intermediate steps. To bridge the gap, we develop an evaluation suite, ``Visual Reasoning with multi-step EXploration (V-REX)'', which is composed of a benchmark of challenging visual reasoning tasks requiring native multi-step exploration and an evaluation protocol. V-REX covers rich application scenarios across diverse domains. V-REX casts the multi-step exploratory reasoning into a Chain-of-Questions (CoQ) and disentangles VLMs' capability to (1) Planning: breaking down an open-ended task by selecting a chain of exploratory questions; and (2) Following: answering curated CoQ sequentially to collect information for deriving the final answer. By curating finite options of questions and answers per step, V-REX achieves a reliable quantitative and fine-grained analysis of the intermediate steps. By assessing SOTA proprietary and open-sourced VLMs, we reveal consistent scaling trends, significant differences between planning and following abilities, and substantial room for improvement in multi-step exploratory reasoning. Dataset:https://huggingface.co/datasets/umd-zhou-lab/V-REX This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend arXiv lens breakdown of this paper ðŸ‘‰https://arxivlens.com/PaperView/Details/v-rex-benchmarking-exploratory-visual-reasoning-via-chain-of-questions-9397-978c5271 Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2551085",
    "title": "V-REX: Benchmarking Exploratory Visual Reasoning via Chain-of-Questions",
    "authors": [
      "Chenrui Fan",
      "Tianyi Zhou"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/tianyi-lab/VREX",
    "huggingface_url": "https://huggingface.co/papers/2512.11995",
    "upvote": 9
  }
}