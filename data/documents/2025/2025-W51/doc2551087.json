{
  "context": "VenusBench-GD is a comprehensive, bilingual benchmark for GUI grounding that spans multiple platforms, offering a hierarchical evaluation framework with extensive data coverage and rich annotations. GUI groundingis a critical component in building capable GUI agents. However, existing grounding benchmarks suffer from significant limitations: they either provide insufficient data volume and narrow domain coverage, or focus excessively on a single platform and require highly specialized domain knowledge. In this work, we presentVenusBench-GD, a comprehensive, bilingual benchmark forGUI groundingthat spans multiple platforms, enabling hierarchical evaluation for real-word applications.VenusBench-GDcontributes as follows: (i) we introduce a large-scale,cross-platform benchmarkwith extensive coverage of applications, diverse UI elements, and rich annotated data, (ii) we establish ahigh-quality data construction pipelinefor grounding tasks, achieving higher annotation accuracy than existing benchmarks, and (iii) we extend the scope of element grounding by proposing ahierarchical task taxonomythat divides grounding into basic and advanced categories, encompassing six distinct subtasks designed to evaluate models from complementary perspectives. Our experimental findings reveal critical insights: general-purposemultimodal modelsnow match or even surpass specialized GUI models on basic grounding tasks. In contrast, advanced tasks, still favorGUI-specialized models, though they exhibit significantoverfittingand poorrobustness. These results underscore the necessity of comprehensive, multi-tieredevaluation frameworks. GUI grounding is a critical component in building capable GUI agents. However, existing grounding benchmarks suffer from significant limitations: they either provide insufficient data volume and narrow domain coverage, or focus excessively on a single platform and require highly specialized domain knowledge. In this work, we present VenusBench-GD, a comprehensive, bilingual benchmark for GUI grounding that spans multiple platforms, enabling hierarchical evaluation for real-word applications. VenusBench-GD contributes as follows: (i) we introduce a large-scale, cross-platform benchmark with extensive coverage of applications, diverse UI elements, and rich annotated data, (ii) we establish a high-quality data construction pipeline for grounding tasks, achieving higher annotation accuracy than existing benchmarks, and (iii) we extend the scope of element grounding by proposing a hierarchical task taxonomy that divides grounding into basic and advanced categories, encompassing six distinct subtasks designed to evaluate models from complementary perspectives. Our experimental findings reveal critical insights: general-purpose multimodal models now match or even surpass specialized GUI models on basic grounding tasks. In contrast, advanced tasks, still favor GUI-specialized models, though they exhibit significant overfitting and poor robustness. These results underscore the necessity of comprehensive, multi-tiered evaluation frameworks. arXiv lens breakdown of this paper ðŸ‘‰https://arxivlens.com/PaperView/Details/venusbench-gd-a-comprehensive-multi-platform-gui-benchmark-for-diverse-grounding-tasks-1573-031154db Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2551087",
    "title": "VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks",
    "authors": [
      "Zhangxuan Gu"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2512.16501",
    "upvote": 8
  }
}