{
  "context": "A self-evolving training pipeline with the Calibrated Step Reward System and GUI-MCP protocol improve GUI automation efficiency, accuracy, and privacy in real-world scenarios. Recent advances inmultimodal large language modelsunlock unprecedented opportunities forGUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training data while maintaining annotation reliability? We introduce aself-evolving training pipelinepowered by theCalibrated Step Reward System, which converts model-generated trajectories into reliable training signals throughtrajectory-level calibration, achieving >90% annotation accuracy with 10-100x lower cost. Leveraging this pipeline, we introduceStep-GUI, a family of models (4B/8B) that achieves state-of-the-artGUI performance(8B: 80.2%AndroidWorld, 48.5%OSWorld, 62.6%ScreenShot-Pro) while maintaining robust general capabilities. As GUI agent capabilities improve, practical deployment demands standardized interfaces across heterogeneous devices while protecting user privacy. To this end, we proposeGUI-MCP, the firstModel Context ProtocolforGUI automationwithhierarchical architecturethat combineslow-level atomic operationsandhigh-level task delegationtolocal specialist models, enablinghigh-privacy executionwhere sensitive data stays on-device. Finally, to assess whether agents can handle authentic everyday usage, we introduceAndroidDaily, a benchmark grounded inreal-world mobile usage patternswith 3146 static actions and 235 end-to-end tasks across high-frequency daily scenarios (8B: static 89.91%, end-to-end 52.50%). Our work advances the development of practical GUI agents and demonstrates strong potential for real-world deployment in everyday digital interactions. Recent advances in multimodal large language models unlock unprecedented opportunities for GUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training data while maintaining annotation reliability? We introduce a self-evolving training pipeline powered by the Calibrated Step Reward System, which converts model-generated trajectories into reliable training signals through trajectory-level calibration, achieving >90% annotation accuracy with 10-100x lower cost. Leveraging this pipeline, we introduce Step-GUI, a family of models (4B/8B) that achieves state-of-the-art GUI performance (8B: 80.2% AndroidWorld, 48.5% OSWorld, 62.6% ScreenShot-Pro) while maintaining robust general capabilities. As GUI agent capabilities improve, practical deployment demands standardized interfaces across heterogeneous devices while protecting user privacy. To this end, we propose GUI-MCP, the first Model Context Protocol for GUI automation with hierarchical architecture that combines low-level atomic operations and high-level task delegation to local specialist models, enabling high-privacy execution where sensitive data stays on-device. Finally, to assess whether agents can handle authentic everyday usage, we introduce AndroidDaily, a benchmark grounded in real-world mobile usage patterns with 3146 static actions and 235 end-to-end tasks across high-frequency daily scenarios (8B: static 89.91%, end-to-end 52.50%). Our work advances the development of practical GUI agents and demonstrates strong potential for real-world deployment in everyday digital interactions. arXiv lens breakdown of this paper ðŸ‘‰https://arxivlens.com/PaperView/Details/step-gui-technical-report-78-2ff7bee9 arXiv explained breakdown of this paper ðŸ‘‰https://arxivexplained.com/papers/step-gui-technical-report Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2551003",
    "title": "Step-GUI Technical Report",
    "authors": [
      "Ziyang Meng"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/stepfun-ai/gelab-zero",
    "huggingface_url": "https://huggingface.co/papers/2512.15431",
    "upvote": 128
  }
}