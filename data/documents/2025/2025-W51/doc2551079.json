{
  "context": "RePlan, a plan-then-execute framework, enhances instruction-based image editing by combining a vision-language planner with a diffusion editor, achieving superior performance in complex and intricate editing tasks using limited data. Instruction-based image editingenables natural-language control over visual modifications, yet existing models falter under Instruction-Visual Complexity (IV-Complexity), where intricate instructions meet cluttered or ambiguous scenes. We introduce RePlan (Region-aligned Planning), a plan-then-execute framework that couples avision-language plannerwith adiffusion editor. The planner decomposes instructions viastep-by-step reasoningand explicitly grounds them to target regions; the editor then applies changes using a training-freeattention-region injection mechanism, enabling precise, parallel multi-region edits without iterative inpainting. To strengthen planning, we applyGRPO-based reinforcement learningusing 1K instruction-only examples, yielding substantial gains in reasoning fidelity and format reliability. We further present IV-Edit, a benchmark focused on fine-grained grounding and knowledge-intensive edits. Across IV-Complex settings, RePlan consistently outperforms strong baselines trained on far larger datasets, improvingregional precisionandoverall fidelity. Our project page: https://replan-iv-edit.github.io Current instruction-based editing models struggle when intricate instructions meet cluttered, realistic scenes‚Äîa challenge we define asInstruction-Visual Complexity (IV-Complexity). In these scenarios, high-level global context is insufficient to distinguish specific targets from semantically similar objects (e.g., distinguishing a \"used cup\" from a clean glass on a messy desk). Existing methods, including unified VLM-diffusion architectures, predominantly rely onGlobal Semantic Guidance. They compress instructions into global feature vectors, lacking spatial grounding. Consequently, edits often \"spill over\" into unrelated areas or modify the wrong targets, failing to preserve background consistency. RePlanintroduces aPlan-then-Executeframework that explicitly links text to pixels. Our key contributions include: üß± Reasoning-Guided Planning A VLM planner performsChain-of-Thought (CoT) reasoningto decompose complex instructions into structured, region-specific guidance (Bounding Boxes + Local Hints). üéØ Training-Free Attention Injection We introduce a mechanism tailored for Multimodal DiT (MMDiT) that executes edits via region-constrained attention. This enablesprecise, multi-region parallel editsin a single pass while preserving the background, without requiring any training of the DiT backbone. ‚ö° Efficient GRPO Training We enhance the planner's reasoning capabilities usingGroup Relative Policy Optimization (GRPO). Remarkably, we achieve strong planning performance using only~1k instruction-only samples, bypassing the need for large-scale paired image datasets. üéõÔ∏è Interactive & Flexible Editing RePlan's intermediate region guidance isfully editable, enabling user-in-the-loop intervention. Users can adjust bounding boxes or hints directly to refine results. Furthermore, our attention mechanism supportsregional negative promptsto prevent bleeding effects. üìä IV-Edit Benchmark To foster future research, we establishIV-Edit, the first benchmark specifically designed to evaluate IV-Complex editing, filling the gap left by current subject-dominated evaluation sets. arXiv lens breakdown of this paper üëâhttps://arxivlens.com/PaperView/Details/replan-reasoning-guided-region-planning-for-complex-instruction-based-image-editing-9525-fdf98056 ¬∑Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2551079",
    "title": "RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing",
    "authors": [
      "Tianyuan Qu"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/dvlab-research/RePlan",
    "huggingface_url": "https://huggingface.co/papers/2512.16864",
    "upvote": 10
  }
}