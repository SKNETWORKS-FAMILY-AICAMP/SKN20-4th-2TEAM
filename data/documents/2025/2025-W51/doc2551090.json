{
  "context": "EVOLVE-VLA, a test-time training framework for Vision-Language-Action models, enables continuous adaptation through environmental interaction with minimal task-specific demonstrations, achieving significant improvements in performance and generalization. Achieving truly adaptive embodied intelligence requires agents that learn not just by imitating static demonstrations, but by continuously improving through environmental interaction, which is akin to how humans master skills through practice. Vision-Language-Action (VLA) models have advanced robotic manipulation by leveraging large language models, yet remain fundamentally limited bySupervised Finetuning(SFT): requiring hundreds of demonstrations per task, rigidly memorizing trajectories, and failing to adapt when deployment conditions deviate from training. We introduce EVOLVE-VLA, atest-time trainingframework enabling VLAs to continuously adapt through environment interaction with minimal or zero task-specific demonstrations. The key technical challenge is replacing oracle reward signals (unavailable at test time) withautonomous feedback. We address this through a learnedprogress estimatorproviding dense feedback, and critically, we design our framework to ``tame'' this inherently noisy signal via two mechanisms: (1) anaccumulative progress estimationmechanism smoothing noisy point-wise estimates, and (2) aprogressive horizon extensionstrategy enabling gradualpolicy evolution. EVOLVE-VLA achieves substantial gains: +8.6\\% onlong-horizon tasks, +22.0\\% in1-shot learning, and enablescross-task generalization-- achieving 20.8\\% success on unseen tasks without task-specific demonstrations training (vs. 0\\% for pure SFT). Qualitative analysis reveals emergent capabilities absent in demonstrations, includingerror recoveryandnovel strategies. This work represents a critical step toward VLAs that truly learn and adapt, moving beyond static imitation toward continuous self-improvements. EVOLVE-VLA is a test-time training framework that enables Vision-Language-Action models to continuously adapt through environment interaction with minimal or no task-specific demonstrations, overcoming the limitations of static supervised finetuning. By using a learned progress estimator with mechanisms to stabilize noisy feedback, it achieves significant performance gains, cross-task generalization, and emergent adaptive behaviors such as error recovery. Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2551090",
    "title": "EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models",
    "authors": [],
    "publication_year": 2025,
    "github_url": "https://github.com/showlab/EVOLVE-VLA",
    "huggingface_url": "https://huggingface.co/papers/2512.14666",
    "upvote": 8
  }
}