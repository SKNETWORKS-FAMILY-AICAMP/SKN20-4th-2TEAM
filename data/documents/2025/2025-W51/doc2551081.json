{
  "context": "DrivePI, a spatial-aware 4D multi-modal large language model, achieves state-of-the-art performance in 3D perception, prediction, and planning for autonomous driving by integrating point clouds, images, and language instructions. Althoughmulti-modal large language models(MLLMs) have shown strong capabilities across diverse domains, their application in generating fine-grained3D perceptionandpredictionoutputs in autonomous driving remains underexplored. In this paper, we proposeDrivePI, a novel spatial-aware 4DMLLMthat serves as a unifiedVision-Language-Action(VLA) framework that is also compatible withvision-action(VA) models. Our method jointly performsspatial understanding,3D perception(i.e.,3D occupancy),prediction(i.e.,occupancy flow), andplanning(i.e., action outputs) in parallel throughend-to-end optimization. To obtain both precise geometric information and rich visual appearance, our approach integratespoint clouds,multi-view images, andlanguage instructionswithin aunified MLLM architecture. We further develop adata engineto generatetext-occupancyandtext-flow QA pairsfor4D spatial understanding. Remarkably, with only a 0.5BQwen2.5model asMLLMbackbone,DrivePIas a single unified model matches or exceeds both existingVLAmodels and specializedVAmodels. Specifically, compared toVLAmodels,DrivePIoutperformsOpenDriveVLA-7Bby 2.5% mean accuracy onnuScenes-QAand reduces collision rate by 70% overORION(from 0.37% to 0.11%) onnuScenes. Against specializedVAmodels,DrivePIsurpassesFB-OCCby 10.3 RayIoU for3D occupancyonOpenOcc, reduces themAVEfrom 0.591 to 0.509 foroccupancy flowonOpenOcc, and achieves 32% lowerL2 errorthanVAD (from 0.72m to 0.49m) forplanningonnuScenes. Code will be available at https://github.com/happinesslz/DrivePI Although multi-modal large language models (MLLMs) have shown strong capabilities across diverse domains, their application in generating fine-grained 3D perception and prediction outputs in autonomous driving remains underexplored. In this paper, we propose DrivePI, a novel spatial-aware 4D MLLM that serves as a unified Vision-Language-Action (VLA) framework that is also compatible with vision-action (VA) models. Our method jointly performs spatial understanding, 3D perception (i.e., 3D occupancy), prediction (i.e., occupancy flow), and planning (i.e., action outputs) in parallel through end-to-end optimization. To obtain both precise geometric information and rich visual appearance, our approach integrates point clouds, multi-view images, and language instructions within a unified MLLM architecture. We further develop a data engine to generate text-occupancy and text-flow QA pairs for 4D spatial understanding. Remarkably, with only a 0.5B Qwen2.5 model as MLLM backbone, DrivePI as a single unified model matches or exceeds both existing VLA models and specialized VA models. Specifically, compared to VLA models, DrivePI outperforms OpenDriveVLA-7B by 2.5% mean accuracy on nuScenes-QA and reduces collision rate by 70% over ORION (from 0.37% to 0.11%) on nuScenes. Against specialized VA models, DrivePI surpasses FB-OCC by 10.3 RayIoU for 3D occupancy on OpenOcc, reduces the mAVE from 0.591 to 0.509 for occupancy flow on OpenOcc, and achieves 32% lower L2 error than VAD (from 0.72m to 0.49m) for planning on nuScenes. Code will be available athttps://github.com/happinesslz/DrivePI This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2551081",
    "title": "DrivePI: Spatial-aware 4D MLLM for Unified Autonomous Driving Understanding, Perception, Prediction and Planning",
    "authors": [
      "Zhe Liu"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/happinesslz/DrivePI",
    "huggingface_url": "https://huggingface.co/papers/2512.12799",
    "upvote": 10
  }
}