{
  "context": "NL2Repo Bench evaluates long-horizon software development capabilities of coding agents by assessing their ability to generate complete Python libraries from natural-language requirements. Recent advances incoding agentssuggest rapid progress towardautonomous software development, yet existing benchmarks fail to rigorously evaluate the long-horizon capabilities required to build complete software systems. Most prior evaluations focus on localized code generation, scaffolded completion, or short-term repair tasks, leaving open the question of whether agents can sustain coherent reasoning, planning, and execution over the extended horizons demanded by real-world repository construction. To address this gap, we presentNL2Repo Bench, a benchmark explicitly designed to evaluate thelong-horizon repository generationability ofcoding agents. Given only a singlenatural-language requirementsdocument and an empty workspace, agents must autonomously design the architecture, manage dependencies, implementmulti-module logic, and produce a fullyinstallable Python library. Our experiments across state-of-the-art open- and closed-source models reveal thatlong-horizon repository generationremains largely unsolved: even the strongest agents achieve below 40% averagetest pass ratesand rarely complete an entire repository correctly. Detailed analysis uncovers fundamental long-horizon failure modes, including premature termination, loss ofglobal coherence, fragilecross-file dependencies, and inadequate planning over hundreds of interaction steps.NL2Repo Benchestablishes a rigorous, verifiable testbed for measuring sustained agentic competence and highlightslong-horizon reasoningas a central bottleneck for the next generation of autonomouscoding agents. Recent advances in coding agents suggest rapid progress toward autonomous software development, yet existing benchmarks fail to rigorously evaluate the long-horizon capabilities required to build complete software systems. Most prior evaluations focus on localized code generation, scaffolded completion, or short-term repair tasks, leaving open the question of whether agents can sustain coherent reasoning, planning, and execution over the extended horizons demanded by real-world repository construction. To address this gap, we present NL2Repo Bench, a benchmark explicitly designed to evaluate the long-horizon repository generation ability of coding agents. Given only a single natural-language requirements document and an empty workspace, agents must autonomously design the architecture, manage dependencies, implement multi-module logic, and produce a fully installable Python library. Our experiments across state-of-the-art open- and closed-source models reveal that long-horizon repository generation remains largely unsolved: even the strongest agents achieve below 40% average test pass rates and rarely complete an entire repository correctly. Detailed analysis uncovers fundamental long-horizon failure modes, including premature termination, loss of global coherence, fragile cross-file dependencies, and inadequate planning over hundreds of interaction steps. NL2Repo Bench establishes a rigorous, verifiable testbed for measuring sustained agentic competence and highlights long-horizon reasoning as a central bottleneck for the next generation of autonomous coding agents. https://github.com/multimodal-art-projection/NL2RepoBench This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2551017",
    "title": "NL2Repo-Bench: Towards Long-Horizon Repository Generation Evaluation of Coding Agents",
    "authors": [
      "Jingzhe Ding",
      "Xiang Gao",
      "Yue Hou",
      "Kai Hua"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/multimodal-art-projection/NL2RepoBench",
    "huggingface_url": "https://huggingface.co/papers/2512.12730",
    "upvote": 43
  }
}