{
  "context": "Sage is a human-free evaluation suite for LLM-as-a-Judge, using rational choice theory to assess local and global consistency, revealing significant reliability issues with current LLM judges. LLM-as-a-Judgehas been widely adopted as an evaluation method and served as supervised rewards in model training. However, existing benchmarks forLLM-as-a-Judgeare mainly relying onhuman-annotated ground truth, which introduceshuman biasthat undermines the assessment of reliability and imposesscalability constraints. To overcome these limitations, we introduce Sage, a novel evaluation suite that assesses the quality of LLM judges without necessitating any human annotation. Inspired by axioms ofrational choice theory, Sage introduces two new lenses for measuringLLM-as-a-Judge:local self-consistency(pair-wise preference stability) andglobal logical consistency(transitivityacross a full set of preferences). We curate a dataset of 650 questions by combining structured benchmark problems with real-world user queries. Our experiments demonstrate both the stability of our metrics and their high correlation with supervised benchmarks likeLLMBarandRewardBench2, confirming Sage's reliability as an evaluation suite for the robustness and accuracy ofLLM-as-a-Judge. Based on Sage, we reveal that current state-of-the-art LLMs exhibit significantreliability problemswhen acting as judges in bothscoringandpairwise settings; even the top-performing models, Gemini-2.5-Pro and GPT-5, fail to maintain consistent preferences in nearly a quarter of difficult cases. We attribute this to a new phenomenon calledsituational preference, which explains why explicit rubrics or criteria can help the model judge consistently across answer pairs. Our further analysis shows thatfinetuned LLM-as-a-Judgeis a feasible method to boost performance, and thepanel-based judgeas well asdeep reasoningcan enhance the judging consistency. We also find substantial inconsistency inhuman judgments, which indicates that human annotation may not be a reliable gold standard. We argue that evaluating LLM-as-a-Judge is biased by human-annotated ground truth, rethink the evaluation of LLM-as-a-Judge, and design metrics that do not need human annotations.  arXiv lens breakdown of this paper ðŸ‘‰https://arxivlens.com/PaperView/Details/are-we-on-the-right-way-to-assessing-llm-as-a-judge-1377-214f73ba Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2552021",
    "title": "Are We on the Right Way to Assessing LLM-as-a-Judge?",
    "authors": [],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2512.16041",
    "upvote": 32
  }
}