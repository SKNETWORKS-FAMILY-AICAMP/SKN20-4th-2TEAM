{
  "context": "SWE-Bench++ is an automated framework generating repository-level coding tasks from live GitHub pull requests, offering a scalable, multilingual benchmark for evaluating and improving code generation models. Benchmarks likeSWE-benchhave standardized the evaluation ofLarge Language Models(LLMs) on repository-level software engineering tasks. However, these efforts remain limited by manual curation, static datasets, and a focus on Python-based bug fixes. We introduceSWE-Bench++, an automated framework that generates repository-level coding tasks from open-source GitHub projects. Unlike synthetic approaches, our pipeline harvests live pull requests to cover both bug fixes and feature requests across 11 languages.SWE-Bench++turnsGitHub pull requests(PRs) into reproducible, execution-based tasks via four stages:programmatic sourcing,environment synthesis,test oracle extraction, andquality assurance. A finalhint-guided trajectory synthesisstep converts instances that strong models fail on into training trajectories. Our initial benchmark consists of 11,133 instances from 3,971 repositories across 11 languages. On a subset of 1,782 instances of this benchmark, today's strongest models perform as follows: claude-sonnet-4.5 achieves 36.20%pass@10, gpt-5-2025-08-07 34.57%, gemini/gemini-2.5-pro 24.92%, and gpt-4o 16.89%. We further demonstrate the utility of our dataset by showing that fine-tuning onSWE-Bench++instances yields measurable improvements on theSWE-bench Multilingualbenchmark.SWE-Bench++provides a scalable, multilingual benchmark for evaluating and improving repository-level code generation. Benchmarks like SWE-bench have standardized the evaluation of Large Language Models (LLMs) on repository-level software engineering tasks. However, these efforts remain limited by manual curation, static datasets, and a focus on Python-based bug fixes. We introduce SWE-Bench++, an automated framework that generates repository-level coding tasks from open-source GitHub projects. Unlike synthetic approaches, our pipeline harvests live pull requests to cover both bug fixes and feature requests across 11 languages. SWE-Bench++ turns GitHub pull requests (PRs) into reproducible, execution-based tasks via four stages: programmatic sourcing, environment synthesis, test oracle extraction, and quality assurance. A final hint-guided trajectory synthesis step converts instances that strong models fail on into training trajectories. Our initial benchmark consists of 11,133 instances from 3,971 repositories across 11 languages. On a subset of 1,782 instances of this benchmark, today's strongest models perform as follows: claude-sonnet-4.5 achieves 36.20% pass@10, gpt-5-2025-08-07 34.57%, gemini/gemini-2.5-pro 24.92%, and gpt-4o 16.89%. We further demonstrate the utility of our dataset by showing that fine-tuning on SWE-Bench++ instances yields measurable improvements on the SWE-bench Multilingual benchmark. SWE-Bench++ provides a scalable, multilingual benchmark for evaluating and improving repository-level code generation. arXiv lens breakdown of this paper ðŸ‘‰https://arxivlens.com/PaperView/Details/swe-bench-a-framework-for-the-scalable-generation-of-software-engineering-benchmarks-from-open-source-repositories-4684-43c43e1f Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2552068",
    "title": "SWE-Bench++: A Framework for the Scalable Generation of Software Engineering Benchmarks from Open-Source Repositories",
    "authors": [],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2512.17419",
    "upvote": 9
  }
}