{
  "context": "Systematic exploration of scaling laws for multilingual code pre-training reveals language-specific benefits and proposes a strategy for optimal token allocation across programming languages. Code large language models (Code LLMs) are powerful but costly to train, withscaling lawspredicting performance from model size, data, and compute. However, differentprogramming languages(PLs) have varying impacts duringpre-trainingthat significantly affect base model performance, leading to inaccurate performance prediction. Besides, existing works focus on language-agnostic settings, neglecting the inherently multilingual nature of modern software development. Therefore, it is first necessary to investigate thescaling lawsof different PLs, and then consider their mutual influences to arrive at the final multilingual scaling law. In this paper, we present the first systematic exploration ofscaling lawsfor multilingual codepre-training, conducting over 1000+ experiments (Equivalent to 336,000+ H800 hours) across multiple PLs, model sizes (0.2B to 14B parameters), and dataset sizes (1T tokens). We establish comprehensivescaling lawsforcode LLMsacross multiple PLs, revealing thatinterpreted languages(e.g., Python) benefit more from increased model size and data thancompiled languages(e.g., Rust). The study demonstrates thatmultilingual pre-trainingprovides synergistic benefits, particularly between syntactically similar PLs. Further, thepre-trainingstrategy of theparallel pairing(concatenating code snippets with their translations) significantly enhances cross-lingual abilities with favorable scaling properties. Finally, aproportion-dependent multilingual scaling lawis proposed to optimally allocate training tokens by prioritizing high-utility PLs (e.g., Python), balancing high-synergy pairs (e.g., JavaScript-TypeScript), and reducing allocation to fast-saturating languages (Rust), achieving superior average performance across all PLs compared to uniform distribution under the same compute budget. Code large language models (Code LLMs) are powerful but costly to train, with scaling laws predicting performance from model size, data, and compute. However, different programming languages (PLs) have varying impacts during pre-training that significantly affect base model performance, leading to inaccurate performance prediction. Besides, existing works focus on language-agnostic settings, neglecting the inherently multilingual nature of modern software development. Therefore, it is first necessary to investigate the scaling laws of different PLs, and then consider their mutual influences to arrive at the final multilingual scaling law. In this paper, we present the first systematic exploration of scaling laws for multilingual code pre-training, conducting over 1000+ experiments (Equivalent to 336,000+ H800 hours) across multiple PLs, model sizes (0.2B to 14B parameters), and dataset sizes (1T tokens). We establish comprehensive scaling laws for code LLMs across multiple PLs, revealing that interpreted languages (e.g., Python) benefit more from increased model size and data than compiled languages (e.g., Rust). The study demonstrates that multilingual pre-training provides synergistic benefits, particularly between syntactically similar PLs. Further, the pre-training strategy of the parallel pairing (concatenating code snippets with their translations) significantly enhances cross-lingual abilities with favorable scaling properties. Finally, a proportion-dependent multilingual scaling law is proposed to optimally allocate training tokens by prioritizing high-utility PLs (e.g., Python), balancing high-synergy pairs (e.g., JavaScript-TypeScript), and reducing allocation to fast-saturating languages (Rust), achieving superior average performance across all PLs compared to uniform distribution under the same compute budget. arXiv lens breakdown of this paper ðŸ‘‰https://arxivlens.com/PaperView/Details/scaling-laws-for-code-every-programming-language-matters-679-be47b574 Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2552069",
    "title": "Scaling Laws for Code: Every Programming Language Matters",
    "authors": [],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2512.13472",
    "upvote": 9
  }
}