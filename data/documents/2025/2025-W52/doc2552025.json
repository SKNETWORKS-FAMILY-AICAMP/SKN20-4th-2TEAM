{
  "context": "Vision-language models exhibit significant popularity bias in architectural recognition, performing poorly on less-known structures despite high accuracy on famous buildings. We expose a significant popularity bias in state-of-the-artvision-language models(VLMs), which achieve up to 34% higher accuracy on famous buildings compared to ordinary ones, indicating a reliance onmemorizationovergeneralizable understanding. To systematically investigate this, we introduce the largest open benchmark for this task: theYearGuessr dataset, a collection of 55,546 building images with multi-modal attributes from 157 countries, annotated with continuous ordinal labels of their construction year (1001-2024), GPS data, and page-view counts as a proxy for popularity. Using this dataset, we frame theconstruction year predictiontask asordinal regressionand introducepopularity-aware interval accuracy metricsto quantify this bias. Our resulting benchmark of 30+ models, including our YearCLIP model, confirms that VLMs excel on popular, memorized items but struggle significantly with unrecognized subjects, exposing a critical flaw in their reasoning capabilities. Project page: https://sytwu.github.io/BeyondMemo/ We expose a significant popularity bias in state-of-the-art vision-language models (VLMs), which achieve up to 34% higher accuracy on famous buildings compared to ordinary ones, indicating a reliance on memorization over generalizable understanding. To systematically investigate this, we introduce the largest open benchmark for this task: the YearGuessr dataset, a collection of 55,546 building images with multi-modal attributes from 157 countries, annotated with continuous ordinal labels of their construction year (1001-2024), GPS data, and page-view counts as a proxy for popularity. Using this dataset, we frame the construction year prediction task as ordinal regression and introduce popularity-aware interval accuracy metrics to quantify this bias. Our resulting benchmark of 30+ models, including our YearCLIP model, confirms that VLMs excel on popular, memorized items but struggle significantly with unrecognized subjects, exposing a critical flaw in their reasoning capabilities. Project page:https://sytwu.github.io/BeyondMemo/ arXiv lens breakdown of this paper ðŸ‘‰https://arxivlens.com/PaperView/Details/beyond-memorization-a-multi-modal-ordinal-regression-benchmark-to-expose-popularity-bias-in-vision-language-models-3678-b13eb442 Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2552025",
    "title": "Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models",
    "authors": [],
    "publication_year": 2025,
    "github_url": "https://github.com/Sytwu/BeyondMemo",
    "huggingface_url": "https://huggingface.co/papers/2512.21337",
    "upvote": 29
  }
}