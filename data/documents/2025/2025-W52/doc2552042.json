{
  "context": "StoryMem enhances multi-shot video generation with cinematic quality and long-range consistency using a memory bank and pre-trained single-shot video diffusion models. Visual storytelling requires generating multi-shot videos with cinematic quality and long-range consistency. Inspired by human memory, we proposeStoryMem, a paradigm that reformulates long-form video storytelling as iterative shot synthesis conditioned on explicit visual memory, transforming pre-trained single-shot video diffusion models into multi-shot storytellers. This is achieved by a novelMemory-to-Video (M2V)design, which maintains a compact and dynamically updatedmemory bankofkeyframesfrom historical generated shots. The stored memory is then injected into single-shot video diffusion models vialatent concatenationandnegative RoPE shiftswith onlyLoRA fine-tuning. Asemantic keyframe selectionstrategy, together withaesthetic preference filtering, further ensures informative and stable memory throughout generation. Moreover, the proposed framework naturally accommodates smoothshot transitionsand customized story generation applications. To facilitate evaluation, we introduceST-Bench, a diverse benchmark for multi-shot video storytelling. Extensive experiments demonstrate thatStoryMemachieves superiorcross-shot consistencyover previous methods while preserving highaesthetic qualityandprompt adherence, marking a significant step toward coherent minute-long video storytelling. Visual storytelling requires generating multi-shot videos with cinematic quality and long-range consistency. Inspired by human memory, we propose StoryMem, a paradigm that reformulates long-form video storytelling as iterative shot synthesis conditioned on explicit visual memory, transforming pre-trained single-shot video diffusion models into multi-shot storytellers. This is achieved by a novel Memory-to-Video (M2V) design, which maintains a compact and dynamically updated memory bank of keyframes from historical generated shots. The stored memory is then injected into single-shot video diffusion models via latent concatenation and negative RoPE shifts with only LoRA fine-tuning. A semantic keyframe selection strategy, together with aesthetic preference filtering, further ensures informative and stable memory throughout generation. Moreover, the proposed framework naturally accommodates smooth shot transitions and customized story generation applications. To facilitate evaluation, we introduce ST-Bench, a diverse benchmark for multi-shot video storytelling. Extensive experiments demonstrate that StoryMem achieves superior cross-shot consistency over previous methods while preserving high aesthetic quality and prompt adherence, marking a significant step toward coherent minute-long video storytelling. Project page:https://kevin-thu.github.io/StoryMem/Code:https://github.com/Kevin-thu/StoryMem arXiv lens breakdown of this paper ðŸ‘‰https://arxivlens.com/PaperView/Details/storymem-multi-shot-long-video-storytelling-with-memory-9131-8c565c87 Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2552042",
    "title": "StoryMem: Multi-shot Long Video Storytelling with Memory",
    "authors": [
      "Kaiwen Zhang"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/Kevin-thu/StoryMem",
    "huggingface_url": "https://huggingface.co/papers/2512.19539",
    "upvote": 17
  }
}