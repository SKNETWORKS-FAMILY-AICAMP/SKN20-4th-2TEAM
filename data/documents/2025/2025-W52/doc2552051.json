{
  "context": "Bolmo, a family of competitive byte-level language models, is trained by converting existing subword-level models, overcoming character understanding and efficiency limitations while achieving performance comparable to subword models. We introduce Bolmo, the first family of competitive fully openbyte-level language models(LMs) at the 1B and 7B parameter scales. In contrast to prior research on byte-level LMs, which focuses predominantly on training from scratch, we train Bolmo by byteifying existingsubword-level LMs.Byteificationenables overcoming the limitations of subword tokenization - such as insufficientcharacter understandingand efficiency constraints due to the fixed subword vocabulary - while performing at the level of leadingsubword-level LMs. Bolmo is specifically designed forbyteification: our architecture resolves a mismatch between the expressivity of prior byte-level architectures andsubword-level LMs, which makes it possible to employ an effectiveexact distillation objectivebetween Bolmo and the source subword model. This allows for converting a subword-level LM to a byte-level LM by investing less than 1\\% of a typical pretrainingtoken budget. Bolmo substantially outperforms all prior byte-level LMs of comparable size, and outperforms the sourcesubword-level LMsoncharacter understandingand, in some cases, coding, while coming close to matching the original LMs' performance on other tasks. Furthermore, we show that Bolmo can achieve inference speeds competitive withsubword-level LMsby training with highertoken compression ratios, and can be cheaply and effectively post-trained by leveraging the existing ecosystem around the source subword-level LM. Our results finally make byte-level LMs a practical choice competitive withsubword-level LMsacross a wide set of use cases. So cool idea to make use of mLSTM and developing this byteifying approach üòç We introduce Bolmo, the first family of competitive fully open byte-level language models (LMs) at the 1B and 7B parameter scales. In contrast to prior research on byte-level LMs, which focuses predominantly on training from scratch, we train Bolmo by byteifying existing subword-level LMs. Byteification enables overcoming the limitations of subword tokenization - such as insufficient character understanding and efficiency constraints due to the fixed subword vocabulary - while performing at the level of leading subword-level LMs. Bolmo is specifically designed for byteification: our architecture resolves a mismatch between the expressivity of prior byte-level architectures and subword-level LMs, which makes it possible to employ an effective exact distillation objective between Bolmo and the source subword model. This allows for converting a subword-level LM to a byte-level LM by investing less than 1% of a typical pretraining token budget. Bolmo substantially outperforms all prior byte-level LMs of comparable size, and outperforms the source subword-level LMs on character understanding and, in some cases, coding, while coming close to matching the original LMs' performance on other tasks. Furthermore, we show that Bolmo can achieve inference speeds competitive with subword-level LMs by training with higher token compression ratios, and can be cheaply and effectively post-trained by leveraging the existing ecosystem around the source subword-level LM. Our results finally make byte-level LMs a practical choice competitive with subword-level LMs across a wide set of use cases. arXiv lens breakdown of this paper üëâhttps://arxivlens.com/PaperView/Details/bolmo-byteifying-the-next-generation-of-language-models-5379-beae1142 ¬∑Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2552051",
    "title": "Bolmo: Byteifying the Next Generation of Language Models",
    "authors": [
      "Benjamin Minixhofer"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2512.15586",
    "upvote": 14
  }
}