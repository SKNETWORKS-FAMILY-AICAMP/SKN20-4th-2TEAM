{
  "context": "Latent diffusion models using representation encoder features face challenges in semantic compactness and pixel-level reconstruction, which are addressed through a semantic-pixel reconstruction objective that enables compact yet semantically rich representations for unified text-to-image and image editing tasks. ModernLatent Diffusion Models(LDMs) typically operate in low-levelVariational Autoencoder(VAE) latent spaces that are primarily optimized for pixel-level reconstruction. To unify vision generation and understanding, a burgeoning trend is to adopt high-dimensional features fromrepresentation encodersasgenerative latents. However, we empirically identify two fundamental obstacles in this paradigm: (1) the discriminative feature space lacks compact regularization, making diffusion models prone tooff-manifold latentsthat lead to inaccurate object structures; and (2) the encoder's inherently weak pixel-level reconstruction hinders the generator from learning accuratefine-grained geometryandtexture. In this paper, we propose a systematic framework to adapt understanding-oriented encoder features for generative tasks. We introduce asemantic-pixel reconstructionobjective to regularize the latent space, enabling the compression of both semantic information and fine-grained details into a highlycompact representation(96 channels with 16x16 spatial downsampling). This design ensures that the latent space remains semantically rich and achieves state-of-the-art image reconstruction, while remaining compact enough for accurate generation. Leveraging this representation, we design a unifiedText-to-Image(T2I) andimage editingmodel. Benchmarking against various feature spaces, we demonstrate that our approach achievesstate-of-the-art reconstruction, fasterconvergence, and substantial performance gains in both T2I and editing tasks, validating thatrepresentation encoderscan be effectively adapted into robust generative components. Modern Latent Diffusion Models (LDMs) typically operate in low-level Variational Autoencoder (VAE) latent spaces that are primarily optimized for pixel-level reconstruction. To unify vision generation and understanding, a burgeoning trend is to adopt high-dimensional features from representation encoders as generative latents. However, we empirically identify two fundamental obstacles in this paradigm: (1) the discriminative feature space lacks compact regularization, making diffusion models prone to off-manifold latents that lead to inaccurate object structures; and (2) the encoder's inherently weak pixel-level reconstruction hinders the generator from learning accurate fine-grained geometry and texture. In this paper, we propose a systematic framework to adapt understanding-oriented encoder features for generative tasks. We introduce a semantic-pixel reconstruction objective to regularize the latent space, enabling the compression of both semantic information and fine-grained details into a highly compact representation (96 channels with 16x16 spatial downsampling). This design ensures that the latent space remains semantically rich and achieves state-of-the-art image reconstruction, while remaining compact enough for accurate generation. Leveraging this representation, we design a unified Text-to-Image (T2I) and image editing model. Benchmarking against various feature spaces, we demonstrate that our approach achieves state-of-the-art reconstruction, faster convergence, and substantial performance gains in both T2I and editing tasks, validating that representation encoders can be effectively adapted into robust generative components. Project Page:https://jshilong.github.io/PS-VAE-PAGE/ great work! Great work!  Do you have any plans to compare SVG in future experiments? Thanks for the question! SVG is indeed a excellent concurrent work to RAE, and we briefly discussed it in the Related Work section. Due to limited compute resources, we did not run SVG experiments in this paper. I do plan to add a comparison once I regain access to sufficient compute, but this may take some time since Iâ€™ve left Adobe and currently donâ€™t have large-scale resources available. From a modeling perspective, SVG largely follows a paradigm similar to RAE. While it introduces additional channels, the reconstruction quality still falls short of practical requirements such as image editing.Moreover, its raw latent space is not very compact and suffers from off-manifold issues, again similar to RAE. This can also be seen in their latest T2I results: even when scaling to high resolutions, structural artifacts (e.g., broken limbs) persist, which are notoriously difficult to resolve. Adobe explored similar directions much earlier (e.g., with Qwenvl encoders) and observed the same limitations. arXiv lens breakdown of this paper ðŸ‘‰https://arxivlens.com/PaperView/Details/both-semantics-and-reconstruction-matter-making-representation-encoders-ready-for-text-to-image-generation-and-editing-5452-fd0fed96 Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2552019",
    "title": "Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing",
    "authors": [],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2512.17909",
    "upvote": 36
  }
}