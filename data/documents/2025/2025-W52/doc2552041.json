{
  "context": "IPC is an unsupervised framework that uses internal probing of large language models to generate code without labeled datasets, achieving competitive performance with reduced resource dependency. Large language models(LLMs) have demonstrated remarkable capabilities in code generation tasks. However, their effectiveness heavily relies on supervised training with extensive labeled (e.g., question-answering pairs) or unlabeled datasets (e.g., code snippets), which are often expensive and difficult to obtain at scale. To address this limitation, this paper introduces a method IPC, anunsupervised frameworkthat leveragesInternal Probingof LLMs for Code generation without any external corpus, even unlabeled code snippets. We introduce theproblem space probing,test understanding probing,solution space probing, andknowledge consolidationandreinforcementto probe the internal knowledge and confidence patterns existing in LLMs. Further, IPC identifies reliable code candidates throughself-consistency mechanismsandrepresentation-based quality estimationto trainUCoder(coder with unsupervised learning). We validate the proposed approach across multiplecode benchmarks, demonstrating that unsupervised methods can achieve competitive performance compared to supervised approaches while significantly reducing the dependency on labeled data and computational resources. Analytic experiments reveal thatinternal model statescontain rich signals aboutcode qualityandcorrectness, and that properly harnessing these signals enables effective unsupervised learning for code generation tasks, opening new directions for training code LLMs inresource-constrained scenarios. This paper introduces UCoder, an unsupervised framework for training code-generating large language models without requiring any external datasets, including unlabeled code snippets. The approach, called IPC (Internal Probing of LLMs for Code generation), leverages latent programming knowledge already present in pre-trained models through a six-stage self-bootstrapping process: (1-3) problem space probing that generates diverse algorithmic problems with specifications, (4) test understanding probing to create comprehensive test suites, (5) solution space probing using dense sampling (128 candidates per problem), and (6) knowledge consolidation through supervised fine-tuning on high-quality solutions. The key innovation is execution-driven consensus clustering, which identifies correct implementations by finding clusters of behaviorally identical solutionsâ€”correct code naturally clusters together while incorrect solutions fail heterogeneously. Experiments on UCoder models (7B, 14B, 32B parameters) demonstrate competitive performance with supervised baselines across multiple benchmarks (HumanEval, MBPP, BigCodeBench, LiveCodeBench, FullStackBench), with smaller models showing greater improvement gains (inverse scaling). The work proves that self-generated data maintains lexical, semantic, and structural diversity sufficient for effective learning, opening possibilities for resource-efficient LLM training without human annotation. arXiv lens breakdown of this paper ðŸ‘‰https://arxivlens.com/PaperView/Details/ucoder-unsupervised-code-generation-by-internal-probing-of-large-language-models-4627-eb4abc20 Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2552041",
    "title": "UCoder: Unsupervised Code Generation by Internal Probing of Large Language Models",
    "authors": [
      "Jiajun Wu",
      "Wei Zhang"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2512.17385",
    "upvote": 18
  }
}