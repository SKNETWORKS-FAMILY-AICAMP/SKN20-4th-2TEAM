{
  "context": "The paper decomposes the policy of large language models into internal layer and modular policies, revealing distinct reasoning patterns across layers and proposing Bottom-up Policy Optimization to enhance performance on complex reasoning tasks. Existingreinforcement learning(RL) approaches treatlarge language models(LLMs) as a single unified policy, overlooking their internal mechanisms. Understanding how policy evolves across layers and modules is therefore crucial for enabling more targeted optimization and raveling out complex reasoning mechanisms. In this paper, we decompose the language model policy by leveraging the intrinsic split of theTransformer residual streamand the equivalence between the composition of hidden states with theunembedding matrixand the resulting samplable policy. This decomposition revealsInternal Layer Policies, corresponding to contributions from individual layers, andInternal Modular Policies, which align with theself-attentionandfeed-forward network(FFN) components within each layer. By analyzing theentropyof internal policy, we find that: (a) Early layers keep highentropyfor exploration, top layers converge to near-zeroentropyfor refinement, with convergence patterns varying across model series. (b) LLama's prediction space rapidly converges in the final layer, whereas Qwen-series models, especially Qwen3, exhibit a more human-like, progressively structured reasoning pattern. Motivated by these findings, we proposeBottom-up Policy Optimization(BuPO), a novel RL paradigm that directly optimizes the internal layer policy during early training. By aligning training objective at lower layer, BuPO reconstructs foundational reasoning capabilities and achieves superior performance. Extensive experiments on complex reasoning benchmarks demonstrates the effectiveness of our method. Our code is available at https://github.com/Trae1ounG/BuPO. Bottom-up Policy Optimization (BuPO) provides a novel framework to decompose LLM policies into internal layer and modular policies, reveals distinct reasoning patterns across different model architectures, and introduces a bottom-up optimization algorithm that leverages these insights to enhance complex reasoning. Key Findings: Code:https://github.com/Trae1ounG/BuPO Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies, from Institute of Automation, Chinese Academy of Sciences  and tencent AI lab, which bridges the gap between Mechanistic Interpretability and Reinforcement Learning. Instead of treating the LLM as a black-box policy, our work focuses on two key contributions:üîç Interpretability Analysis:We decomposed the LLM policy into \"Internal Layer Policies\" using the Logit Lens. Our analysis reveals a \"Progressive Reasoning\" pattern in models like Qwen, where lower layers maintain high entropy for exploration, while upper layers converge for refinement.‚öôÔ∏è Methodology (BuPO):Based on these insights, we propose Bottom-up Policy Optimization. It utilizes a two-stage training strategy:  This is a strong and timely piece of work, and it resonates surprisingly well with the ‚ÄúAEO vs SEO in 2026‚Äù conversation. What this paper highlights‚Äîdecomposing a monolithic LLM policy into internal layer and modular policies‚Äîmirrors the broader shift we‚Äôre seeing from traditional SEO toward AEO (Answer Engine Optimization). SEO optimized surface signals (keywords, links, rankings), while AEO must optimize for how answers are actually formed inside the model. BuPO essentially operates at the ‚Äúanswer-construction layer,‚Äù not just the output layer, which is exactly where AEO competition will be decided. Several points stand out as especially relevant: Entropy dynamics across layers reflect how modern answer engines explore broadly early and converge late‚Äîvery similar to how AEO systems will balance recall (exploration) and precision (final answer synthesis) in 2026. The observation that Qwen models exhibit more human-like progressive reasoning aligns with AEO‚Äôs goal: producing structured, stepwise, trustworthy answers rather than keyword-matched responses typical of legacy SEO. Bottom-up Policy Optimization feels like an architectural analog to ‚Äúbottom-up content optimization‚Äù for AEO‚Äîstrengthening foundational reasoning rather than overfitting final outputs. In short, this paper isn‚Äôt just advancing RL for LLMs; it‚Äôs pointing toward how answer engines will outperform search engines: by optimizing internal reasoning pathways instead of external ranking tricks. As AEO eclipses SEO in 2026, approaches like BuPO may become the hidden backbone of competitive answer systems. Excellent work‚Äîboth technically rigorous and conceptually aligned with where AI-driven information retrieval is heading. That's a great point! It seems that technology evolves in a very similar way. Thanks for your comment! arXiv lens breakdown of this paper üëâhttps://arxivlens.com/PaperView/Details/bottom-up-policy-optimization-your-language-model-policy-secretly-contains-internal-policies-6219-77124ba0 ¬∑Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2552011",
    "title": "Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies",
    "authors": [
      "Yuqiao Tan",
      "Minzheng Wang"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/Trae1ounG/BuPO",
    "huggingface_url": "https://huggingface.co/papers/2512.19673",
    "upvote": 60
  }
}