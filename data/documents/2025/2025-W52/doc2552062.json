{
  "context": "SWE-EVO benchmark evaluates AI coding agents on complex, multi-step software evolution tasks across multiple files, highlighting a significant gap in current models' capabilities. Existing benchmarks for AI coding agents focus on isolated, single-issue tasks such as fixing a bug or implementing a small feature. However, real-world software engineering is fundamentally a long-horizon endeavor: developers must interpret high-level requirements, plan coordinated changes across many files, and evolve codebases over multiple iterations while preserving existing functionality. We introduceSWE-EVO, a benchmark that evaluates agents on thislong-horizon software evolutionchallenge. Constructed fromrelease notesandversion historiesof seven mature open-source Python projects, Tool comprises 48 evolution tasks that require agents to implementmulti-step modificationsspanning an average of 21 files, validated againstcomprehensive test suitesaveraging 874 tests per instance. Experiments with state-of-the-art models reveal a striking capability gap: evenGPT-5withOpenHandsachieves only a 21 percent resolution rate on Tool, compared to 65 percent on thesingle-issue SWE-Bench Verified. This demonstrates that current agents struggle withsustained,multi-file reasoning. We also proposeFix Rate, a fine-grained metric that captures partial progress toward solving these complex, long-horizon tasks. Existing benchmarks for AI coding agents focus on isolated, single-issue tasks such as fixing a bug or implementing a small feature. However, real-world software engineering is fundamentally a long-horizon endeavor: developers must interpret high-level requirements, plan coordinated changes across many files, and evolve codebases over multiple iterations while preserving existing functionality. We introduce SWE-EVO, a benchmark that evaluates agents on this long-horizon software evolution challenge. Constructed from release notes and version histories of seven mature open-source Python projects, Tool comprises 48 evolution tasks that require agents to implement multi-step modifications spanning an average of 21 files, validated against comprehensive test suites averaging 874 tests per instance. Experiments with state-of-the-art models reveal a striking capability gap: even GPT-5 with OpenHands achieves only a 21 percent resolution rate on Tool, compared to 65 percent on the single-issue SWE-Bench Verified. This demonstrates that current agents struggle with sustained, multi-file reasoning. We also propose Fix Rate, a fine-grained metric that captures partial progress toward solving these complex, long-horizon tasks. arXiv lens breakdown of this paper ðŸ‘‰https://arxivlens.com/PaperView/Details/swe-evo-benchmarking-coding-agents-in-long-horizon-software-evolution-scenarios-3642-2ce677ee Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2552062",
    "title": "SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios",
    "authors": [],
    "publication_year": 2025,
    "github_url": "https://github.com/bdqnghi/SWE-EVO",
    "huggingface_url": "https://huggingface.co/papers/2512.18470",
    "upvote": 10
  }
}