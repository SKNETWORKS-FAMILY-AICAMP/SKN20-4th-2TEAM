{
  "context": "Large Language Models struggle to accurately estimate human cognitive difficulty due to a misalignment with human perceptions and a lack of introspection regarding their own limitations. Accurate estimation of item (question or task) difficulty is critical for educational assessment but suffers from the cold start problem. WhileLarge Language Modelsdemonstrate superhuman problem-solving capabilities, it remains an open question whether they can perceive thecognitive strugglesof human learners. In this work, we present a large-scale empirical analysis ofHuman-AI Difficulty Alignmentfor over 20 models across diverse domains such as medical knowledge and mathematical reasoning. Our findings reveal a systematic misalignment where scaling up model size is not reliably helpful; instead of aligning with humans, models converge toward a sharedmachine consensus. We observe that high performance often impedes accurate difficulty estimation, as models struggle to simulate the capability limitations of students even when being explicitly prompted to adopt specificproficiency levels. Furthermore, we identify a critical lack ofintrospection, as models fail to predict their own limitations. These results suggest that general problem-solving capability does not imply an understanding of humancognitive struggles, highlighting the challenge of using current models for automated difficulty prediction. Key Findings of our Human-LLM difficulty alignment study: arXiv lens breakdown of this paper ðŸ‘‰https://arxivlens.com/PaperView/Details/can-llms-estimate-student-struggles-human-ai-difficulty-alignment-with-proficiency-simulation-for-item-difficulty-prediction-3456-f73a3a88 Agreed - based on experience it is definitely not a native capacity of an LLM to know how difficult things are. Looking at the one line experimental prompts in the appendix, I can see why they didnâ€™t work.  I find to get LLMs to reliably estimate on a task (that they are natively poor at) requires a lot of prompting and domain-specific thinking that takes hundreds of feedback runs to refine. The result is an huge prompt that is often particular to the domain and not easily extended to other tasks even if it does provide the functionality. Thanks for sharing. Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2552034",
    "title": "Can LLMs Estimate Student Struggles? Human-AI Difficulty Alignment with Proficiency Simulation for Item Difficulty Prediction",
    "authors": [
      "Jian Chen",
      "Hong Jiao",
      "Tianyi Zhou"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/MingLiiii/Difficulty_Alignment",
    "huggingface_url": "https://huggingface.co/papers/2512.18880",
    "upvote": 24
  }
}