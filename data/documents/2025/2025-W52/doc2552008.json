{
  "context": "A novel framework, Robust-R1, enhances multimodal large language models' robustness to visual degradations through explicit modeling, supervised fine-tuning, reward-driven alignment, and dynamic reasoning depth scaling, achieving state-of-the-art performance on real-world degradation benchmarks. Multimodal Large Language Modelsstruggle to maintain reliable performance under extreme real-worldvisual degradations, which impede their practical robustness. Existing robust MLLMs predominantly rely on implicit training/adaptation that focuses solely onvisual encoder generalization, suffering from limited interpretability and isolated optimization. To overcome these limitations, we propose Robust-R1, a novel framework that explicitly modelsvisual degradationsthrough structured reasoning chains. Our approach integrates: (i)supervised fine-tuningfor degradation-aware reasoning foundations, (ii)reward-driven alignmentfor accurately perceiving degradation parameters, and (iii)dynamic reasoning depth scalingadapted to degradation intensity. To facilitate this approach, we introduce a specialized 11K dataset featuring realistic degradations synthesized across four critical real-world visual processing stages, each annotated with structured chains connecting degradation parameters, perceptual influence, pristine semantic reasoning chain, and conclusion. Comprehensive evaluations demonstrate state-of-the-art robustness: Robust-R1 outperforms all general and robust baselines on the real-world degradation benchmarkR-Bench, while maintaining superior anti-degradation performance under multi-intensity adversarial degradations onMMMB,MMStar, andRealWorldQA. Multimodal Large Language Models struggle to maintain reliable performance under extreme real-world visual degradations, which impede their practical robustness. Existing robust MLLMs predominantly rely on implicit training/adaptation that focuses solely on visual encoder generalization, suffering from limited interpretability and isolated optimization. To overcome these limitations, we propose Robust-R1, a novel framework that explicitly models visual degradations through structured reasoning chains. Our approach integrates: (i) supervised fine-tuning for degradation-aware reasoning foundations, (ii) reward-driven alignment for accurately perceiving degradation parameters, and (iii) dynamic reasoning depth scaling adapted to degradation intensity. To facilitate this approach, we introduce a specialized 11K dataset featuring realistic degradations synthesized across four critical real-world visual processing stages, each annotated with structured chains connecting degradation parameters, perceptual influence, pristine semantic reasoning chain, and conclusion. Comprehensive evaluations demonstrate state-of-the-art robustness: Robust-R1 outperforms all general and robust baselines on the real-world degradation benchmark R-Bench, while maintaining superior anti-degradation performance under multi-intensity adversarial degradations on MMMB, MMStar, and RealWorldQA. arXiv lens breakdown of this paper ðŸ‘‰https://arxivlens.com/PaperView/Details/robust-r1-degradation-aware-reasoning-for-robust-visual-understanding-3921-1ce675ff Â·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2552008",
    "title": "Robust-R1: Degradation-Aware Reasoning for Robust Visual Understanding",
    "authors": [
      "Jiaqi Tang"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/jqtangust/Robust-R1",
    "huggingface_url": "https://huggingface.co/papers/2512.17532",
    "upvote": 65
  }
}