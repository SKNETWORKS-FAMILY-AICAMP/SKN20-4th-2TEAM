{
  "context": "CASA, a cross-attention method enhanced with self-attention, improves vision-language models' performance on detailed visual tasks while maintaining scalability for long-context multimodal applications. Vision-language models(VLMs) are commonly trained by insertingimage tokensfrom apretrained vision encoderinto thetextual streamof a language model. This allows text and image information to fully attend to one another within the model, but becomes extremely costly for high-resolution images, long conversations, or streaming videos, both in memory and compute.VLMsleveragingcross-attentionare an efficient alternative to token insertion but exhibit a clear performance gap, in particular on tasks involving fine-grained visual details. We find that a key to improving such models is to also enablelocal text-to-text interactionin the dedicatedcross-attentionlayers. Building on this, we proposeCASA,Cross-Attention via Self-Attention, a simple and efficient paradigm which substantially reduces the gap with full token insertion on commonimage understanding benchmarks, while enjoying the same scalability ascross-attentionmodels when applied to long-context multimodal tasks such asstreaming video captioning. For samples and code, please see our project page at https://kyutai.org/casa. Code:https://github.com/kyutai-labs/casa ·Sign uporlog into comment",
  "metadata": {
    "doc_id": "doc2552056",
    "title": "CASA: Cross-Attention via Self-Attention for Efficient Vision-Language Fusion",
    "authors": [
      "Amélie Royer"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/kyutai-labs/casa",
    "huggingface_url": "https://huggingface.co/papers/2512.19535",
    "upvote": 11
  }
}