"""
LangGraph ê¸°ë°˜ RAG ì‹œìŠ¤í…œ (ìµœì¢… ì™„ì„± ë²„ì „ - Sources í¬í•¨)

ì „ì²´ ê°œì„  ì‚¬í•­:
1. âœ… TypedDict import ì˜¤ë¥˜ ìˆ˜ì •
2. âœ… SAM3 ë¬¸ì œ í•´ê²°: ë©”íƒ€ë°ì´í„° ë¶€ìŠ¤íŒ…
3. âœ… langchain ë¬¸ì œ í•´ê²°: ìŠ¤ë§ˆíŠ¸ ë¼ìš°íŒ…
4. âœ… ì£¼ì œ ë¬¸ì œ í•´ê²°: topic_guard_node
5. âœ… RRF/Cluster ìž„ê³„ê°’ ìµœì í™”
6. âœ… Sources êµ¬ì„±: í—ˆê¹…íŽ˜ì´ìŠ¤ URL, ì›¹ ê²€ìƒ‰ URL í‘œì‹œ
"""

# ===== SECTION 1: IMPORTS =====
import os
import sys
import json
import re
import warnings
import hashlib
from pathlib import Path
from typing import List, Dict, Any, Optional, Literal, Set

# âœ… TypedDict import ìˆ˜ì •
try:
    from typing import TypedDict
except ImportError:
    from typing_extensions import TypedDict

from dotenv import load_dotenv

# LangChain
from langchain_core.documents import Document
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# LangChain Community (BM25)
from langchain_community.retrievers import BM25Retriever

# LangGraph
from langgraph.graph import StateGraph, START, END

# ===== SECTION 2: ENVIRONMENT & PATHS =====
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT / "02_src" / "02_utils"))

warnings.filterwarnings("ignore")
load_dotenv()

MODEL_NAME = os.getenv("MODEL_NAME", "MiniLM-L6")
CHUNK_SIZE = int(os.getenv("CHUNK_SIZE", 300))
CHUNK_OVERLAP = int(os.getenv("CHUNK_OVERLAP", 45))

# ===== SECTION 3: GRAPHSTATE =====
class GraphState(TypedDict):
    """LangGraph ìƒíƒœ ê´€ë¦¬"""
    question: str
    original_question: str
    translated_question: Optional[str]
    is_korean: bool
    documents: List[Document]
    doc_scores: List[float]
    cluster_id: Optional[int]
    cluster_similarity_score: Optional[float]
    search_type: str
    relevance_level: str
    answer: str
    sources: List[Dict[str, Any]]
    is_ai_ml_related: bool
    _vectorstore: Any
    _llm: Any
    _bm25_retriever: Any
    _cluster_metadata_path: str

# ===== SECTION 4: HELPER FUNCTIONS =====
def get_doc_hash_key(doc: Document) -> str:
    content = doc.page_content[:1000]
    source = doc.metadata.get('source', '')
    data_to_hash = f"{content}|{source}"
    return hashlib.sha256(data_to_hash.encode('utf-8')).hexdigest()

def is_korean_text(text: str) -> bool:
    korean_pattern = re.compile(r'[ê°€-íž£ã„±-ã…Žã…-ã…£]')
    return bool(korean_pattern.search(text))

def extract_keywords(text: str) -> Set[str]:
    """í‚¤ì›Œë“œ ì¶”ì¶œ"""
    keywords = set()
    pattern1 = r'\b[a-zA-Z]+-?\w*\d+\w*\b'
    keywords.update(re.findall(pattern1, text.lower()))
    pattern2 = r'\b[A-Z]{2,}[a-z]?\d*\b'
    keywords.update([w.lower() for w in re.findall(pattern2, text)])
    pattern3 = r'\b\w+[-_]\w+\b'
    keywords.update(re.findall(pattern3, text.lower()))
    tech_terms = {
        'transformer', 'attention', 'diffusion', 'gan', 'vae', 'bert', 
        'gpt', 'llama', 'sam', 'clip', 'vit', 'resnet', 'unet',
        'rag', 'retrieval', 'embedding', 'tokenizer', 'langchain',
        'pytorch', 'tensorflow', 'huggingface', 'audio', 'model', 'paper', 'papers'
    }
    words = set(re.findall(r'\b\w+\b', text.lower()))
    keywords.update(words & tech_terms)
    return keywords

def calculate_metadata_boost(doc, query_keywords: Set[str]) -> float:
    """ë©”íƒ€ë°ì´í„° ë¶€ìŠ¤íŒ…"""
    boost = 0.0
    metadata = doc.metadata or {}
    title = metadata.get('title', '').lower()
    for keyword in query_keywords:
        if keyword in title:
            boost += 0.05
            break
    doc_keywords = metadata.get('keywords', [])
    if isinstance(doc_keywords, list):
        doc_keywords_lower = [k.lower() for k in doc_keywords]
        for keyword in query_keywords:
            if keyword in doc_keywords_lower:
                boost += 0.02
                break
    doc_id = metadata.get('doc_id', '').lower()
    for keyword in query_keywords:
        if keyword in doc_id:
            boost += 0.01
            break
    return boost

def is_ai_ml_related_by_llm(question: str, llm) -> bool:
    """LLMìœ¼ë¡œ AI/ML ê´€ë ¨ì„± íŒë³„"""
    if not question or llm is None:
        return False

    prompt = ChatPromptTemplate.from_messages([
        ("system", """ë‹¹ì‹ ì€ ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì´ AI/ML/DL/LLM ë¶„ì•¼ì™€ ê´€ë ¨ë˜ì–´ ìžˆëŠ”ì§€ íŒë‹¨í•˜ëŠ” ì´ì§„ ë¶„ë¥˜ê¸°ìž…ë‹ˆë‹¤.

**"YES"ë¥¼ ë°˜í™˜í•´ì•¼ í•˜ëŠ” ê²½ìš°:**
- **ì‹¤ì œ ì¡´ìž¬í•˜ëŠ”** AI/ML/DL/LLM ëª¨ë¸ (GPT, Claude, BERT, LLaMA, Stable Diffusion, YOLO ë“±)
-  **ì‹¤ì œ ì¡´ìž¬í•˜ëŠ”** AI/ML/DL/LLM í”„ë ˆìž„ì›Œí¬/ë„êµ¬ (PyTorch, TensorFlow, Hugging Face, LangChain, Ollama ë“±)
- AI/ML/DL/LLM í”Œëž«í¼ (Hugging Face, Replicate, OpenAI API, Anthropic API ë“±)
- AI/ML/DL/LLM ê°œë… (RAG, íŒŒì¸íŠœë‹, ìž„ë² ë”©, ì–´í…ì…˜, í”„ë¡¬í”„íŒ…, ì–‘ìží™” ë“±)
- AI/ML/DL/LLM ì•„í‚¤í…ì²˜ (Transformer, CNN, RNN, GAN, Diffusion ë“±)
- AI/ML/DL/LLM í•™ìŠµ/ì¶”ë¡  (í›ˆë ¨, ì¶”ë¡ , ë°°í¬, ìµœì í™”, ë²¤ì¹˜ë§ˆí¬ ë“±)
- AI/ML/DL/LLM ì‘ìš© (ì±—ë´‡, ì´ë¯¸ì§€ ìƒì„±, ìŒì„± ì¸ì‹, ì¶”ì²œ ì‹œìŠ¤í…œ ë“±)
- AI/ML/DL/LLM ì—°êµ¬ (ë…¼ë¬¸, ë²¤ì¹˜ë§ˆí¬, SOTA, arXiv ë“±)
- AI/ML/DL/LLM ê´€ë ¨ ë°ì´í„° ì²˜ë¦¬ (ë°ì´í„°ì…‹, ì „ì²˜ë¦¬, augmentation ë“±)
- AI/ML/DL/LLM ìš©ì–´ ì„¤ëª… ìš”ì²­ ("~ëž€?", "~ì´ ë­ì•¼?" ë“±)

**"NO"ë¥¼ ë°˜í™˜í•´ì•¼ í•˜ëŠ” ê²½ìš°:**
- ì¼ìƒ ëŒ€í™”, ê°œì¸ì  ê³ ë¯¼, ê±´ê°•, ì¸ê°„ê´€ê³„
- ì—”í„°í…Œì¸ë¨¼íŠ¸, ìŠ¤í¬ì¸ , ë‰´ìŠ¤, ì¼ë°˜ ìƒì‹
- AI/ML/DL/LLMê³¼ ë¬´ê´€í•œ ìˆ˜í•™, í†µê³„, ê³¼í•™
- ë¹„ì¦ˆë‹ˆìŠ¤, ê¸ˆìœµ, ë²•ë¥  (AI ì‘ìš©ì´ ì•„ë‹Œ ê²½ìš°)

**ì• ë§¤í•œ ê²½ìš° íŒë‹¨ ê¸°ì¤€:**
- "AI/ML/DL/LLMì„ ìœ„í•œ ë°ì´í„° ë¶„ì„" â†’ YES
- "ì¼ë°˜ ë°ì´í„° ë¶„ì„" â†’ NO
- "ì‹ ê²½ë§ì„ ìœ„í•œ ìˆ˜í•™" â†’ YES
- "ì¼ë°˜ ë¯¸ì ë¶„í•™" â†’ NO


**ì¤‘ìš”:** ì§ˆë¬¸ì— AI/ML/DL/LLM ê´€ë ¨ ìš©ì–´ë‚˜ ë„êµ¬ê°€ ì–¸ê¸‰ë˜ë©´ YESë¡œ íŒë‹¨í•˜ì„¸ìš”.

**ì¶œë ¥ í˜•ì‹:** "YES" ë˜ëŠ” "NO"ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ì„¤ëª…, êµ¬ë‘ì , ì¶”ê°€ í…ìŠ¤íŠ¸ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”."""),
        ("human", "{question}")
    ])

    chain = prompt | llm | StrOutputParser()
    try:
        result = chain.invoke({"question": question}).strip().upper()
        return result.startswith("Y")
    except Exception as e:
        print(f"[WARN] LLM topic classification failed: {e}")
        return True

# ===== SECTION 5: NODE FUNCTIONS =====
def translate_node(state: GraphState) -> dict:
    """ë…¸ë“œ 0: í•œê¸€ ì§ˆë¬¸ ë²ˆì—­"""
    print("\n" + "="*60)
    print("[NODE: translate] ì§ˆë¬¸ ì–¸ì–´ í™•ì¸ ë° ë²ˆì—­")
    print("="*60)

    original_question = state["original_question"]
    llm = state.get("_llm")
    has_korean = is_korean_text(original_question)

    if not has_korean:
        print(f"[translate] ì˜ì–´ ì§ˆë¬¸ - ë²ˆì—­ ìŠ¤í‚µ")
        return {
            "question": original_question,
            "original_question": original_question,
            "translated_question": None,
            "is_korean": False
        }

    print(f"[translate] í•œê¸€ ì§ˆë¬¸ - ì˜ì–´ë¡œ ë²ˆì—­ ì¤‘...")
    try:
        translate_prompt = ChatPromptTemplate.from_messages([
            ("system", '''     "ë‹¤ìŒ ê·œì¹™ì— ë”°ë¼ ì‚¬ìš©ìžì˜ ì§ˆì˜ë¥¼ ë²ˆì—­í•˜ë¼.\n"
     "1. ìž…ë ¥ì´ í•œêµ­ì–´ì¼ ê²½ìš° ì˜ì–´ë¡œ ë²ˆì—­í•œë‹¤.\n"
     "2. ë‹¨ìˆœ ë²ˆì—­ì´ ì•„ë‹ˆë¼ AI, ML, ë°ì´í„°, ëª¨ë¸ëª…, í”„ë ˆìž„ì›Œí¬, í•™ìˆ  ìš©ì–´ë¥¼ ì •í™•í•œ ì˜ì–´ ê¸°ìˆ  ìš©ì–´ë¡œ ì •ê·œí™”í•œë‹¤.\n"
     "   ì˜ˆ) ë ˆê·¸/ëž˜ê·¸â†’RAG, ëž­ì²´ì¸/langchainâ†’LangChain, ëž­ê·¸ëž˜í”„â†’LangGraph, íŒŒì¸íŠœë‹â†’fine-tuning,\n"
     "       ìƒì„±í˜• AIâ†’generative AI, í—ˆê¹…íŽ˜ì´ìŠ¤â†’Hugging Face, íŠ¸ëžœìŠ¤í¬ë¨¸â†’Transformer,\n"
     "       ìž„ë² ë”©â†’embedding, ë²¡í„°ë””ë¹„â†’vector database, íŒŒì´í† ì¹˜â†’PyTorch\n"
     "3. ì˜ë¯¸, ê¸°ìˆ ì  ë§¥ë½, ì „ë¬¸ ìš©ì–´ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë©° ë¶ˆí•„ìš”í•œ ì˜ì—­ì„ í•˜ì§€ ì•ŠëŠ”ë‹¤.\n"
     "4. ë¬¸ìž¥ì´ ë„ˆë¬´ ê¸¸ ê²½ìš° ê²€ìƒ‰ ì„±ëŠ¥ì„ ìœ„í•´ í•µì‹¬ ì˜ë¯¸ë§Œ ìœ ì§€í•œ compact English queryë¡œ ìš”ì•½í•  ìˆ˜ ìžˆë‹¤.\n"
     "5. ì•Œ ìˆ˜ ì—†ëŠ” ì•½ì–´ë‚˜ ê¸°ìˆ  ìš©ì–´ë„ ë§¥ë½ìƒ AI/ML ê´€ë ¨ì´ë©´ ê·¸ëŒ€ë¡œ ìœ ì§€í•œë‹¤. (ì˜ˆ: GSW, XYZ ê¸°ë²• ë“±)\n"
     "6. ì¶œë ¥ì€ ì˜¤ì§ ì˜ì–´ë§Œ í•œë‹¤.
             
    **ì¤‘ìš”:** ì§ˆë¬¸ì˜ ì£¼ì œê°€ ë¬´ì—‡ì´ë“  ìƒê´€ì—†ì´ "ë²ˆì—­"ë§Œ ìˆ˜í–‰í•˜ì„¸ìš”. 
            ë‚´ìš©ì˜ ì ì ˆì„± íŒë‹¨í•´ì„œ ìž„ì˜ë¡œ ë²ˆì—­ì„ ìˆ˜ì •í•˜ì§€ë§ˆì„¸ìš”!!!!
             
    **ì˜ˆì‹œ:**
    ìž…ë ¥: "í•´ë¦¬í¬í„° ì¤„ê±°ë¦¬ ì•Œë ¤ì£¼ì„¸ìš”"
    ì¶œë ¥: "Tell me the plot of Harry Potter"

    ìž…ë ¥: "RAG ì‹œìŠ¤í…œ êµ¬ì¶• ë°©ë²•"
    ì¶œë ¥: "How to build a RAG system"

    ìž…ë ¥: "íŠ¸ëžœìŠ¤í¬ë¨¸ ëª¨ë¸ ì„¤ëª…í•´ì¤˜"
    ì¶œë ¥: "Explain the Transformer model
             '''),
            ("human", "{korean_text}")
        ])
        chain = translate_prompt | llm | StrOutputParser()
        translated = chain.invoke({"korean_text": original_question}).strip()
        print(f"[translate] ë²ˆì—­ ì™„ë£Œ: {translated}")
        return {
            "question": translated,
            "original_question": original_question,
            "translated_question": translated,
            "is_korean": True
        }
    except Exception as e:
        print(f"[ERROR] ë²ˆì—­ ì‹¤íŒ¨: {e}")
        return {
            "question": original_question,
            "original_question": original_question,
            "translated_question": None,
            "is_korean": True
        }

def topic_guard_node(state: GraphState) -> dict:
    """âœ… ë…¸ë“œ 1: AI/ML/DL/LLM ê´€ë ¨ì„± ì‚¬ì „ ì²´í¬"""
    print("\n" + "="*60)
    print("[NODE: topic_guard] AI/ML/DL/LLM ê´€ë ¨ì„± ì‚¬ì „ ì²´í¬")
    print("="*60)
    
    original_q = state.get("original_question", "")
    question = state.get("question", "")
    llm = state.get("_llm")
    
    print(f"[topic_guard] ì§ˆë¬¸: {original_q}")
    
    try:
        is_related = is_ai_ml_related_by_llm(question if question else original_q, llm)
        if is_related:
            print(f"[topic_guard] âœ… AI/ML/DL/LLM ê´€ë ¨ â†’ ê²€ìƒ‰ ì§„í–‰")
            return {"is_ai_ml_related": True}
        else:
            print(f"[topic_guard] âŒ ë¹„ AI/ML/DL/LLM â†’ ê²€ìƒ‰ ìŠ¤í‚µ")
            return {"is_ai_ml_related": False}
    except Exception as e:
        print(f"[WARN] í† í”½ ê°€ë“œ ì‹¤íŒ¨: {e}")
        return {"is_ai_ml_related": True}

def retrieve_node(state: GraphState) -> dict:
    """âœ… ë…¸ë“œ 2: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ + ë©”íƒ€ë°ì´í„° ë¶€ìŠ¤íŒ…"""
    print("\n" + "="*60)
    print("[NODE: retrieve] í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰")
    print("="*60)

    question = state["question"]
    vectorstore = state.get("_vectorstore")
    bm25_retriever = state.get("_bm25_retriever")

    if vectorstore is None:
        print("[ERROR] VectorStore ì—†ìŒ")
        return {"documents": [], "doc_scores": [], "cluster_id": None, "search_type": "vector"}

    query_keywords = extract_keywords(question)
    if query_keywords:
        print(f"[retrieve] í‚¤ì›Œë“œ: {query_keywords}")

    use_bm25 = bm25_retriever is not None

    try:
        vector_docs_with_scores = vectorstore.similarity_search_with_score(question, k=5)
        vector_docs = [doc for doc, score in vector_docs_with_scores]
        print(f"[retrieve] ë²¡í„° ê²€ìƒ‰: {len(vector_docs)}ê°œ")

        if not use_bm25:
            print("[retrieve] BM25 ì—†ìŒ â†’ ë²¡í„°ë§Œ ì‚¬ìš©")
            boosted_docs = []
            boosted_scores = []
            for doc, score in vector_docs_with_scores[:5]:
                boost = calculate_metadata_boost(doc, query_keywords)
                adjusted_score = max(0.0, score - boost * 2.0)
                boosted_docs.append(doc)
                boosted_scores.append(adjusted_score)
                if boost > 0:
                    print(f"[retrieve] ë¶€ìŠ¤íŒ…: {doc.metadata.get('title', 'N/A')[:50]} (+{boost:.3f})")

            cluster_id = boosted_docs[0].metadata.get("cluster_id", -1) if boosted_docs else None
            return {
                "documents": boosted_docs,
                "doc_scores": boosted_scores,
                "cluster_id": cluster_id,
                "search_type": "vector"
            }

        bm25_docs = bm25_retriever.invoke(question)
        print(f"[retrieve] BM25 ê²€ìƒ‰: {len(bm25_docs)}ê°œ")

        RRF_K = 60
        fusion_scores = {}
        doc_map = {}
        metadata_boosts = {}

        for rank, (doc, _score) in enumerate(vector_docs_with_scores):
            doc_key = get_doc_hash_key(doc)
            if doc_key not in doc_map:
                doc_map[doc_key] = doc
            score = 1.5 / (RRF_K + rank + 1)
            fusion_scores[doc_key] = fusion_scores.get(doc_key, 0.0) + score
            if doc_key not in metadata_boosts:
                metadata_boosts[doc_key] = calculate_metadata_boost(doc, query_keywords)

        for rank, doc in enumerate(bm25_docs):
            doc_key = get_doc_hash_key(doc)
            if doc_key not in doc_map:
                doc_map[doc_key] = doc
            score = 0.5 / (RRF_K + rank + 1)
            fusion_scores[doc_key] = fusion_scores.get(doc_key, 0.0) + score
            if doc_key not in metadata_boosts:
                metadata_boosts[doc_key] = calculate_metadata_boost(doc, query_keywords)

        for doc_key in fusion_scores:
            boost = metadata_boosts.get(doc_key, 0.0)
            if boost > 0:
                original_score = fusion_scores[doc_key]
                fusion_scores[doc_key] += boost
                doc = doc_map[doc_key]
                print(f"[retrieve] ë¶€ìŠ¤íŒ…: {doc.metadata.get('title', 'N/A')[:50]}")
                print(f"           {original_score:.4f} â†’ {fusion_scores[doc_key]:.4f} (+{boost:.4f})")

        sorted_items = sorted(fusion_scores.items(), key=lambda x: x[1], reverse=True)
        documents = []
        scores = []
        for doc_key, score in sorted_items[:5]:
            documents.append(doc_map[doc_key])
            scores.append(score)

        if not documents:
            print("[retrieve] ë¬¸ì„œ ì—†ìŒ")
            return {"documents": [], "doc_scores": [], "cluster_id": None, "search_type": "hybrid"}

        cluster_id = documents[0].metadata.get("cluster_id", -1)
        print(f"[retrieve] ì™„ë£Œ: {len(documents)}ê°œ, ìµœìƒìœ„={scores[0]:.4f}, Cluster={cluster_id}")

        return {
            "documents": documents,
            "doc_scores": scores,
            "cluster_id": cluster_id,
            "search_type": "hybrid"
        }

    except Exception as e:
        print(f"[ERROR] ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return {"documents": [], "doc_scores": [], "cluster_id": None, "search_type": "hybrid"}

def evaluate_document_relevance_node(state: GraphState) -> dict:
    """âœ… ë…¸ë“œ 3: ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€"""
    print("\n" + "="*60)
    print("[NODE: evaluate] ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€")
    print("="*60)

    original_q = state.get("original_question", "")
    question = state.get("question", "")
    documents = state["documents"]
    scores = state["doc_scores"]
    is_ai_ml_related = state.get("is_ai_ml_related", True)
    
    print(f"[evaluate] AI/ML/DL/LLM ê´€ë ¨ì„±: {is_ai_ml_related}")

    query_keywords = extract_keywords(question)
    if not query_keywords:
        query_keywords = extract_keywords(original_q)

    if documents and query_keywords:
        for doc in documents[:3]:
            metadata = doc.metadata or {}
            title = metadata.get('title', '').lower()
            doc_keywords = [k.lower() for k in metadata.get('keywords', [])]
            for keyword in query_keywords:
                if keyword in title or keyword in ' '.join(doc_keywords):
                    print(f"[evaluate] âœ… ë©”íƒ€ë°ì´í„° ë§¤ì¹­: '{keyword}'")
                    return {"relevance_level": "high", "is_ai_ml_related": is_ai_ml_related}

    if not documents or not scores:
        print("[evaluate] ë¬¸ì„œ ì—†ìŒ â†’ LOW")
        return {"relevance_level": "low", "is_ai_ml_related": is_ai_ml_related}

    best_score = max(scores)
    if best_score >= 0.0325:
        level = "high"
    elif best_score >= 0.0120:
        level = "medium"
    else:
        level = "low"

    print(f"[evaluate] RRF={best_score:.6f} â†’ {level.upper()}")
    return {"relevance_level": level, "is_ai_ml_related": is_ai_ml_related}

def cluster_similarity_check_node(state: GraphState) -> dict:
    """ë…¸ë“œ 4: í´ëŸ¬ìŠ¤í„° ìœ ì‚¬ë„ ì²´í¬"""
    print("\n" + "="*60)
    print("[NODE: cluster_check] í´ëŸ¬ìŠ¤í„° ì²´í¬")
    print("="*60)

    cluster_id = state["cluster_id"]
    question = state["question"]
    vectorstore = state.get("_vectorstore")
    cluster_metadata_path = state.get("_cluster_metadata_path")

    if cluster_id is None or cluster_id == -1:
        print("[cluster_check] cluster_id ì—†ìŒ")
        return {"cluster_similarity_score": 0.0, "search_type": "cluster"}

    try:
        with open(cluster_metadata_path, 'r', encoding='utf-8') as f:
            cluster_meta = json.load(f)

        cluster_info = cluster_meta["clusters"].get(str(cluster_id))
        if not cluster_info:
            print(f"[cluster_check] ì •ë³´ ì—†ìŒ")
            return {"cluster_similarity_score": 0.0, "search_type": "cluster"}

        cluster_density = cluster_info.get("density", 0.0)
        print(f"[cluster_check] Cluster {cluster_id} ë°€ë„: {cluster_density:.3f}")

        all_docs = vectorstore.similarity_search_with_score(question, k=20)
        filtered_docs = [
            (doc, score) for doc, score in all_docs
            if doc.metadata.get("cluster_id") == cluster_id
        ][:5]

        if not filtered_docs:
            print("[cluster_check] ì¶”ê°€ ë¬¸ì„œ ì—†ìŒ")
            return {"cluster_similarity_score": 0.0, "search_type": "cluster"}

        avg_score = sum(score for doc, score in filtered_docs) / len(filtered_docs)
        print(f"[cluster_check] {len(filtered_docs)}ê°œ, í‰ê· ={avg_score:.4f}")

        existing_docs = state["documents"]
        additional_docs = [doc for doc, score in filtered_docs[:3]]
        existing_contents = {doc.page_content for doc in existing_docs}
        unique_additional = [doc for doc in additional_docs if doc.page_content not in existing_contents]
        merged_docs = existing_docs + unique_additional

        print(f"[cluster_check] ì¶”ê°€ {len(unique_additional)}ê°œ ë³‘í•©")

        return {
            "cluster_similarity_score": avg_score,
            "documents": merged_docs,
            "search_type": "cluster"
        }

    except Exception as e:
        print(f"[ERROR] í´ëŸ¬ìŠ¤í„° ì²´í¬ ì˜¤ë¥˜: {e}")
        return {"cluster_similarity_score": 0.0, "search_type": "cluster"}

def web_search_node(state: GraphState) -> dict:
    """ë…¸ë“œ 5: ì›¹ ê²€ìƒ‰"""
    print("\n" + "="*60)
    print("[NODE: web_search] ì›¹ ê²€ìƒ‰ ì‹œìž‘")
    print("="*60)

    question = state["question"]

    try:
        from langchain_community.retrievers import TavilySearchAPIRetriever
        print("[web_search] Tavily API ì‚¬ìš©")

        retriever = TavilySearchAPIRetriever(k=5)
        web_docs_raw = retriever.invoke(question)
        print(f"[web_search] Tavily: {len(web_docs_raw)}ê°œ")

        processed_web_docs = []
        for i, doc in enumerate(web_docs_raw):
            original_meta = doc.metadata
            title = original_meta.get('title', 'ì›¹ ê²€ìƒ‰ ê²°ê³¼')
            source_url = original_meta.get('source', '')
            score = original_meta.get('score', 0.5)
            
            web_doc = Document(
                page_content=doc.page_content,
                metadata={
                    'title': title,
                    'source': source_url,
                    'source_type': 'web',
                    'score': score,
                    'index': i
                }
            )
            processed_web_docs.append(web_doc)
            print(f"  [{i+1}] {title[:60]}")

        return {
            "documents": processed_web_docs,
            "search_type": "web",
            "doc_scores": [doc.metadata['score'] for doc in processed_web_docs]
        }

    except Exception as e:
        print(f"[web_search] Tavily ì‹¤íŒ¨: {e}")
        return {
            "documents": [Document(
                page_content="ì›¹ ê²€ìƒ‰ ì‹¤íŒ¨",
                metadata={"source": "system", "source_type": "error"}
            )],
            "search_type": "web_failed",
            "doc_scores": [2.0]
        }

def generate_final_answer_node(state: GraphState) -> dict:
    """âœ… ë…¸ë“œ 6: ìµœì¢… ë‹µë³€ ìƒì„± + Sources êµ¬ì„±"""
    print("\n" + "=" * 60)
    print("[NODE: generate] ìµœì¢… ë‹µë³€ ìƒì„±")
    print("=" * 60)

    original_question = state["original_question"]
    documents = state["documents"]
    search_type = state.get("search_type", "internal")
    is_korean = state.get("is_korean", False)
    llm = state.get("_llm")

    if is_korean:
        print(f"[generate] ì›ë³¸(í•œê¸€): {original_question}")
        print(f"[generate] ê²€ìƒ‰(ì˜ì–´): {state.get('question')}")

    # 1) CONTEXT ë¸”ë¡
    if not documents:
        context_str = "NO_RELEVANT_PAPERS"
    else:
        context_blocks = []
        for i, doc in enumerate(documents[:5], 1):
            meta = doc.metadata or {}
            title = meta.get("title", meta.get("paper_name", "No information"))
            authors = meta.get("authors", "No information")
            hf_url = meta.get("huggingface_url", meta.get("source", "No information"))
            gh_url = meta.get("github_url", "No information")
            upvote = meta.get("upvote", "No information")
            year = meta.get("publication_year", "No information")
            total_chunks = meta.get("total_chunks", "No information")
            doc_id = meta.get("doc_id", "No information")
            chunk_index = meta.get("chunk_index", "No information")

            block = f"""
[DOCUMENT {i}]
page_content:
{doc.page_content}

metadata:
  title: {title}
  huggingface_url: {hf_url}
  github_url: {gh_url}
  upvote: {upvote}
  authors: {authors}
  publication_year: {year}
  total_chunks: {total_chunks}
  doc_id: {doc_id}
  chunk_index: {chunk_index}
"""
            context_blocks.append(block)
        context_str = "\n".join(context_blocks)

    # 2) í”„ë¡¬í”„íŠ¸
    prompt = ChatPromptTemplate.from_messages([
        ("system", """You are "AI Tech Trend Navigator", an expert for AI/ML/DL/LLM papers.
Summarize papers clearly. Explain in simple terms. Highlight practical use-cases.
Rely only on given context. Do not invent details.
ALWAYS respond in Korean."""),
        ("human", """
[QUESTION]
{question}

[CONTEXT]
{context}

Answer structure:
1) One-line summary
2) Key insights (max 3 bullets)
3) Detailed explanation

âš  Do not hallucinate. ALWAYS respond in Korean. No bold/italics.
""")
    ])

    # 3) ì²´ì¸ ì‹¤í–‰
    try:
        chain = prompt | llm | StrOutputParser()
        answer = chain.invoke({"question": original_question, "context": context_str})
        print("[generate] ë‹µë³€ ìƒì„± ì™„ë£Œ")
    except Exception as e:
        print(f"[ERROR] ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {e}")
        answer = f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    # âœ… 4) SOURCES êµ¬ì„±
        # 4) sources êµ¬ì„± - ì¤‘ë³µ ì œê±° í¬í•¨
    seen_docs = set()
    sources: List[Dict[str, Any]] = []
    
    for doc in documents[:5]:  # ë‹µë³€ì— ì‚¬ìš©ëœ ë¬¸ì„œë“¤
        meta = doc.metadata or {}
        doc_id = meta.get('doc_id')
        
        # doc_idê°€ ìžˆëŠ” ê²½ìš° ì¤‘ë³µ ì²´í¬
        if doc_id and doc_id in seen_docs:
            print(f"[generate] ì¤‘ë³µ ë¬¸ì„œ ìŠ¤í‚µ: doc_id={doc_id}")
            continue
        
        # doc_id ê¸°ë¡
        if doc_id:
            seen_docs.add(doc_id)
        
        # ì›¹ ê²€ìƒ‰ ê²°ê³¼ì¸ ê²½ìš°
        if meta.get('source_type') == 'web':
            sources.append({
                "type": "web",
                "title": meta.get("title", "ì›¹ ê²€ìƒ‰ ê²°ê³¼"),
                "url": meta.get("source", ""),
                "score": meta.get("score", 0.5)
            })
            print(f"[generate] ì›¹ ì¶œì²˜ ì¶”ê°€: {meta.get('title', 'Unknown')[:50]}")
        # ë…¼ë¬¸ ë¬¸ì„œì¸ ê²½ìš°
        else:
            title = meta.get("title", meta.get("paper_name", "Unknown"))
            hf_url = meta.get("huggingface_url", meta.get("source", ""))
            gh_url = meta.get("github_url", "")
            authors = meta.get("authors", "Unknown")
            year = meta.get("publication_year", "Unknown")
            upvote = meta.get("upvote", 0)
            doc_id = meta.get("doc_id", "")
            
            sources.append({
                "type": "paper",
                "title": title,
                "huggingface_url": hf_url,
                "github_url": gh_url,
                "authors": authors,
                "year": year,
                "upvote": upvote,
                "doc_id": doc_id
            })
            print(f"[generate] ë…¼ë¬¸ ì¶œì²˜ ì¶”ê°€: {meta.get('title', 'Unknown')[:50]}")
    
    print(f"[generate] ì´ {len(sources)}ê°œ ê³ ìœ  ì¶œì²˜ ì¶”ê°€ë¨")

    return {
        "answer": answer,
        "sources": sources
    }
    
    
def reject_node(state: GraphState) -> dict:
    """âœ… ë…¸ë“œ 7: ê±°ë¶€ ì‘ë‹µ"""
    print("\n" + "="*60)
    print("[NODE: reject] ì§ˆë¬¸ ê±°ë¶€")
    print("="*60)

    question = state["original_question"]
    is_ai_ml_related = state.get("is_ai_ml_related", False)

    if not is_ai_ml_related:
        answer = f"""ì£„ì†¡í•©ë‹ˆë‹¤. "{question}"ëŠ” AI/ML/DL/LLM ì—°êµ¬ ë…¼ë¬¸ê³¼ ê´€ë ¨ì´ ì—†ëŠ” ì§ˆë¬¸ìž…ë‹ˆë‹¤.

ì´ ì‹œìŠ¤í…œì€ ë‹¤ìŒê³¼ ê°™ì€ ì§ˆë¬¸ì— ë‹µë³€í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤:
â€¢ AI/ML/DL/LLM ëª¨ë¸ ë° ì•„í‚¤í…ì²˜ (GPT, BERT, SAM, Diffusion ë“±)
â€¢ AI/ML/DL/LLM ê°œë… ë° ê¸°ìˆ  (Transformer, Attention, Fine-tuning ë“±)
â€¢ AI/ML/DL/LLM ë„êµ¬ ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ (LangChain, Transformers, PyTorch ë“±)
â€¢ ìµœê·¼ AI/ML/DL/LLM ì—°êµ¬ ë™í–¥ ë° ë…¼ë¬¸

AI/ML/DL/LLM ê´€ë ¨ ì§ˆë¬¸ìœ¼ë¡œ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”! ðŸ¤—"""
        print(f"[reject] ë¹„ AI/ML/DL/LLM ì§ˆë¬¸ ê±°ë¶€")
    else:
        answer = f"""ì£„ì†¡í•©ë‹ˆë‹¤. '{question}'ì™€ ê´€ë ¨ëœ ì ì ˆí•œ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.

ë‹¤ìŒê³¼ ê°™ì´ ì‹œë„í•´ë³´ì„¸ìš”:
1. ë” êµ¬ì²´ì ì¸ í‚¤ì›Œë“œ ì‚¬ìš© (ì˜ˆ: "transformer", "attention mechanism")
2. ì˜ì–´ í•™ìˆ  ìš©ì–´ ì‚¬ìš©
3. ì§ˆë¬¸ì„ ë‹¤ì‹œ í‘œí˜„í•´ë³´ê¸°
4. ìµœê·¼ ë°œí‘œëœ ë…¼ë¬¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰

ì´ ì‹œìŠ¤í…œì€ HuggingFaceì— ê²Œì‹œëœ ìµœê·¼ 10ì£¼ê°„ì˜ AI/ML ì—°êµ¬ ë…¼ë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤."""
        print(f"[reject] AI/ML ê´€ë ¨ì´ì§€ë§Œ ë¬¸ì„œ ì—†ìŒ")

    return {"answer": answer, "sources": [], "search_type": "rejected"}

# ===== SECTION 6: CONDITIONAL EDGE FUNCTIONS =====
def route_after_topic_guard(state: GraphState) -> Literal["retrieve", "reject"]:
    is_ai_ml_related = state.get("is_ai_ml_related", True)
    print(f"\n[ROUTING] topic_guard â†’ ", end="")
    if is_ai_ml_related:
        print("retrieve")
        return "retrieve"
    else:
        print("reject")
        return "reject"

def route_after_evaluate(state: GraphState) -> Literal["generate", "cluster_check", "web_search", "reject"]:
    level = state.get("relevance_level", "low")
    documents = state.get("documents", [])
    is_ai_ml_related = state.get("is_ai_ml_related", True)

    print(f"\n[ROUTING] evaluate â†’ {level.upper()} (AI/ML: {is_ai_ml_related})", end=" â†’ ")

    if level == "high":
        print("generate")
        return "generate"
    elif level == "medium":
        print("cluster_check")
        return "cluster_check"
    else:
        if is_ai_ml_related:
            if documents and len(documents) > 0:
                print("cluster_check")
                return "cluster_check"
            else:
                print("web_search")
                return "web_search"
        else:
            print("reject")
            return "reject"

def route_after_cluster_check(state: GraphState) -> Literal["generate", "web_search"]:
    cluster_score = state.get("cluster_similarity_score", 0.0)
    cluster_id = state.get("cluster_id", -1)
    cluster_metadata_path = state.get("_cluster_metadata_path")

    print(f"\n[ROUTING] cluster_check â†’ ", end="")

    try:
        with open(cluster_metadata_path, 'r', encoding='utf-8') as f:
            cluster_meta = json.load(f)
        cluster_info = cluster_meta["clusters"].get(str(cluster_id), {})
        density = cluster_info.get("density", 0.0)

        if cluster_score <= 0.85 and density >= 1.572:
            print(f"HIGH (score={cluster_score:.3f}, density={density:.3f}) â†’ generate")
            return "generate"
        else:
            print(f"LOW (score={cluster_score:.3f}, density={density:.3f}) â†’ web_search")
            return "web_search"
    except Exception as e:
        print(f"ERROR ({e}) â†’ web_search")
        return "web_search"

# ===== SECTION 7: GRAPH BUILDER =====
def build_langgraph_rag():
    print("\n" + "="*60)
    print("[GRAPH BUILD] LangGraph êµ¬ì¶•")
    print("="*60)

    graph = StateGraph(GraphState)

    graph.add_node("translate", translate_node)
    graph.add_node("topic_guard", topic_guard_node)
    graph.add_node("retrieve", retrieve_node)
    graph.add_node("evaluate", evaluate_document_relevance_node)
    graph.add_node("cluster_check", cluster_similarity_check_node)
    graph.add_node("web_search", web_search_node)
    graph.add_node("generate", generate_final_answer_node)
    graph.add_node("reject", reject_node)

    print("[GRAPH] 8ê°œ ë…¸ë“œ ì¶”ê°€ ì™„ë£Œ")

    graph.add_edge(START, "translate")
    graph.add_edge("translate", "topic_guard")
    graph.add_edge("retrieve", "evaluate")

    graph.add_conditional_edges("topic_guard", route_after_topic_guard, {"retrieve": "retrieve", "reject": "reject"})
    graph.add_conditional_edges("evaluate", route_after_evaluate, {"generate": "generate", "cluster_check": "cluster_check", "web_search": "web_search", "reject": "reject"})
    graph.add_conditional_edges("cluster_check", route_after_cluster_check, {"generate": "generate", "web_search": "web_search"})

    graph.add_edge("web_search", "generate")
    graph.add_edge("generate", END)
    graph.add_edge("reject", END)

    print("[GRAPH] ì—£ì§€ ì¶”ê°€ ì™„ë£Œ")

    compiled_graph = graph.compile()
    print("[GRAPH] ì»´íŒŒì¼ ì™„ë£Œ")

    return compiled_graph

# ===== SECTION 8: INTERACTIVE MODE =====
def run_interactive_mode(vectorstore, llm, bm25_retriever, cluster_metadata_path, langgraph_app):
    print("\n" + "="*60)
    print("ëŒ€í™”í˜• ëª¨ë“œ ì‹œìž‘")
    print("="*60)

    while True:
        try:
            question = input("\n[ì§ˆë¬¸] >> ").strip()
            if question.lower() in ['quit', 'exit', 'q', 'ì¢…ë£Œ']:
                print("\nì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            if not question:
                continue

            initial_state = {
                "question": "", "original_question": question, "translated_question": None, "is_korean": False,
                "documents": [], "doc_scores": [], "cluster_id": None, "cluster_similarity_score": None,
                "search_type": "", "relevance_level": "", "answer": "", "sources": [], "is_ai_ml_related": True,
                "_vectorstore": vectorstore, "_llm": llm, "_bm25_retriever": bm25_retriever,
                "_cluster_metadata_path": cluster_metadata_path
            }

            result = langgraph_app.invoke(initial_state)

            print("\n" + "="*60)
            print("[ìµœì¢… ê²°ê³¼]")
            print("="*60)
            print(f"\n[ë‹µë³€]\n{result['answer']}")
            print(f"\n[ì¶œì²˜] {len(result.get('sources', []))}ê°œ")
            for i, src in enumerate(result.get('sources', [])[:3], 1):
                if src.get('type') == 'web':
                    print(f"  {i}. {src.get('title')} (ì›¹)")
                    print(f"     {src.get('url')}")
                else:
                    print(f"  {i}. {src.get('title')}")
                    print(f"     HF: {src.get('huggingface_url')}")
            print("\n" + "-"*60)

        except KeyboardInterrupt:
            print("\n\nì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        except Exception as e:
            print(f"\n[ERROR] {e}")

# ===== SECTION 9: EXTERNAL API FUNCTIONS =====
_vectorstore = None
_llm = None
_bm25_retriever = None
_cluster_metadata_path = None
_langgraph_app = None

def initialize_langgraph_system(
    model_name: Optional[str] = None,
    chunk_size: Optional[int] = None,
    chunk_overlap: Optional[int] = None,
    llm_model: str = "gpt-4o-mini",
    llm_temperature: float = 0
) -> dict:
    global _vectorstore, _llm, _bm25_retriever, _cluster_metadata_path, _langgraph_app

    try:
        print("\n[INIT] ì´ˆê¸°í™” ì¤‘...")
        if model_name is None:
            model_name = MODEL_NAME
        if chunk_size is None:
            chunk_size = CHUNK_SIZE
        if chunk_overlap is None:
            chunk_overlap = CHUNK_OVERLAP

        from vectordb import load_vectordb
        print(f"[LOADING] VectorStore: {model_name}")
        _vectorstore = load_vectordb(model_name, chunk_size, chunk_overlap)

        print("[LOADING] BM25 Retriever")
        collection_data = _vectorstore._collection.get(include=['documents', 'metadatas'])
        all_documents = [
            Document(page_content=content, metadata=metadata)
            for content, metadata in zip(collection_data['documents'], collection_data['metadatas'])
        ]
        if not all_documents:
            raise ValueError('ë¬¸ì„œ ì—†ìŒ')

        _bm25_retriever = BM25Retriever.from_documents(all_documents)
        _bm25_retriever.k = 3
        print(f"[SUCCESS] BM25: {len(all_documents)}ê°œ")

        print(f"[LOADING] LLM: {llm_model}")
        _llm = ChatOpenAI(model=llm_model, temperature=llm_temperature)

        _cluster_metadata_path = str(PROJECT_ROOT / "01_data" / "clusters" / "cluster_metadata.json")

        print("[LOADING] LangGraph ì»´íŒŒì¼")
        _langgraph_app = build_langgraph_rag()

        print("[SUCCESS] ì´ˆê¸°í™” ì™„ë£Œ!\n")

        return {'status': 'success', 'message': 'Initialized successfully'}

    except Exception as e:
        print(f"[ERROR] ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return {'status': 'error', 'message': str(e)}

def ask_question(question: str, verbose: bool = False) -> dict:
    global _vectorstore, _llm, _bm25_retriever, _cluster_metadata_path, _langgraph_app

    if _langgraph_app is None:
        return {'success': False, 'error': 'Not initialized'}

    try:
        if verbose:
            print(f"\n[QUESTION] {question}")

        initial_state = {
            "question": "", "original_question": question, "translated_question": None, "is_korean": False,
            "documents": [], "doc_scores": [], "cluster_id": None, "cluster_similarity_score": None,
            "search_type": "", "relevance_level": "", "answer": "", "sources": [], "is_ai_ml_related": True,
            "_vectorstore": _vectorstore, "_llm": _llm, "_bm25_retriever": _bm25_retriever,
            "_cluster_metadata_path": _cluster_metadata_path
        }

        result = _langgraph_app.invoke(initial_state)

        if verbose:
            print(f"[ANSWER] ì™„ë£Œ")
            print(f"[SOURCES] {len(result.get('sources', []))}ê°œ")

        return {
            'success': True,
            'question': question,
            'answer': result.get('answer', ''),
            'sources': result.get('sources', []),
            'metadata': {
                'search_type': result.get('search_type', ''),
                'relevance_level': result.get('relevance_level', ''),
                'cluster_id': result.get('cluster_id'),
                'is_korean': result.get('is_korean', False),
                'translated_question': result.get('translated_question'),
                'is_ai_ml_related': result.get('is_ai_ml_related', True)
            }
        }

    except Exception as e:
        print(f"[ERROR] ì§ˆë¬¸ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return {'success': False, 'error': str(e)}

def get_system_status() -> dict:
    return {
        'initialized': _langgraph_app is not None,
        'vectorstore_loaded': _vectorstore is not None,
        'llm_loaded': _llm is not None,
        'bm25_retriever_loaded': _bm25_retriever is not None,
        'cluster_metadata_loaded': _cluster_metadata_path is not None
    }

# ===== SECTION 10: MAIN =====
if __name__ == "__main__":
    print("\n" + "="*60)
    print("LangGraph RAG System")
    print("="*60)

    try:
        from vectordb import load_vectordb

        print("[LOADING] VectorStore")
        vectorstore = load_vectordb(MODEL_NAME, CHUNK_SIZE, CHUNK_OVERLAP)

        print("[LOADING] BM25")
        collection_data = vectorstore._collection.get(include=['documents', 'metadatas'])
        all_documents = [
            Document(page_content=content, metadata=metadata)
            for content, metadata in zip(collection_data['documents'], collection_data['metadatas'])
        ]
        bm25_retriever = BM25Retriever.from_documents(all_documents)
        bm25_retriever.k = 3
        print(f"[SUCCESS] BM25: {len(all_documents)}ê°œ")

        print("[LOADING] LLM")
        llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

        cluster_metadata_path = str(PROJECT_ROOT / "01_data" / "clusters" / "cluster_metadata.json")

        print("[LOADING] LangGraph")
        langgraph_app = build_langgraph_rag()

        print("\n[SUCCESS] ëª¨ë“  ë¦¬ì†ŒìŠ¤ ë¡œë“œ ì™„ë£Œ!\n")

        run_interactive_mode(vectorstore, llm, bm25_retriever, cluster_metadata_path, langgraph_app)

    except Exception as e:
        print(f"\n[ERROR] {e}")
        import traceback
        traceback.print_exc()